[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the UCR Data Center!",
    "section": "",
    "text": "Learn more about the Data Center, or check out our general and course-specific resources for improving your data science skills.\nIf you’re looking for data science support with a UCR course or your SEPR, or you’re interested in doing an internship with the Data Center, please email us or attend our office hours."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "r_basics.html",
    "href": "r_basics.html",
    "title": "Getting started with R and RStudio",
    "section": "",
    "text": "This tutorial assumes that you have already installed R and RStudio. If not, please follow this installation tutorial.\nA good resource to get a basic familiarity with the setup of R and some key definitions is A (very) short introduction to R.\nPlease watch this video (4:17), then read and follow along with the written tutorial below."
  },
  {
    "objectID": "r_basics.html#first-steps",
    "href": "r_basics.html#first-steps",
    "title": "Getting started with R and RStudio",
    "section": "",
    "text": "This tutorial assumes that you have already installed R and RStudio. If not, please follow this installation tutorial.\nA good resource to get a basic familiarity with the setup of R and some key definitions is A (very) short introduction to R.\nPlease watch this video (4:17), then read and follow along with the written tutorial below."
  },
  {
    "objectID": "r_basics.html#creating-a-project-in-rstudio",
    "href": "r_basics.html#creating-a-project-in-rstudio",
    "title": "Getting started with R and RStudio",
    "section": "Creating a project in RStudio",
    "text": "Creating a project in RStudio\nIt is convenient to create an R project for each assignment that you are working on. A project is basically a folder that stores all files related to the assignment.\nYou can create a project as follows:\n\nOpen RStudio and click on “Project: (None)” in the top right corner.\nOpen the dropdown window and click on “New Project….”\nIn the popup window select “New Directory”, then “New Project.”\nChoose a sensible name for your project and enter it as the Directory Name. You can either use the default file path or change it by clicking “Browse…” next to “Create project as a subdirectory of:.”\nFinally, click on “Create project.”\n\nAfter a project is created, there are two easy ways of accessing it. You can either use the same dropdown window in the top right corner of RStudio that you used to create the project, and click on the name of the project there, or you can find the project folder within your files and click on the file with the .Rproj extension."
  },
  {
    "objectID": "r_basics.html#the-rstudio-environment",
    "href": "r_basics.html#the-rstudio-environment",
    "title": "Getting started with R and RStudio",
    "section": "The RStudio environment",
    "text": "The RStudio environment\nWhen you open RStudio, your screen is most divided into 3 or 4 panels (depending on whether you have any open scripts or data viewer tabs). The most important tabs are the Console, the Environment, Files, Plots, Help, Viewer, and the script/data viewer pane.\nThe Console runs any code just by pressing Enter. It runs code and displays output. It is good practice to run small functions that don’t need to be saved, such as installing packages or searching for help files directly from the Console. When you run a script, your code automatically gets sent to the Console.\nThe Environment tab lists all objects currently defined in your R session. You can work with them within R, but they are not saved anywhere outside of R, and therefore disappear when your session is over. If you click on the name of a dataframe in the Environment tab, it opens the full data in the data viewer.\nThe Files tab lists the files in your current working directory by default, but allows you to look at the contents of the other folders on your computer as well. It is equivalent to browsing your file explorer.\nThe Plots and Viewer tabs display the static and and interactive plots you create, while the Help tab lets you browse help files."
  },
  {
    "objectID": "r_basics.html#working-with-scripts",
    "href": "r_basics.html#working-with-scripts",
    "title": "Getting started with R and RStudio",
    "section": "Working with scripts",
    "text": "Working with scripts\nScripts are the basis of reproducible workflows: you save all your code into a script (basically a text file), and you can use that file to rerun your analysis and get the same result every time. It also makes your code easier to edit and gives you a clear overview.\nYou can open a new script by File -&gt; New File -&gt; R Script or by the keyboard shortcut Control/Command + Shift + N. You can run parts of you script by selecting the relevant lines and clicking “Run” or using the shortcut Control/Command + Enter. Control/Command + Shift + Enter runs your whole script. Make sure to save your script regularly."
  },
  {
    "objectID": "r_basics.html#installing-packages",
    "href": "r_basics.html#installing-packages",
    "title": "Getting started with R and RStudio",
    "section": "Installing packages",
    "text": "Installing packages\nMost of the time when you work with R, you’ll need to use functions or data from packages next to the base R functions that are automatically loaded when you open R.\nOne such package that you should almost always load when working with R is tidyverse, which is a collection of packages that allow clean workflows of data import, cleaning and manipulation in R. The ggplot2 package that you can use to create figures is one of these included packages.\nYou need to install each package only once, but you need to load them every time you open and use R. It is good practice to load the package on the top of each script. This is how you would install and load tidyverse (and any other package).\n\ninstall.packages(\"tidyverse\") # install the package\n\n\nlibrary(tidyverse) # load the package"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Setting up R\n\nStart here to learn how to use R.\n\nInstalling R"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Ongoing data encounters in UCR courses",
    "section": "",
    "text": "Ongoing data encounters in UCR courses\n\nAH-ANTQ103 - Introduction to Archeology\nAH-RHET101 - Introduction to Rhetoric and Argumentation\nSCIBIOM303 - Mechanisms of Disease\n\n\n\nData encounter archive\n\nAH-LING203 Corpus Linguistics:\n\nSpring 2023\n\nAH-ANTQ103 Introduction to Archeology:\n\nSpring 2024\n\nSCIBIOM303 Mechanisms of Disease:\n\nFall 2023\n\nSCICOGN302 Psycholinguistics:\n\nFall 2023\nFall 2024\n\nSCIENVI Ecology & Topics in Ecology:\n\nSpring 2022\nSpring 2023\n\nSSCECON207 International Macroeconomics:\n\nSpring 2022\nSpring 2023 (June)\n\nSSCECON209 Money, Banking and Finance:\n\nSpring 2022\n\nSSCECON307 Econometrics:\n\nFall 2022\n\nSSCPOLI302 European Union Politics:\n\nFall 2023\nFall 2024\n\n\n\n\nOther developed data encounters\n\nAH-FTME101 - Introduction to Film, Theatre, and Media"
  },
  {
    "objectID": "apprenticeship.html",
    "href": "apprenticeship.html",
    "title": "Data Center Apprenticeships",
    "section": "",
    "text": "The Data Center Apprenticeship program is designed to help faculty and staff integrate data science and AI tools into teaching, research, and administrative processes. Students work together with UCR a member of faculty or staff to develop ways to incorporate data science tools into the UCR curriculum, faculty research, or university-wide administrative processes.\nApprenticeship programs take place in January and June, and consist of a 2-week intensive training program tailored to the needs of the projects that the apprentices will be working on. The participating students are expected to attend these workshops, and have until the end of the following semester to complete their projects; participation in the Apprenticeship program counts as an academic internship and earns students one course’s worth of credits (7.5 ECTS).\nFor faculty and staff: If you have a project idea that could benefit from the help of a data science apprentice, please consider responding to the call for project proposals that we send out in the first half of the semester.\nFor students: If you’re interested in becoming an apprentice, please email us at datacenter@ucr.nl, describing your academic background and the kinds of projects you would be interested in.\n\n2025 January\nThe January 2025 Apprenticeship program is now running! All are welcome to join our daily workshops. You can access the schedule and workshop materials here.\n\n\n2024 June\nAfter a successful pilot, the second iteration of the Apprenticeship program took place in June 2024. Our apprentices support two data encounters and an interdisciplinary research project. They will continue to work on their projects in the Fall 2024 semester. You can access all workshop materials here.\n\n\n2024 January\nThe first iteration of the Apprenticeship program took place in January 2024. Our six apprentices worked on a wide range of projects, including data encounters, UCR marketing efforts, and a faculty research project. Read more about the projects and access all workshop materials here."
  },
  {
    "objectID": "ai.html",
    "href": "ai.html",
    "title": "Generative AI",
    "section": "",
    "text": "Introduction\nAI is transforming the way we work, think, and create—but are you making the most of it? Our Gen AI Workshop Series, led by Dr. Alexei Karas and Lina Mooren, is designed to help you integrate AI into your daily tasks with practical, hands-on sessions. One lunch time a week from now until Week 15, we will be exploring a new AI use case—from writing emails and brainstorming ideas to enhancing research, automating workflows, and more. Whether you’re a beginner or already experimenting with AI, these sessions will give you the tools to work smarter, save time, and boost your creativity. Sessions will be hybrid—attendees can join in-person or via the Teams link provided in the table below, no sign-ups necessary. We hope to see you there!\n\n\nWorkshop schedule\n\n\n\n\n\n\nWeek\n\n\nDate\n\n\nDay\n\n\nTime\n\n\nRoom\n\n\nTheme\n\n\nTeams Meeting Link\n\n\n\n\n\n\n3\n\n\n14 Feb\n\n\nFriday\n\n\n13:10-13:35\n\n\nA-21\n\n\nResearch Question Generation\n\n\nLink\n\n\n\n\n8\n\n\n28 Mar\n\n\nFriday\n\n\n13:10-13:35\n\n\nA-21\n\n\nWriting in Collaboration with AI\n\n\nLink\n\n\n\n\n9\n\n\n4 Apr\n\n\nFriday\n\n\n13:10-13:35\n\n\nA-21\n\n\nMarketing with AI\n\n\nLink\n\n\n\n\n13\n\n\n2 May\n\n\nFriday\n\n\n13:10-13:35\n\n\nA-21\n\n\nExam Question Generation\n\n\nLink\n\n\n\n\n14\n\n\n9 May\n\n\nFriday\n\n\n13:10-13:35\n\n\nA-21\n\n\nNotebook LM\n\n\nLink\n\n\n\n\n15\n\n\n16 May\n\n\nFriday\n\n\n13:10-13:35\n\n\nA-21\n\n\nTBA\n\n\nLink"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact us",
    "section": "",
    "text": "The Data Center student fellows hold office hours in the Data Center’s office (Anne ground floor) to answer questions and help with assignments. Drop by to office hours if you’re looking for some data science help, whether for a data encounter in a course, another UCR course, or your SEPR!\n\nOffice hour schedule\nOffice hours are held in the Data Center’s office (Anne ground floor), every week during the semester between week 4 and week 15. You are welcome to drop by any time during the office hours, no appointment needed.\nOffice hours are with the current Data Center interns; you can learn more about their specific expertise below the schedule.\n\n\n\n\n\nMonday\nTuesday\nWednesday\nThursday\nFriday\n\n\n\n\n9:00-10:00\n\n\n\n\n\n\n\n10:00-11:00\n\n\n\nMichel\n\n\n\n11:00-12:00\nJulia\n\n\n\nLili\n\n\n12:00-13:00\n\n\n\n\nGabriela\n\n\n13:00-14:00\n\n\n\n\n\n\n\n14:00-15:00\n\nMare\n\n\n\n\n\n15:00-16:00\n\n\nMichel\n\n\n\n\n16:00-17:00\nClement\n\n\n\n\n\n\n17:00-18:00\nMichel\n\n\n\n\n\n\n18:00-19:00\n\n\n\n\n\n\n\n\n\nClement: Spatial data, rasters, maps, Ecology\nGabriela: R, Python, Stata, SQL (soon), statistics, visualization, data tidying and transformation, data import/export, spatial data\nJulia: R, data cleaning and wrangling, data visualization, statistics\nLili: R, text analysis, character strings\nMare: Basic R and Python; Working with Corpora / Text analysis\nMichel: Data Science in R: Data wrangling, spatial data, data visualizations and simple text analysis\n\n\n\nContact us\nIf you would like to request an individual consultation, or would like to know more about becoming an intern for the Data Center, feel free to send an email to datacenter@ucr.nl.\nMake sure to check this website for news and updates, and follow us on Instagram!"
  },
  {
    "objectID": "tutorials_old.html",
    "href": "tutorials_old.html",
    "title": "Tutorials",
    "section": "",
    "text": "Setting up R\nStart here if you have never used R before.\n\nInstalling R\nGetting started with R and RStudio\nObjects and functions\n\n\n\nIntroduction to R\nIf you can find your way around R and RStudio, but want to learn more about the basics. Most of these tutorials can be used independently of each other. Any prerequisites are mentioned in the introduction of each tutorial.\n\nImporting data\nData wrangling: filtering rows and selecting columns\nData wrangling: creating new variables\nData wrangling: summarizing data (within groups)\nPivoting: data in wide and long format\nThe tidy workflow\nData visualization: introduction to ggplot2\nData visualization: distributions\nData visualization: relationships between variables\nData visualization: patterns over time\n\n\n\nR topics for beginners\nIf you’re familiar with basic data cleaning and visualization, but want to learn more about specific topics that are relevant to your disciplinary background or interests. The tutorials in this section can be used independently of each other.\n\nWorking with spatial data\nWorking with text\nStatistics in R\n\n\n\nAdvanced R topics\nIf you have more experience with R (e.g. ENGDATA101) and want to learn more. These tutorials are particularly useful for students in the Data Center Apprenticeship program.\n\nMore on importing data\nMore on data visualization\nText analysis with regular expressions\nWeb and pdf scraping\nInteractive applications with Shiny\nWriting your own functions\nAdvanced data types and functionals\nNeuroimaging in R\n\n\n\nReproducible research workflows\nIf you want to learn more about how to make your workflows more reproducible and efficient.\n\nVersion control with GitHub\nCollaborating on GitHub\nCompile documents with RMarkdown/Quarto\nWrite papers in LaTeX (with Overleaf)\n\n\n\nOther\n\nUseful data sources\nAdditional materials"
  },
  {
    "objectID": "tutorials/data.html",
    "href": "tutorials/data.html",
    "title": "Useful data sources",
    "section": "",
    "text": "If you have suggestions for adding more resources, please email us at datacenter@ucr.nl."
  },
  {
    "objectID": "tutorials/data.html#ecology",
    "href": "tutorials/data.html#ecology",
    "title": "Useful data sources",
    "section": "Ecology",
    "text": "Ecology\n\nGlobal Biodiversity Information Facility (gbif): citizen science based open-access database containing information on species occurrences all across the world\nEDI Data Portal: collection of environmental data and metadata derived from publicly funded research\nEcological Data Wiki: Wikipedia of ecology data\nInternational Institute for Sustainable Development- Experimental Lakes Area (IISD-ELA): biological, environmental and meteorological data on freshwater lakes and watersheds\nEuropean Soil Data Centre (ESDAC): biological, physical, and chemical information on the European soils\nDRYAD: open-access research data\neBird: data collection on bird distribution, abundance, habitat use, and trends\nwaterinfo: physical and chemical data on water in the Netherlands\nBritish Antarctic Survey: database on agriculture, atmosphere, biosphere, climate indicators, cryosphere, human dimension, hydrosphere, land surface, oceans, paleoclimate, solid earth and sun-earth interactions in the antarctic region\nWorldClim: database of high resolution global weather and climate data including future predictions (spatial data)\nLand-Use Harmonization: database of high resolution land use information (spatial data)"
  },
  {
    "objectID": "tutorials/data.html#economics",
    "href": "tutorials/data.html#economics",
    "title": "Useful data sources",
    "section": "Economics",
    "text": "Economics\n\nTotal Economy Database: data on GDP and labour statistics for 131 countries (available after creating a free account)\nFRED (US Federal Reserve): mostly US economic data\nMaddison Historical Statistics: GDP per capita and population data for 169 countries between 1000–2018\nDBnomics: collection of publicly available economics data"
  },
  {
    "objectID": "tutorials/data.html#political-science",
    "href": "tutorials/data.html#political-science",
    "title": "Useful data sources",
    "section": "Political Science",
    "text": "Political Science\n\nV-Dem: country and party level measures of democracy (global)\nParlGov: party and election information (mainly EU)\nEU Open Data Portal: various EU-level data\nCHORUS: large dataset of interest group positions"
  },
  {
    "objectID": "courses/archive/SCICOGN302_2024h2.html",
    "href": "courses/archive/SCICOGN302_2024h2.html",
    "title": "SCICOGN302 Psycholinguistics",
    "section": "",
    "text": "This page collects materials for the data-driven assignments in the Fall 2024 edition of Psycholinguistics at UCR. You can also access the related files directly on Github.\nData Center office hours will be announced soon; to schedule an individual meeting, please email ucrdatacenter@ucr.nl."
  },
  {
    "objectID": "courses/archive/SCICOGN302_2024h2.html#homework-assignment",
    "href": "courses/archive/SCICOGN302_2024h2.html#homework-assignment",
    "title": "SCICOGN302 Psycholinguistics",
    "section": "Homework assignment",
    "text": "Homework assignment\nUse what you learned in the Data Center workshops to complete the following assignment.\nThe CHILDES database provides data on the use of (mostly spoken) language of a large number of young children. Use parts of the dataset to explore how the spoken language (word choice or number of words, for instance) of children changes over time . Keep your question simple, focus on a small part of the dataset (one or a few children). Make one clear visualization of your result and briefly report on (max 300 words) the research question and your outcome. Also briefly discuss your result in the context of scientific literature. Cite at least two papers (the latter are not part of 300 words)."
  },
  {
    "objectID": "tutorials/r_stats.html",
    "href": "tutorials/r_stats.html",
    "title": "Statistics in R",
    "section": "",
    "text": "This document provides an overview of the basic statistical functions in R, including descriptive statistics and summary tables.\nThis tutorial shows you examples of using statistical methods on the diamonds dataset, which comes pre-loaded with the tidyverse package.\nLet’s load the tidyverse package and have a look at the diamonds dataset:\n\nlibrary(tidyverse)\ndata(diamonds)"
  },
  {
    "objectID": "tutorials/r_stats.html#summaries-in-the-r-console",
    "href": "tutorials/r_stats.html#summaries-in-the-r-console",
    "title": "Statistics in R",
    "section": "Summaries in the R Console",
    "text": "Summaries in the R Console\nTo get a descriptive statistic of a single variable in a tibble, we can use that variable as an argument to a relevant function (using $ to refer to a variable in a tibble).\n\nmean(diamonds$price)\nmedian(diamonds$price)\nsd(diamonds$price)\n\nTo get the frequencies of a categorical variable, we can use the count() function, with the sort = TRUE argument returning the values in descending frequency. count() is a tidy function that works well with pipe workflows and can count the joint frequencies of multiple variables.\n\n# frequencies of a single variable\ncount(diamonds, cut)\n\n# joint frequency distribution\ncount(diamonds, cut, color)\n\nTo get the correlation coefficient between two variables, we can use the cor() function in the same way we used other descriptives such as mean().\n\ncor(diamonds$price, diamonds$carat)\n\nThe easiest way to get summary statistics of all variables in a tibble is with the summary() function: this function shows the distribution of numeric variables, the frequencies of categorical variables, and the number of missing values for each variable.\n\nsummary(diamonds)"
  },
  {
    "objectID": "tutorials/r_stats.html#publication-ready-summaries-with-gtsummary",
    "href": "tutorials/r_stats.html#publication-ready-summaries-with-gtsummary",
    "title": "Statistics in R",
    "section": "Publication-ready summaries with gtsummary",
    "text": "Publication-ready summaries with gtsummary\nThe summary() function is useful for viewing the data in the Console, but doesn’t export to outside of R nicely. There are a few packages available for generating simple summary statistics tables that contain information about the central tendencies and dispersion of the data that also export nicely, such as gtsummary.\n\nlibrary(gtsummary)\n\n# tbl_summary() creates a summary table of the data\ntbl_summary(diamonds)\n\nYou can stratify your summary by a grouping variable using the by argument:\n\ntbl_summary(diamonds, by = cut)\n\n# add p-value of difference between groups\ntbl_summary(diamonds, by = cut) |&gt; \n  add_p()\n\nYou can also customize the appearance of the table.\n\ntbl_summary(diamonds, by = cut) |&gt; \n  modify_header(label ~ \"Variable\") |&gt; \n  modify_caption(\"Summary Table by Cut\")\n\nTo export the table as a Word document, use the gtsave() function. Note that we first use the as_gt() function to convert the tbl_summary() output to a gt object, and load the gt package in order to use the Word export function defined for the gt package.\n\nlibrary(gt)\n\ntbl_summary(diamonds, by = cut) |&gt; \n  as_gt() |&gt; \n  gtsave(\"summary_table.docx\")\n\nIf you use LaTeX, you can also export as a LaTeX table, also relying on the gt package.\n\ntbl_summary(diamonds, by = cut) |&gt; \n  as_gt() |&gt; \n  gtsave(\"summary_table.tex\")"
  },
  {
    "objectID": "tutorials/r_stats.html#t-tests",
    "href": "tutorials/r_stats.html#t-tests",
    "title": "Statistics in R",
    "section": "t-tests",
    "text": "t-tests\nWe can run one and two samples t-tests to evaluate group means with the t.test() function. The function supports various options and model specifications: a simple one-sample t-test only requires specifying the variable of interest, either with x = data$variable or x = variable, data = data syntax. For two-sample t-tests, we can use the formula syntax y ~ x to specify the dependent and independent variables or the x and y (and optionally data) arguments. Additional options include specifying the alternative hypothesis, the confidence level, the value of \\(\\mu\\), and whether we want a paired t-test and assume equal variances. Helper functions such as tidy() convert the Console output to an easy-to-export tibble of results.\nTo demonstrate two-sample t-tests, we define a subset of the data that contains only two possible values of cut.\n\n# simple t-test (H0: mean=mu)\nt.test(diamonds$carat, mu = 1)\n\n# define data subsample of fair and good diamonds to have only two groups of cut\ndiamonds_sub &lt;- diamonds |&gt; \n  filter(cut %in% c(\"Fair\", \"Good\"))\n\n# can also use data argument instead of data$...\n# price ~ cut is formula specification: variable ~ group\n# H0: fair and good diamonds have the same average price\nt.test(price ~ cut, alternative = \"greater\", data = diamonds_sub)\n\n# tidy() function turns results into a tibble\nt.test(price ~ cut, alternative = \"greater\", data = diamonds_sub) |&gt; tidy()"
  },
  {
    "objectID": "tutorials/r_stats.html#correlation-test",
    "href": "tutorials/r_stats.html#correlation-test",
    "title": "Statistics in R",
    "section": "Correlation test",
    "text": "Correlation test\nThe cor.test() function calculates the correlation between two variables. Again, the function supports various specifications: x and y arguments, formula syntax (see below for an example), and the data argument.\n\ncor.test( ~ price + carat, data = diamonds)\n\n# tidy() function turns results into a tibble\ncor.test( ~ price + carat, data = diamonds) |&gt; tidy()"
  },
  {
    "objectID": "tutorials/r_stats.html#simple-regression",
    "href": "tutorials/r_stats.html#simple-regression",
    "title": "Statistics in R",
    "section": "Simple regression",
    "text": "Simple regression\nA key building block of statistical analysis is linear regression. The lm() function fits a linear model to the data, with a wide range of specifications, passed as the formula argument (first argument if unnamed). The formula syntax is y ~ x, where y is the dependent variable and x is the independent variable. Again, optional function arguments allow for a lot of customization, but the default settings are sufficient for most applications. Helper functions such as tidy() and summary() provide extensive summaries of the model fit and coefficients, and tbl_regression() from the gtsummary package creates neat tables of the results. Assigning the result of a model to an object saves computational time, as then we can work with the results without having to re-run the analysis every time.\n\n# assign outcome to object\nfit &lt;- lm(price ~ carat, data = diamonds)\n\n# extensive result summary\nfit |&gt; summary()\n\n# tidy coefficients\nfit |&gt; tidy()\n\n# display-ready table with gtsummary\ntbl_regression(fit)"
  },
  {
    "objectID": "tutorials/r_stats.html#multiple-regression",
    "href": "tutorials/r_stats.html#multiple-regression",
    "title": "Statistics in R",
    "section": "Multiple regression",
    "text": "Multiple regression\nMultiple regression extends simple regression to multiple independent variables. The only difference is the formula specification, which now connects multiple independent variables with + signs. The formula specification also allows for interactions between variables, which can be specified with * (if the main effects should be included) or : (for only the interaction term). The DV ~ .~ syntax includes all variables in the data except the dependent variable as independent variables.\n\nlm(price ~ x + y + z + table + depth, data = diamonds) |&gt; summary()\n\n# all variables in data\nlm(price ~ ., data = diamonds) |&gt; summary()\n\n# interactions\nlm(price ~ x * y, data = diamonds) |&gt; summary()"
  },
  {
    "objectID": "tutorials/r_stats.html#anova",
    "href": "tutorials/r_stats.html#anova",
    "title": "Statistics in R",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of variance (ANOVA) is a generalization of the t-test to multiple groups. The aov() function fits an ANOVA model to the data, with the formula syntax y ~ x, where y is the dependent variable and x is the independent variable. The same helper functions as with lm() can be used to summarize the results.\nNote that ANOVA is a specific case of a linear regression model, so the results are equivalent to those of a linear regression model with a categorical independent variable.\n\nanova_fit &lt;- aov(price ~ cut, data = diamonds)\n\nsummary(anova_fit)\ntidy(anova_fit)\n\n# equivalent regression\nlm(price ~ cut, data = diamonds) |&gt; summary()"
  },
  {
    "objectID": "tutorials/r_text.html",
    "href": "tutorials/r_text.html",
    "title": "Working with text",
    "section": "",
    "text": "This tutorial introduces how to treat text as data in R using the tidytext package. It introduces methods of importing text, tokenizing text, looking for (partial) matches with simple regular expressions, and analyzing word frequencies.\nWe start by installing and loading the tidytext package, and loading the tidyverse package.\n\n# install.packages(\"tidytext\")\nlibrary(tidytext)\nlibrary(tidyverse)"
  },
  {
    "objectID": "tutorials/r_text.html#counting-word-frequencies",
    "href": "tutorials/r_text.html#counting-word-frequencies",
    "title": "Working with text",
    "section": "Counting word frequencies",
    "text": "Counting word frequencies\nThe simplest method of getting a quick overview of a long text is to count the number of times each word appears in the text, and looking at what the most frequent words are. We can get these word frequencies using the count() function, specifying which variable we want to count.\n\nclean_text |&gt; \n  count(word)\n\n# A tibble: 60 × 2\n   word         n\n   &lt;chr&gt;    &lt;int&gt;\n 1 academic     1\n 2 aiding       1\n 3 analysis     1\n 4 analyze      1\n 5 and          5\n 6 by           1\n 7 can          2\n 8 content      1\n 9 customer     1\n10 data         1\n# ℹ 50 more rows\n\n\nThe count() function has an argument sort, which allows us to sort the output from most frequent to least frequent words.\n\nclean_text |&gt; \n  count(word, sort = TRUE)\n\n# A tibble: 60 × 2\n   word          n\n   &lt;chr&gt;     &lt;int&gt;\n 1 and           5\n 2 can           2\n 3 sentiment     2\n 4 trends        2\n 5 academic      1\n 6 aiding        1\n 7 analysis      1\n 8 analyze       1\n 9 by            1\n10 content       1\n# ℹ 50 more rows"
  },
  {
    "objectID": "tutorials/r_text.html#looking-for-exact-and-partial-word-matches",
    "href": "tutorials/r_text.html#looking-for-exact-and-partial-word-matches",
    "title": "Working with text",
    "section": "Looking for exact and partial word matches",
    "text": "Looking for exact and partial word matches\nIn many cases we are interested only in analyzing parts of a text that contain our topic of interest. For example, we may want to find which parts of a text talk about “data” and in what context. In that case, we can use the filter() function to keep only observations that meet a particular criteria. You can see some more explanation and general examples of the filter() function in this tutorial.\nWhen we are working with words, exact matches are often enough for our purposes. For example, we can look at how many rows in our clean_text tibble have “data” as the value of the word variable.\n\nclean_text |&gt; \n  filter(word == \"data\")\n\n# A tibble: 1 × 1\n  word \n  &lt;chr&gt;\n1 data \n\n\nHowever, this filter tells us nothing about the context in which “data” appears. For that, it would be better to split the text into sentences, and find which sentence contains the word “data”. But if we tokenize the text into sentences, an exact match won’t find the sentence we’re looking for.\n\n# split the text into sentences\nsentences &lt;- raw_text |&gt; \n  as_tibble() |&gt; \n  unnest_tokens(output = sentence, input = value, token = \"sentences\")\n\n# keep only rows with an exact match to \"data\" (no such rows)\nsentences |&gt; \n  filter(sentence == \"data\")\n\n# A tibble: 0 × 1\n# ℹ 1 variable: sentence &lt;chr&gt;\n\n\nIf we want to find a partial string match (i.e. a sentence that among other content contains the word “data”), we need to use a special function to detect partial matches. This function is called str_detect() and takes the arguments of the variable that contains the elements you want to evaluate and the pattern you’re looking for. In our case, this variable is sentence and the pattern is data. str_detect() returns a logical vector, i.e. for each element of your variable it tells you whether it matches the pattern (TRUE) or not (FALSE).\n\n# example of str_detect()\nstr_detect(string = c(\"A\", \"AB\", \"BB\"), pattern = \"A\")\n\n[1]  TRUE  TRUE FALSE\n\n# look for partial match to \"data\" (one sentence)\nsentences |&gt; \n  filter(str_detect(string = sentence, pattern = \"data\"))\n\n# A tibble: 1 × 1\n  sentence                                                                      \n  &lt;chr&gt;                                                                         \n1 text analysis in r provides valuable insights by uncovering patterns, trends,…"
  },
  {
    "objectID": "tutorials/neuroimaging.html",
    "href": "tutorials/neuroimaging.html",
    "title": "Neuroimaging in R",
    "section": "",
    "text": "This document gives a basic tutorial on how to work with structural MRI data stored in NIfTI files. The first half gives a recap of the theory behind MRI machines and what kind of data we are actually working with. The second half is about visualizing neuroimaging data and manipulating it to draw correlation between disease severity and pathological manifestation (more specifically lesion size), by using the dataset including people with post-stroke aphasia. Hope you find it interesting and this tutorial inspires you to explore the topic more! ☺️\n\n\nTo start working with neuroimaging, it’s important to understand the basics of MRI and what the images we see actually represent. The following text is based on Berger’s explanation and Azhar & Chong’s paper on MRI imaging (Azhar & Chong, 2023; Magnetic Resonance Imaging - PMC, n.d.). Feel free to check these out if something remains unclear!\nMagnetic Resonance Imaging (MRI) is a technique used to visualize the internal structures of the body, and in our case, the brain. What we’re trying to do is quantify the properties of different tissues, such as gray matter, white matter, cerebrospinal fluid (CSF), or lesions. To achieve this, we use weighted MRI images. These images don’t have absolute or objective brightness values; instead, their intensities are relative and depend on how the MRI sequence is set up. By using several types of weighted images, we can extract meaningful information.\nMRI works because water is magnetic, and our bodies are about 60% water. Each water molecule has two hydrogen atoms, and each hydrogen atom has a single proton at its nucleus, surrounded by one electron. These protons have a property called “spin” which gives a tiny magnetic field – like a miniature bar magnet.\nUnder normal conditions, the magnetic fields of these protons are randomly oriented. But when someone is placed in the strong magnetic field of an MRI scanner, more of these protons align with or against the field. The protons that align with the magnetic field are said to be in a “low-energy state”, while the protons that align against the magnetic field are said to be in a “high-energy state”. However, there is an imbalance, as more protons are in the low-energy state (because this is more energetically favorable), so the magnetic fields of the protons aligned with the field don’t get completely cancelled out by the protons aligned against it. What is left, creates a net magnetization vector in the direction of the MRI scanner’s magnetic field. This is what the MRI uses as a “starting point”.\nThe MRI scanner then uses a technique called resonance to gather information. It sends out radiofrequency (RF) pulses to the low-energy protons, causing them to flip to the high-energy state. When the RF pulse is turned off, the protons gradually return to their original alignment. As they do, they release a tiny amount of electromagnetic energy, which is picked up by RF coils placed around the body. These signals differ based on the type of tissue, the amount of water or fat it contains, and how the molecules behave. Stronger signals are associated with tissues that have more hydrogen atoms, like in areas of edema or inflammation, while the timing of the signal tells us about the environment and state of the tissue.\nTwo important measurements in MRI are T1 and T2 relaxation times, which create contrast in the MRI image:\n\nT1 relaxation – How quickly protons realign with the magnetic field\nT2 relaxation – How quickly protons fall out of sync (lose phase coherence) with each other after the RF pulse, when protons are spinning together in coherence\n\nThese two properties are used to generate T1-weighted and T2-weighted images, which emphasize different tissue characteristics.\nT1-weighted imagesprovide a clear picture of brain anatomy and is excellent for structural imaging. When a contrast agent is used, active or inflamed lesions can become more visible. On T1 images, fat and suba-cute blood typically appear bright, while fluids like CSF, edema, and lesions appear dark. You can think of T1 like viewing the brain under regular lighting—it shows you structure and clarity.\nT2-weighted images aremore sensitive to water content and is particularly good for spotting fluid, inflammation, or other pathological changes. In T2 images, things like CSF, edema, cysts, and lesions appear bright, while fat, calcifications, and bone tend to appear dark. You might imagine T2 as looking at the brain under blacklight—suddenly all the ‘wet’ or abnormal areas light up.\nSometimes, T2 images are modified using a sequence called FLAIR (Fluid-Attenuated Inversion Recovery). CSF appears very bright on T2, which can make it hard to see subtle lesions near the ventricles or cortex. FLAIR solves this by suppressing the CSF signal, allowing small lesions to stand out while maintaining the T2 sensitivity of surrounding tissues. This makes FLAIR ideal for spotting MS plaques, small metastases, or cortical strokes. On FLAIR images, edema, demyelination, tumors, and strokes appear bright, while normal CSF appears dark—like turning off the glow from water to focus on hidden areas.\nSee Figure 1. for a visual representation on how T1, T2 and FLAIR images appear from an MRI scan.\nThere are of course also other types of sequences, but in this tutorial we will be mainly focusing on structural MRI and more specifically the T1 and T2 weighted images.\n\n\n\nFigure 1. MRI of a patient with acute hypoglycemia that mimicked symptoms of acute ischemic stroke, but did not show the usual pathologies on the brain scan. (A) T1 weighted image. Note that the CSF, appearing dark in the ventricles and between the meninges (B) T2 weighted image. Note the CSF appearing bright. (C) FLAIR image. Note that the CSF no longer appears bright, but dark and the signal is less exaggerated.\n\n\nNote: the picture was taken from: Rodriguez-Hernandez A, Babici D, Campbell M, Carranza-Reneteria O, Hammond T. Hypoglycemic hemineglect a stroke mimic. eNeurologicalSci. 2023 Jan 17;30:100444.\nNow that we have the basics, let’s try to understand what kind of data is actually portrayed on these MRI scans and what we can do with them.\nAn MRI picture is like a 3D map of our brain, which is built from approx. 7 million of tiny building blocks, called voxels. A voxel is a 3D pixel. It’s a little cube of brain tissue, and the MRI machine measures how water behaves in it and assigns it a number. That number becomes the shade of gray in the picture. Dark areas have low numbers (less signal), while bright areas have high numbers (more signal). \nHowever, whether something appears bright or dark depends on the contrast type (so if it’s T1 or T2 for example). What changes between T1 and T2 is what kind of tissue gives off a strong or weak signal. For example: in T1 the CSF appears dark, meaning it has a lower signal, and a lower number assigned. On the other hand, in the T2 the CSF appears bright, meaning it has a higher signal, and a higher number assigned. See Figure 2. of a single, axial slice of the brain, showing how to imagine the portrayal of the “data” on these MRI pictures. Although, here a single slice is presented for better understanding, keep in mind that we usually look at the whole of the brain, which is made up of many-many different slices. \n\n\n\nFigure 2. Voxels with assigned numbers corresponding to the highlighted area on an MRI axial scan of the brain. Note, that the areas appearing darker on the image have lower numbers, while areas that appear lighter have higher numbers.\n\n\nNote: this picture was taken from the video: Elizabeth Sweeney -  Neuroimaging Analysis in R [Internet]. 2019 [cited 2025 Apr 18]. Available from: https://www.youtube.com/watch?v=9HkEq01nrco\nIn this tutorial we will be working with NIfTI files (in ‘nii.’ or ‘nii.gz’ formats), which is the standard file format for storing MRI brain imaging data (both the image data and metadata, like voxel size, orientation etc.). It is a 3D array (X, Y, Z), where:\n\nX is the width (left to right in the brain)\nY is the depth (front/back (anterior/posterior))\nand Z is the height (up/down (superior/inferior).\n\nLet’s say that we want to take a slice of the brain at the 125th index “deep” in the brain, including its whole width and height (creating a coronal view). Figure 3. shows how we can do this, and how you should imagine this in the 3D plane. \n\n\n\nFigure 3. Visual representation about the different axes of the 3D arrays of NIfTI files. The axes are denoted in black. When we call these different slices we use a format like this: data [X, Y, Z]. If we want to include the whole width and height, we can just leave a comma at the place of X and Z. So, the red section represents the 125th slice along the Y axis (which is the axis of anterior-to-posterior), including its whole width and whole height (so represented as: [, 125 ,])\n\n\nFor future reference, the different axes of the brain are also visualized in the figure below. It’s good to have this on hand when working with neuroimaging.\n\n\n\n\nFor this tutorial we will be using the Aphasia Recovery Cohort (ARC) Dataset, which is a collection of data from multiple studies conducted over several years. This is a large, open-access neuroimaging dataset, that contains longitudinal data from 230 individuals with post-stroke aphasia. The dataset includes multimodal MRI data (T1, T2, FLAIR, fMRI, DWI),  detailed behavioral assessments (via the Western Aphasia Battery (WAB)) and demographic information. The dataset contains scanning sessions across different time points, ranging from days to years post-stroke. \nEnrollment requirements included individuals who had experienced a left-hemisphere stroke at least 6 months (or 12 months for some studies) prior to enrollment, between the ages of 21 and 80 years old, with no contraindications to MRI or additional neurological impairments (such as multiple sclerosis, Parkinson’s, dementia, etc). It is important to note that many individuals participated in multiple studies. \nIf you would like to learn more about a specific aspect of the study, see the link taking you to the publication. \nHowever, the scans that are included in the ARC dataset are raw MRI scans. This means that the scans came directly from the scanner, with no preprocessing applied. This means that the images are still in the original orientation and the position that the person’s head was during the scan. Because of this and also the variability of brains between individuals makes it difficult to compare multiple brains. Therefore, the MRI scans have to be “spatially normalized”, meaning that they have to be transformed to match a standard template (a common reference brain). Additionally, these images still include non-brain structures like the skull, which provide unnecessary data and thus interfere with our data analysis.\nIf you are new to neuroimaging, working with raw data can be tricky, because it often need a lot of pre-processing before it can used in analysis or visualization.\nLuckily, the researchers behind the ARC dataset provided post-processed images for educational purposes that can be accessed through this link. These images include:\n\nnormalized brain scans, where each scan has been aligned to a standard template,\nlesion masks, which highlight the damaged brain regions in each participant’s brain,\nlesion incidence maps, which show which brain regions are most affected across the whole group.\n\nThese post-processed images are what we’ll be using for the next steps of the tutorial, as they let us focus on interpreting results rather than struggling with preprocessing.\n\n\n\nWe will be using these packages:\n\noro.nifti - This package is the core R package for working with NifTI files. It writes and loads NifTI files into R as 3D or 4D arrays. It’s basic functions include: readNIfTI() and writeNifTI()\nneurobase - This package is a part of the Neuroconductor project, which hosts a collection of R packages specifically for neuroimaging analysis, building on R’s existing ecosystem. It builds on oro.nift, and adds helpful functions for image math, masks, plotting, etc. \n\nThe earlier mentioned Neuroconductor system can also come in handy later for your future projects. If you type this in R you will be able to download and run an R script hosted on the Neuroconductor website:\n\nsource(\"https://neuroconductor.org/neurocLite.R\")\n\nThat script defines the neuro_install() function, which can be used to install any Neuroconductor package like this:\n\nneuro_install(\"neurobase\")\n\nSo, let’s get started!\nAs stated earlier, we will be using the post-processed images. For now let’s use the NifTi file that is called “wbsub-M2001_ses-1253x1076_T1w.nii.gz” and you will find it under ‘files’ within the project’s folder.\nHere is what each part of this file name means:\n\n‘sub-M2001’ corresponds to the subject ID\n‘ses-1253x1076’ means that they likely combined scans from session 1253 and session 1076. The numbers refer to how many days after the stroke the given imaging session took place, so in this case it was 1076 and 1253 days post-stroke\n‘T1w’ means the scan type."
  },
  {
    "objectID": "tutorials/neuroimaging.html#before-we-begin-a-little-recap-on-mri",
    "href": "tutorials/neuroimaging.html#before-we-begin-a-little-recap-on-mri",
    "title": "Neuroimaging in R",
    "section": "",
    "text": "To start working with neuroimaging, it’s important to understand the basics of MRI and what the images we see actually represent. The following text is based on Berger’s explanation and Azhar & Chong’s paper on MRI imaging (Azhar & Chong, 2023; Magnetic Resonance Imaging - PMC, n.d.). Feel free to check these out if something remains unclear!\nMagnetic Resonance Imaging (MRI) is a technique used to visualize the internal structures of the body, and in our case, the brain. What we’re trying to do is quantify the properties of different tissues, such as gray matter, white matter, cerebrospinal fluid (CSF), or lesions. To achieve this, we use weighted MRI images. These images don’t have absolute or objective brightness values; instead, their intensities are relative and depend on how the MRI sequence is set up. By using several types of weighted images, we can extract meaningful information.\nMRI works because water is magnetic, and our bodies are about 60% water. Each water molecule has two hydrogen atoms, and each hydrogen atom has a single proton at its nucleus, surrounded by one electron. These protons have a property called “spin” which gives a tiny magnetic field – like a miniature bar magnet.\nUnder normal conditions, the magnetic fields of these protons are randomly oriented. But when someone is placed in the strong magnetic field of an MRI scanner, more of these protons align with or against the field. The protons that align with the magnetic field are said to be in a “low-energy state”, while the protons that align against the magnetic field are said to be in a “high-energy state”. However, there is an imbalance, as more protons are in the low-energy state (because this is more energetically favorable), so the magnetic fields of the protons aligned with the field don’t get completely cancelled out by the protons aligned against it. What is left, creates a net magnetization vector in the direction of the MRI scanner’s magnetic field. This is what the MRI uses as a “starting point”.\nThe MRI scanner then uses a technique called resonance to gather information. It sends out radiofrequency (RF) pulses to the low-energy protons, causing them to flip to the high-energy state. When the RF pulse is turned off, the protons gradually return to their original alignment. As they do, they release a tiny amount of electromagnetic energy, which is picked up by RF coils placed around the body. These signals differ based on the type of tissue, the amount of water or fat it contains, and how the molecules behave. Stronger signals are associated with tissues that have more hydrogen atoms, like in areas of edema or inflammation, while the timing of the signal tells us about the environment and state of the tissue.\nTwo important measurements in MRI are T1 and T2 relaxation times, which create contrast in the MRI image:\n\nT1 relaxation – How quickly protons realign with the magnetic field\nT2 relaxation – How quickly protons fall out of sync (lose phase coherence) with each other after the RF pulse, when protons are spinning together in coherence\n\nThese two properties are used to generate T1-weighted and T2-weighted images, which emphasize different tissue characteristics.\nT1-weighted imagesprovide a clear picture of brain anatomy and is excellent for structural imaging. When a contrast agent is used, active or inflamed lesions can become more visible. On T1 images, fat and suba-cute blood typically appear bright, while fluids like CSF, edema, and lesions appear dark. You can think of T1 like viewing the brain under regular lighting—it shows you structure and clarity.\nT2-weighted images aremore sensitive to water content and is particularly good for spotting fluid, inflammation, or other pathological changes. In T2 images, things like CSF, edema, cysts, and lesions appear bright, while fat, calcifications, and bone tend to appear dark. You might imagine T2 as looking at the brain under blacklight—suddenly all the ‘wet’ or abnormal areas light up.\nSometimes, T2 images are modified using a sequence called FLAIR (Fluid-Attenuated Inversion Recovery). CSF appears very bright on T2, which can make it hard to see subtle lesions near the ventricles or cortex. FLAIR solves this by suppressing the CSF signal, allowing small lesions to stand out while maintaining the T2 sensitivity of surrounding tissues. This makes FLAIR ideal for spotting MS plaques, small metastases, or cortical strokes. On FLAIR images, edema, demyelination, tumors, and strokes appear bright, while normal CSF appears dark—like turning off the glow from water to focus on hidden areas.\nSee Figure 1. for a visual representation on how T1, T2 and FLAIR images appear from an MRI scan.\nThere are of course also other types of sequences, but in this tutorial we will be mainly focusing on structural MRI and more specifically the T1 and T2 weighted images.\n\n\n\nFigure 1. MRI of a patient with acute hypoglycemia that mimicked symptoms of acute ischemic stroke, but did not show the usual pathologies on the brain scan. (A) T1 weighted image. Note that the CSF, appearing dark in the ventricles and between the meninges (B) T2 weighted image. Note the CSF appearing bright. (C) FLAIR image. Note that the CSF no longer appears bright, but dark and the signal is less exaggerated.\n\n\nNote: the picture was taken from: Rodriguez-Hernandez A, Babici D, Campbell M, Carranza-Reneteria O, Hammond T. Hypoglycemic hemineglect a stroke mimic. eNeurologicalSci. 2023 Jan 17;30:100444.\nNow that we have the basics, let’s try to understand what kind of data is actually portrayed on these MRI scans and what we can do with them.\nAn MRI picture is like a 3D map of our brain, which is built from approx. 7 million of tiny building blocks, called voxels. A voxel is a 3D pixel. It’s a little cube of brain tissue, and the MRI machine measures how water behaves in it and assigns it a number. That number becomes the shade of gray in the picture. Dark areas have low numbers (less signal), while bright areas have high numbers (more signal). \nHowever, whether something appears bright or dark depends on the contrast type (so if it’s T1 or T2 for example). What changes between T1 and T2 is what kind of tissue gives off a strong or weak signal. For example: in T1 the CSF appears dark, meaning it has a lower signal, and a lower number assigned. On the other hand, in the T2 the CSF appears bright, meaning it has a higher signal, and a higher number assigned. See Figure 2. of a single, axial slice of the brain, showing how to imagine the portrayal of the “data” on these MRI pictures. Although, here a single slice is presented for better understanding, keep in mind that we usually look at the whole of the brain, which is made up of many-many different slices. \n\n\n\nFigure 2. Voxels with assigned numbers corresponding to the highlighted area on an MRI axial scan of the brain. Note, that the areas appearing darker on the image have lower numbers, while areas that appear lighter have higher numbers.\n\n\nNote: this picture was taken from the video: Elizabeth Sweeney -  Neuroimaging Analysis in R [Internet]. 2019 [cited 2025 Apr 18]. Available from: https://www.youtube.com/watch?v=9HkEq01nrco\nIn this tutorial we will be working with NIfTI files (in ‘nii.’ or ‘nii.gz’ formats), which is the standard file format for storing MRI brain imaging data (both the image data and metadata, like voxel size, orientation etc.). It is a 3D array (X, Y, Z), where:\n\nX is the width (left to right in the brain)\nY is the depth (front/back (anterior/posterior))\nand Z is the height (up/down (superior/inferior).\n\nLet’s say that we want to take a slice of the brain at the 125th index “deep” in the brain, including its whole width and height (creating a coronal view). Figure 3. shows how we can do this, and how you should imagine this in the 3D plane. \n\n\n\nFigure 3. Visual representation about the different axes of the 3D arrays of NIfTI files. The axes are denoted in black. When we call these different slices we use a format like this: data [X, Y, Z]. If we want to include the whole width and height, we can just leave a comma at the place of X and Z. So, the red section represents the 125th slice along the Y axis (which is the axis of anterior-to-posterior), including its whole width and whole height (so represented as: [, 125 ,])\n\n\nFor future reference, the different axes of the brain are also visualized in the figure below. It’s good to have this on hand when working with neuroimaging."
  },
  {
    "objectID": "tutorials/neuroimaging.html#description-of-the-arc-dataset",
    "href": "tutorials/neuroimaging.html#description-of-the-arc-dataset",
    "title": "Neuroimaging in R",
    "section": "",
    "text": "For this tutorial we will be using the Aphasia Recovery Cohort (ARC) Dataset, which is a collection of data from multiple studies conducted over several years. This is a large, open-access neuroimaging dataset, that contains longitudinal data from 230 individuals with post-stroke aphasia. The dataset includes multimodal MRI data (T1, T2, FLAIR, fMRI, DWI),  detailed behavioral assessments (via the Western Aphasia Battery (WAB)) and demographic information. The dataset contains scanning sessions across different time points, ranging from days to years post-stroke. \nEnrollment requirements included individuals who had experienced a left-hemisphere stroke at least 6 months (or 12 months for some studies) prior to enrollment, between the ages of 21 and 80 years old, with no contraindications to MRI or additional neurological impairments (such as multiple sclerosis, Parkinson’s, dementia, etc). It is important to note that many individuals participated in multiple studies. \nIf you would like to learn more about a specific aspect of the study, see the link taking you to the publication. \nHowever, the scans that are included in the ARC dataset are raw MRI scans. This means that the scans came directly from the scanner, with no preprocessing applied. This means that the images are still in the original orientation and the position that the person’s head was during the scan. Because of this and also the variability of brains between individuals makes it difficult to compare multiple brains. Therefore, the MRI scans have to be “spatially normalized”, meaning that they have to be transformed to match a standard template (a common reference brain). Additionally, these images still include non-brain structures like the skull, which provide unnecessary data and thus interfere with our data analysis.\nIf you are new to neuroimaging, working with raw data can be tricky, because it often need a lot of pre-processing before it can used in analysis or visualization.\nLuckily, the researchers behind the ARC dataset provided post-processed images for educational purposes that can be accessed through this link. These images include:\n\nnormalized brain scans, where each scan has been aligned to a standard template,\nlesion masks, which highlight the damaged brain regions in each participant’s brain,\nlesion incidence maps, which show which brain regions are most affected across the whole group.\n\nThese post-processed images are what we’ll be using for the next steps of the tutorial, as they let us focus on interpreting results rather than struggling with preprocessing."
  },
  {
    "objectID": "tutorials/neuroimaging.html#description-of-the-libraries-and-packages.",
    "href": "tutorials/neuroimaging.html#description-of-the-libraries-and-packages.",
    "title": "Neuroimaging in R",
    "section": "",
    "text": "We will be using these packages:\n\noro.nifti - This package is the core R package for working with NifTI files. It writes and loads NifTI files into R as 3D or 4D arrays. It’s basic functions include: readNIfTI() and writeNifTI()\nneurobase - This package is a part of the Neuroconductor project, which hosts a collection of R packages specifically for neuroimaging analysis, building on R’s existing ecosystem. It builds on oro.nift, and adds helpful functions for image math, masks, plotting, etc. \n\nThe earlier mentioned Neuroconductor system can also come in handy later for your future projects. If you type this in R you will be able to download and run an R script hosted on the Neuroconductor website:\n\nsource(\"https://neuroconductor.org/neurocLite.R\")\n\nThat script defines the neuro_install() function, which can be used to install any Neuroconductor package like this:\n\nneuro_install(\"neurobase\")\n\nSo, let’s get started!\nAs stated earlier, we will be using the post-processed images. For now let’s use the NifTi file that is called “wbsub-M2001_ses-1253x1076_T1w.nii.gz” and you will find it under ‘files’ within the project’s folder.\nHere is what each part of this file name means:\n\n‘sub-M2001’ corresponds to the subject ID\n‘ses-1253x1076’ means that they likely combined scans from session 1253 and session 1076. The numbers refer to how many days after the stroke the given imaging session took place, so in this case it was 1076 and 1253 days post-stroke\n‘T1w’ means the scan type."
  },
  {
    "objectID": "tutorials/neuroimaging.html#loading-our-packages-and-files",
    "href": "tutorials/neuroimaging.html#loading-our-packages-and-files",
    "title": "Neuroimaging in R",
    "section": "Loading our packages and files",
    "text": "Loading our packages and files\nFirst you will have to install the packages and only then will you be able to load them in from the library:\n\ninstall.packages(\"oro.nifti\")\ninstall.packages(\"neurobase\")\ninstall.packages(\"tidyverse\") \n\n\nlibrary(oro.nifti) # handling NifTI images\nlibrary(neurobase) # has additional neuroimaging analysis tools\nlibrary(tidyverse) \n\nNext we will load in our NIfTI file from patient M2001. Since we are using one file for now, it is easier if you download it into your local folder, where your R file is also saved (! this is important). To load our file we will use the read NIfTI command from the oro.nifti package. You can download the file using this link.\n\nt1_img_M2001 &lt;- readNIfTI(\"wbsub-M2001_ses-1253x1076_T1w.nii\", reorient = TRUE)\n\nThe reorient = TRUE ensures that the image is aligned properly in a standard orientation, which is useful for consistency."
  },
  {
    "objectID": "tutorials/neuroimaging.html#checking-image-properties",
    "href": "tutorials/neuroimaging.html#checking-image-properties",
    "title": "Neuroimaging in R",
    "section": "Checking image properties",
    "text": "Checking image properties\nBefore we can do anything with pretty pictures, we have to inspect the properties of our NIfTI file. It’s good to know the physical size of the voxels, for accurate measurements of brain structures as well as understanding the orientation, so that the anatomical structures are correctly identified and the analyses are consistent across datasets.\nFirst, we can check the number of voxels along each axis. This helps us understand the resolution and size of the 3D image. Run this chunk of code and see the output:\n\ndim(t1_img_M2001) \n\nHowever, maybe it is smarter to have a comprehensive summary of the NIfTI object, including data type (each voxel’s intensity is stored as a 16-bit signed integer), dimensions, (number of voxels in X,Y and Z planes), pixel dimension (indicates how much each voxel measures) and voxel units.This provides a quick overview of the image’s metadata and structure.\n\nt1_img_M2001\n\nNext, we can also list all the metadata slots available in the NIfTI object, which allows us to see what additional information is stored and is accessible within the object. We can do this by typing this:\n\nslotNames(t1_img_M2001)\n\nFrom all of this the most important thing is that we know the dimension of the images: 157 × 189 × 156. This means that it comprises 157 voxels in the x-axis, 189 in the y-axis, and 156 in the z-axis."
  },
  {
    "objectID": "tutorials/neuroimaging.html#visualizing-our-brain-images",
    "href": "tutorials/neuroimaging.html#visualizing-our-brain-images",
    "title": "Neuroimaging in R",
    "section": "Visualizing our brain images",
    "text": "Visualizing our brain images\nNow let’s look at our image. We can do this two ways. If we want a quick visualization of our image, the orthographic() function from the oro.nift package displays NIfTI objects in an orthogonal view, across 3 different planes (axial, sagittal and coronal). This way of looking at our data is more neuroimaging-specific and offers a convenient way to inspect 3D images. The picture is also very cool, and could even be published! You will see the image under “Plots” in R. Run this code to see the visual output we get:\n\northographic(t1_img_M2001)\n\nThe other option is to use the image() function from the oro.nifti package. This shows a mosaic view with all slices in a given orientation from a brain MRI. It might take a few minutes to run the code below. Can you recognize which view these are oriented in?\n\nimage(t1_img_M2001)\n\nHere we can see that it portrays consecutive slices row-wise from inferior to superior. We can see that the brain has been well pre-processed. The brain structures are symmetrical and clean, and there is no visible skull tissue, indicating successful brain extraction. There is also a good contrast between the white and gray matter. Keep in mind that this simplifies the portrayal of the brain into axial slices, but in reality we have a 3D array of voxels from the MRI scan.\nIf we would like to extract a specific slice from the array we have to specify its position along the 3 axes. We can do this by using the image() function from the oro.nifti package.\nThis line of code displays the sagittal slice at index 125 (so we have to specify which slice we want to get along the X axis, representing the left-to-right dimension). See the description in the beginning with the 3D picture of the brain and the annotations (Figure 3). This can be especially useful for examining structures along the mid-line of the brain or assessing lateralized features.\n\nimage(t1_img_M2001[125,,]) \n# the commas represent that we include all indexes along X and Y axes\n\nThe X and Y axis on this picture that is produced actually translates as:\n\nX axis (on output) is actually the Y plane of the brain (so left-to-right)\nY axis (on output) is actually the Z plane of the brain (so front-to-back)"
  },
  {
    "objectID": "tutorials/neuroimaging.html#lesion-volume-calculation",
    "href": "tutorials/neuroimaging.html#lesion-volume-calculation",
    "title": "Neuroimaging in R",
    "section": "Lesion volume calculation",
    "text": "Lesion volume calculation\nLet’s say we would like to calculate the volume of the lesion this patient has. \nTo do this, we would need a lesion mask. A lesion mask is a binary image used in neuroimaging to identify and isolate regions of brain tissue that have been damaged due to conditions such as stroke. In this mask, each voxel (a 3D pixel) is assigned a value:​\n\n1 indicates the presence of a lesion\n0 indicates healthy or unaffected tissue\n\nThis allows for the calculation of lesion volume by summing the number of voxels labeled as lesions (so 1) and multiplying by the volume of each voxel.\nLesion masks can be created from the MRI images, however it has to be done manually with external tools, like FSLeyes. Unfortunately, R does not provide built-in tools for manual lesion delineation.\nAlthough it is very interesting to learn how to use these tools, we will be working with a lesion mask that is already provided for subject M2001. This is also available on the GitHub page with the processed images (in the ‘NIfTI’ folder, if you search for your subject with Ctrl+F there will be two files corresponding, one is the MRI scan and one is the extracted brain lesion).\nWe read this lesion mask, similar to how we did the MRI scan. You can download all lesion masks files we will work with from GitHub. In the same folder as your R script, create a folder called “lesion_masks”/. Then click on each file in the GitHub folder, and find the “Download raw file” button in the top right. Make sure to move the downloaded files into your lesion_masks folder. Then the following file path will work.\n\nlesion_mask &lt;- readNIfTI(\"lesion_masks/wsub-M2001_ses-1253x1076_lesion.nii\", reorient = TRUE)\n\nThen we look at a summary of this NIfTI object. We can do this by simply running the variable we stored the lesion mask in. Keep an eye out on the ‘Pixel Dimension’ as we will need to use this later to calculate the lesion volume:\n\nlesion_mask\n\nIf you are curious what this looks like just run image(lesion_mask) or orthographic(lesion_mask).\nOkay now the calculating begins. First we will count the number of voxels labeled as 1 (remember that 1 indicates the presence of a lesion).\n\northographic(lesion_mask)\n\nlesion_voxels &lt;- sum(lesion_mask == 1)\n\nThen, we retrieve voxel dimensions (in mm) via the pixdim() function. The array in the NIfTI header contains scaling information for each dimension of the image data (you can get this information by running lesion_mask@pixdim).\n(The output if we run lesion_mask\\@pixdim is : -1 1 1 1 0 0 0 0)\n\nThe first value indicates the orientation of the image axes. For this example it would be -1, which suggests left-handed coordinate system (while 1 would indicate a right-handed system).\nThe second, third and fourth value in this array indicates the voxel dimensions along the x, y, and z axes, respectively. In our case these are all 1.\nThe rest of the values are usually used for other dimensions like time. In our case they are 0, indicating that they are not used in our dataset.\n\nSo we are only interested in the second to fourth value from the ‘pixdim’ array, meaning we will have to filter these out with the use of the square-brackets.\n\nvoxel_dims &lt;- pixdim(lesion_mask)[2:4] \n\nThen we calculate the volume of a single voxel in mm³. We will be doing this via the prod() function, which is part of R’s base package.\n\nvoxel_volume_mm3 &lt;- prod(voxel_dims)\n\nNow we calculate the product of the number of voxels (lesion_voxels) and the volume of a single voxel (voxel_volume_mm3) to get the total lesion volume.  \n\nlesion_volume_mm3 &lt;- lesion_voxels * voxel_volume_mm3\n\nNow we can have a nice output of our result by using the cat() base R function, which concatenates and outputs its arguments:\n\ncat(\"Lesion Volume:\", lesion_volume_mm3, \"mm³\")\n\nThere is a table also linked to this dataset that contains participants’ information such as sex, age at stroke, race, the days after the stroke when participant took the WAB (Western Aphasia Battery) test, the Aphasia Quotient (AQ) from this test (which is a score out of 100, measuring language function in people with brain injury (like stroke) - see Table 1. on how to interpret the scores for the AQ part of the WAB) and the type of aphasia.\nTable 1. WAB-AQ scores\n\n\n\nAQ Score\nSeverity\n\n\n\n\n0-25\nVery severe\n\n\n26-50\nSevere\n\n\n51-75\nModerate\n\n\n76+\nMild\n\n\n\nNote: The information for this table was taken from: Western Aphasia Battery (WAB) – Strokengine\nWe can load the table containing all this information by directly from GitHub with the following code. Hit View(df) to see the data frame:\n\ndf &lt;- read_tsv(\"https://github.com/ucrdatacenter/projects/raw/refs/heads/main/misc/neuroimaging_tutorial/participants.tsv\")\nView(df)\n\nLet’s put to use what we learnt. Let’s say we want to see how lesion size and WAB-AQ scores correlate. We can do this by following these steps:\nLet’s pick subjects M2001, M2004, M2060 and M2069 who all have the same type of aphasia (anemic, which is a type of aphasia where patients find it hard to find words, but have near-normal speech). \nLet’s make a function to calculate the lesion volume using what we did for M2001. We can do this by: \n\ncalculate_lesion_volume &lt;- function(mask_path) {\n  lesion_mask &lt;- readNIfTI(mask_path, reorient = TRUE)\n  lesion_voxels &lt;- sum(lesion_mask == 1)\n  voxel_dims &lt;- pixdim(lesion_mask)[2:4]\n  voxel_volume_mm3 &lt;- prod(voxel_dims)\n  lesion_voxels * voxel_volume_mm3\n}\n\nHow this works is the following:\n\nthe function is called: calculate_lesion_volume\nthis function will take a variable that is the path to the lesion mask: function(mask_path)\nthe following are just copied from what we did earlier:\n\nlesion_mask &lt;- readNIfTI(mask_path, reorient = TRUE)\nlesion_voxels &lt;- sum(lesion_mask == 1)\nvoxel_dims &lt;- pixdim(lesion_mask)[2:4]\nvoxel_volume_mm3 &lt;- prod(voxel_dims)\nlesion_volume &lt;- lesion_voxels * voxel_volume_mm3\nNow we will use these functions to compute lesion volume per subject. We call function for each subject’s lesion mask files (M2001, M2004, M2060, and M2069). This will return a number (volume in mm³ for each participant), which will be stored in named variables (lesion_volume_2001, lesion_volume_2004 etc. - see on the right under ‘Values’):\n\nlesion_volume_M2001 &lt;- calculate_lesion_volume(\"lesion_masks/wsub-M2001_ses-1253x1076_lesion.nii\")\nlesion_volume_M2004 &lt;- calculate_lesion_volume(\"lesion_masks/wsub-M2006_ses-2381x1773_lesion.nii\")\nlesion_volume_M2060 &lt;- calculate_lesion_volume(\"lesion_masks/wsub-M2060_ses-220_lesion.nii\")\nlesion_volume_M2069 &lt;- calculate_lesion_volume(\"lesion_masks/wsub-M2069_ses-5818_lesion.nii\")\n\nNow we will use the table with the participant information.\nWe add the lesion volume variable to this table, but first we create a data frame where we store the participant id with the matching lesion volume. We create the new table like this:\n\nlesion_volumes &lt;- tibble(\n  participant_id =c(\"sub-M2001\", \"sub-M2004\", \"sub-M2060\", \"sub-M2069\"),\n  lesion_volume_mm3 = c(lesion_volume_M2001, lesion_volume_M2004, lesion_volume_M2060, lesion_volume_M2069)\n)\n\nNow we can merge this table with the bigger participants table (df) to do analysis. We do so by using the inner_join() function, which merges rows from the df and lesion_volumes table only where participant_id matches in both:\n\nmerged_data &lt;- inner_join(df, lesion_volumes, by = \"participant_id\")\n\nFinally, we can do our analysis. Let’s do a correlation analysis to see whether higher WAB-AQ scores correlate positively to higher lesion volumes. The cor.test() function performs a Pearson’s correlation test, using the lesion volume and WAB-AQ scores as arguments. This outputs the correlation coefficient, p-value and other statistics.\n\ncor.test(merged_data$lesion_volume_mm3, as.numeric(merged_data$wab_aq))\n\nSo the correlation is a negative value (-0.5635701), indicating a trend where larger lesion volume indicates lower WAB-AQ scores. The p-value is &gt; 0.05 (it is 0.4364), meaning that this is not statistically significant. However, we have a very small sample size (4 participants) so that can explain why our results are so insignificant.\nHowever, it would be nice to visualize this correlation. Let’s create a scatter plot, using ggplot, with the lesion volume on the X-axis, and the WAB-AQ score on the Y-axis.\nTo add a regression line we use geom_smooth, where we set method = “lm” to specify the linear model, and additionally set se=FALSE, to hide the shaded confidence interval around the line (optional).\n\nggplot(merged_data, aes(x = lesion_volume_mm3, y = wab_aq, label = participant_id)) + \n  geom_point(size = 3, color=\"black\") +  \n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", size=0.6) +  \n  geom_text(vjust = -0.5, size = 3) +  # participant labels\n  labs(\n    title = \"Lesion Volume vs WAB-AQ\",\n    x = \"Lesion Volume (mm³)\",\n    y = \"WAB-AQ Score\"\n  ) +\n  theme_minimal()\n\nWe can see a trend that could be negative correlation between higher WAB-AQ scores and larger lesion volumes. Is this a result you expected? You can also explore different aphasia types and see whether this correlation is also present in those.\nHope you enjoyed this tutorial and you feel more inspired to explore its potential more in R.\nSome other useful resources if you are interested in neuroimaging:\n\nThe Neuroconductor webpage has three online-courses: Courses | Neuroconductor. I highly recommend the one called “Imaging in R”.\nThere are two videos from Elizabeth Sweeney that give a very nice insight into what is possible with Neuroimaging in R. I recommend checking it out if you are interested:\n\nElizabeth Sweeney - Neuroimaging Analysis in R (16:45 minutes)\nNeuroimaging Analysis in R: Image Preprocessing (51:12 minutes)"
  }
]