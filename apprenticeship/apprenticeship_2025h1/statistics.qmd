---
title: "Data Center Apprenticeship:\nStatistics in R"
subtitle: "January 2025" 
date: "Last updated: `r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, error = FALSE)
```

```{r, echo=FALSE}
library(tidyverse)
data <- read_csv("data/data.csv")
```

# Introduction

This document provides an overview of the basic statistical functions in R, including descriptive statistics and summary tables. We work with some example data of student characteristics and grades. You can import the data from a CSV file with the following code:

```{r, eval = FALSE}
library(tidyverse)
data <- read_csv("https://github.com/ucrdatacenter/projects/raw/refs/heads/main/apprenticeship/2025h1/student_data.csv")
```

# Descriptive statistics

## Summaries in the R Console

To get a descriptive statistic of a single variable in a tibble, we can use that variable as an argument to a relevant function (using `$` to refer to a variable in a tibble).

```{r, eval = FALSE}
mean(data$age)
median(data$age)
sd(data$grade)
```

To get the frequencies of a categorical variable, we can use the `count()` function, with the `sort = TRUE` argument returning the values in descending frequency. `count()` is a tidy function that works well with pipe workflows and can count the joint frequencies of multiple variables.

```{r, eval = FALSE}
# frequencies of a single variable
count(data, reading)

# joint frequency distribution
count(data, reading, listening, notes)
```

To get the correlation coefficient between two variables, we can use the `cor()` function in the same way we used other descriptives such as `mean()`.

```{r, eval = FALSE}
cor(data$age, data$grade)
```

The easiest way to get summary statistics of all variables in a tibble is with the `summary()` function: this function shows the distribution of numeric variables, the frequencies of categorical variables, and the number of missing values for each variable.

```{r, eval = FALSE}
summary(data)
```

## Publication-ready summaries with `gtsummary`

The `summary()` function is useful for viewing the data in the Console, but doesn't export to outside of R nicely. There are a few packages available for generating simple summary statistics tables that contain information about the central tendencies and dispersion of the data that also export nicely, such as `gtsummary`.

```{r, eval = FALSE}
library(gtsummary)

# tbl_summary() creates a summary table of the data
tbl_summary(data)
```

You can stratify your summary by a grouping variable using the `by` argument:

```{r, eval = FALSE}
tbl_summary(data, by = sex)

# add p-value of difference between groups
tbl_summary(data, by = sex) |> 
  add_p()
```

You can also customize the appearance of the table.

```{r, eval = FALSE}
tbl_summary(data, by = sex) |> 
  modify_header(label ~ "Variable") |> 
  modify_caption("Summary Table by Sex")
```

To export the table as a Word document, use the `gtsave()` function. Note that we first use the `as_gt()` function to convert the `tbl_summary()` output to a `gt` object, and load the `gt` package in order to use the Word export function defined for the `gt` package.

```{r, eval=FALSE}
library(gt)

tbl_summary(data, by = sex) |> 
  as_gt() %>%
  gtsave("summary_table.docx")
```

If you use LaTeX, you can also export as a LaTeX table, also relying on the `gt` package.

```{r, eval=FALSE}
tbl_summary(data, by = sex) |> 
  as_gt() %>%
  gtsave("summary_table.tex")
```

## Creating custom summary tables

Alternatively, we can define our own summary statistics with the `dplyr` functions `group_by()` and `summarize()`, which also easily allows the calculation of more complex descriptive statistics, including grouped statistics based on categorical variables. The `across()` helper function in the `summarize()` function can be used to apply the same calculation to multiple variables at once: it requires the first argument as the list of variables (potentially with the help of selector functions) and the function we'd like to apply.

```{r}
# tibble of mean and sd for a single variable
data |> 
  summarize(mean_grade = mean(grade),
            sd_grade = sd(grade))

# mean and sd of age and grade variables, grouped by reading
data |> 
  group_by(reading) |> 
  # .names allows overriding default option to reuse original column names
  summarize(across(c(age, grade), mean, .names = "mean_{.col}"),
            across(c(age, grade), sd, .names = "sd_{.col}"))

# mean of all numeric variables, grouped by reading
data |> 
  group_by(reading) |> 
  # where() is a helper function evaluating the contents of variables
  # specify full function call with ~ at the start and .x replacing the variable name
  summarize(across(where(is.numeric), ~mean(.x, na.rm = TRUE)))

# mean of all variables with names containing the letter a
data |> 
  summarize(across(contains("a"), ~mean(.x, na.rm = TRUE)))

# sample size of each group and correlation between age and grade per group
data |> 
  group_by(reading, listening) |> 
  summarize(age_grade_correlation = cor(age, grade),
            n = n())
```

The list of helper functions that can be used instead of listing which variables to include/exclude is in the help file accessible with `?dplyr_tidy_select`.

To export a descriptive statistics table, we can use the relevant `write...()` function shown in the data importing section (e.g. `write_csv()` for tibbles, general `write()` for HTML, plain text, LaTeX, other general types). CSV tables already copy nicely into e.g. MS Word. Altenatively, the `gt` package can be used to export the table as a Word or LaTeX document the same way as before.

```{r, eval=FALSE}
data |> 
  count(reading, listening) |> 
  write_csv("table1.csv")
```

```{r, eval=FALSE}
data |> 
  count(reading, listening) |> 
  gt() |> 
  gtsave("table1.docx")
```

# Hypothesis testing

Most of the simple statistical tests are from base R, so they don't rely on tidy principles, but many are compatible with tidy workflows to at least some extent. In the following we'll cover some of the key methods that show up in methods and statistics courses at UCR. In addition, the `tidy()` function from the `broom` package converts most text output into simple tibbles, which are useful for exporting and visualizing results.

```{r, eval = FALSE}
library(broom)
```

## t-tests

We can run one and two samples t-tests to evaluate group means with the `t.test()` function. The function supports various options and model specifications: a simple one-sample t-test only requires specifying the variable of interest, either with `x = data$variable` or `x = variable, data = data` syntax. For two-sample t-tests, we can use the formula syntax `y ~ x` to specify the dependent and independent variables or the `x` and `y` (and optionally `data`) arguments. Additional options include specifying the alternative hypothesis, the confidence level, the value of $\mu$, and whether we want a paired t-test and assume equal variances. Helper functions such as `tidy()` convert the Console output to an easy-to-export tibble of results.

```{r, eval = FALSE}
# simple t-test (H0: mean=mu)
t.test(data$scholarship, mu = 50)

# can also use data argument instead of data$...
# grade ~ reading is formula specification: variable ~ group
t.test(grade ~ reading, alternative = "greater", data = data)

# tidy() function turns results into a tibble
t.test(grade ~ reading, alternative = "greater", data = data) |> tidy()
```

## Correlation test

The `cor.test()` function calculates the correlation between two variables. Again, the function supports various specifications: `x` and `y` arguments, formula syntax (see below for an example), and the `data` argument.

```{r, eval = FALSE}
cor.test( ~ grade + age, data = data)

# tidy() function turns results into a tibble
cor.test( ~ grade + age, data = data) |> tidy()
```

## Simple regression

A key building block of statistical analysis is linear regression. The `lm()` function fits a linear model to the data, with a wide range of specifications, passed as the formula argument (first argument if unnamed). The formula syntax is `y ~ x`, where `y` is the dependent variable and `x` is the independent variable. Again, optional function arguments allow for a lot of customization, but the default settings are sufficient for most applications. Helper functions such as `tidy()` and `summary()` provide extensive summaries of the model fit and coefficients, and `tbl_regression()` from the `gtsummary` package creates neat tables of the results. Assigning the result of a model to an object saves computational time, as then we can work with the results without having to re-run the analysis every time.

```{r, eval = FALSE}
# assign outcome to object
fit <- lm(grade ~ age, data = data)

# extensive result summary
fit |> summary()

# tidy coefficients
fit |> tidy()

# display-ready table with gtsummary
tbl_regression(fit)
```

## Multiple regression

Multiple regression extends simple regression to multiple independent variables. The only difference is the formula specification, which now connects multiple independent variables with `+` signs. The formula specification also allows for interactions between variables, which can be specified with `*` (if the main effects should be included) or `:` (for only the interaction term). The `DV ~ .~` syntax includes all variables in the data except the dependent variable as independent variables.

```{r, eval = FALSE}
lm(grade ~ age + scholarship, data = data) |> summary()

# all variables in data
lm(grade ~ ., data = data) |> summary()

# interactions
lm(grade ~ age * scholarship, data = data) |> summary()
```

## ANOVA

Analysis of variance (ANOVA) is a generalization of the t-test to multiple groups. The `aov()` function fits an ANOVA model to the data, with the formula syntax `y ~ x`, where `y` is the dependent variable and `x` is the independent variable. The same helper functions as with `lm()` can be used to summarize the results.

Note that ANOVA is a specific case of a linear regression model, so the results are equivalent to those of a linear regression model with a categorical independent variable.

```{r, eval = FALSE}
anova_fit <- aov(grade ~ reading, data = data)

summary(anova_fit)
tidy(anova_fit)

# equivalent regression
lm(grade ~ reading, data = data) |> summary()
```

## Chi-square test

The chi-square test is used to test the independence of two categorical variables. The `chisq.test()` function calculates the chi-square statistic and the p-value for the test. Unlike the previous functions, the function does not allow for a `data` argument, and is therefore difficult to implement in tidy workflows.

```{r, eval = FALSE}
chisq.test(data$reading, data$notes)
```

Add contingency tables:

```{r, eval = FALSE}
# with table()
table(data$reading, data$notes)

# with xtabs()
xtabs(~ reading + notes, data = data)

# with count() and pivot_wider()
data |> 
  count(reading, notes) |>
  pivot_wider(names_from = notes, values_from = n, values_fill = 0)

# proportions with table() and prop.table()
table(data$reading, data$notes) |> prop.table()

# proportions with count() and pivot_wider()
data |> 
  count(reading, notes) |>
  mutate(prop = n / sum(n)) |>
  select(-n) |> 
  pivot_wider(names_from = notes, values_from = prop, values_fill = 0)

```

## Logistic regression

When it comes to predicting binary outcomes, linear regression has some problems, such as predicting values outside the 0-1 range. Therefore in those cases, we often use logistic regression instead. The `glm()` function fits a logistic regression model to the data. Other than the `family` argument, which specifies the distribution of the dependent variable, the function works in the same as `lm()`. A logistic regression uses `family = "binomial"`.

```{r, eval = FALSE}
logit_fit <- glm(reading ~ age, data = data, family = "binomial")

logit_fit |> summary()
logit_fit |> tidy()

# display-ready table with gtsummary
tbl_regression(logit_fit)
```

## Non-parametric tests

Non-parametric tests are used when the assumptions of parametric tests are violated. Running them in R follows the same structure as running the parametric alternative, other than the function name itself and potential alternative optional arguments.

```{r, eval = FALSE}
# Wilcoxon signed-rank test
wilcox.test(data$grade, mu = 2)

# Mann-Whitney U test
wilcox.test(grade ~ reading, data = data)

# Kruskal-Wallis test (scholarship is a number in the data so convert to factor for this example)
kruskal.test(grade ~ factor(scholarship), data = data)
```

## PCA and factor analysis

Principal component analysis (PCA) and factor analysis are used to reduce the dimensionality of a dataset. The `prcomp()` function fits a PCA model to the data, and the `factanal()` function fits a factor analysis model. Both functions work with the formula syntax, and the `data` argument can be used to specify the data frame.

```{r, eval = FALSE}
# PCA
pca_fit <- prcomp(~ grade + age + scholarship, data = data)

# summary of PCA
summary(pca_fit)
# component loadings for each observation
tidy(pca_fit)

# Factor analysis
fa_fit <- factanal(~ grade + age + scholarship, factors = 1, data = data)

# summary of factor analysis
fa_fit
tidy(fa_fit)
```
