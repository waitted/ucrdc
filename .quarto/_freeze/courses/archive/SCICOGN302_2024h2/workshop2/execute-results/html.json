{
  "hash": "d4fd15ef207ca8b523d160a0f2e9afdd",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlayout: page\ntitle: \"SCICOGN302 workshop II:<br> Some measures of language development\"\nsubtitle: \"Fall 2024\"\ndate: \"Last updated: 2025-08-17\"\n---\n\n\n\n\n# Learning outcomes\n\nIn this tutorial you learn the steps needed to calculate and visualize some measures of languages development over time for three children, using R and the CHILDES database of child language development. You will need these skills in the small homework assignment following this workshop, and in case you choose to complete a data assignment instead of an experiment for your final poster.\n\nYou learn how to import data into R with the `childesr` package, how to use basic text analysis tools, and how to visualize your results using the `ggplot2` package.\n\n# First steps\n\nThis tutorial assumes that you completed the [first Data Center workshop](../workshop1) and its prerequisites.\n\nNote that if you get stuck at any point, check the help files of functions (access by running `?functionname`), look at more extensive [Data Center tutorials](../../../tutorials), try googling your question, attend Data Center office hours ([schedule](../../../contact)) or email [datacenter\\@ucr.nl](mailto:datacenter@ucr.nl).\n\nThe code used in this tutorial is also available on [Github](https://github.com/ucrdatacenter/projects/blob/main/SCICOGN302/2024h2/workshop2code.R).\n\n# Accessing the data\n\nOur goal is to analyze speech samples from three children, calculate different measures of language development, and visualize the results. All three children were studied by Roger Brown and his students, and their data is available in the CHILDES database.\n\nFirst, we load the packages we need for this analysis. Make sure that all packages are installed on your computer. If not, you can install them by running `install.packages(\"packagename\")`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)\nlibrary(childesr)\nlibrary(qdapDictionaries)\n\n# look at all the available corpora in the data set\ncorpora <- get_corpora()\nView(corpora)\n```\n:::\n\n\nWe are going to work with tokens and utterances a lot, so first, a little explanation on what they actually are. Utterances are data elements from a recorded piece of conversation (such as \"I had good dinner\"). Tokens are specific pieces of utterances (\"I\", \"had\", \"good\", \"dinner\"). Utterances are usually more useful when we look into the development of a child as a whole (syntax, semantics, context of the utterance), whereas with tokens you can look into the development of a child's vocabulary (child's vocabulary complexity and its development over time.)\n\nWhen picking a child/children for your research question, it is very important to make sure you have enough data (utterances and tokens) to work with. In a study done by MacWhinney, B., & Snow, C. (1990) you have a detailed overview of all the researchers who have added to the childes data set. There are some which follow a single child (Snow), some that look into the short-term speech of multiple children (Higginson), and some that follow the mothers and children as well (Howe).\n\nIn this example, we will work with Brown's addition to the data set (Brown's corpora, specifically tokens) - data acquired from three children Adam, Eve and Sarah, collected by Roger Brown and his students. Adam was studied from 2;3 to 4;10; Eve from 1;6 to 2;3; and Sarah from 2;3 to 5;1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# download tokens from the Brown corpus\ntokens <- get_tokens(token = \"*\", collection = \"Eng-NA\", \n                     corpus = \"Brown\", role = \"target_child\")\n```\n:::\n\n\n# Share of valid words\n\nOne way to measure the development of the children's language using token-level data is to evaluate whether each token is a valid English word or not, and use this information to calculate summary statistics per time period.\n\nWe can introduce a variable for whether the each word is present in an English dictionary. This variable will be a logical vector, with `TRUE` and `FALSE` as possible values. The computer treats `TRUE` values as 1 and `FALSE` values as 0, so you can use mathematical operations such as averages on logical data. Then the average of the variable over a certain time period can be interpreted as the share of words that are present in an English dictionary: average values closer to zero would mean the child still has a lot of mumbling/incorrect pronunciation or misspoken words in their vocabulary, whereas a child with a score near 1 has a very developed vocabulary. This way, we can assign a value to every token, and then average it per certain time period. Afterwards, we can compare this value progression for all three children. This comparison does not look into syntax of the sentence, meaning that even a sentence \"banana mango apples man\" would be evaluated as entirely correct, when in reality this sentence is nonsensical. So this method certainly has its limitations, but is nevertheless interesting to look at.\n\nIn order to find out which words exist in an English dictionary, we need to choose a dictionary. The `qdapDictionaries` package contains the `GradyAugmented` object, which is a character vector containing over 120,000 English words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# let's check the total number of words in the GradyAugmented vector\nlength(GradyAugmented)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 122806\n```\n\n\n:::\n:::\n\n\nIt is suggested by multiple sources that we need approximately 3000 words to be able to communicate our point in English without any problems (VocabularyFirst, 2019; Yang, 2016). This data set has 40 times that amount, so we can deem it appropriate for our purposes. Even though it's not an exhaustive list of all English words, the chances that a child said a word that is correct and is not on the list above is very small.\n\nNow we need to compare the data sets of children's tokens with the valid words in our English dictionary. We can define a logical condition with the `%in%` operator, which checks if each token is present in the `GradyAugmented` vector and returns `TRUE` or `FALSE` as the result. We call the new variable `is_valid` to reflect whether the token in each row is a valid word.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a new column 'is_valid' in tokens\nvalid_tokens <- tokens %>%\n  mutate(is_valid = gloss %in% GradyAugmented)\n\n# View the first few rows of gloss and is_valid to check the results\nvalid_tokens |> \n  select(gloss, is_valid) |>\n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 2\n  gloss     is_valid\n  <chr>     <lgl>   \n1 yeah      TRUE    \n2 choo_choo FALSE   \n3 train     TRUE    \n4 there     TRUE    \n5 water     TRUE    \n6 water     TRUE    \n```\n\n\n:::\n:::\n\n\nSince these children are not studied from the time when they first start speaking and are therefore more likely to stutter or create their own words to fill in gaps in vocabulary, most tokens are valid words. However, there will still be some slips, such as when Adam used the word choo-choo, probably to adress a train.\n\nLooking at these \"slips\" may show us how developed a child's language is. In reality, creating their own words and bridging gaps in their vocabulary is just a natural step in language development, as it is experimentation with meaning, words structures and sounds (Michigan State University Extension, 2023, Social Sci LibreTexts, 2023). However, for us it shows how \"far\" a child is in their language development journey.\n\nIn order to get some statistics over time, let's aggregate the data per child and age bracket, and calculate the share of valid words in each period. We use the `floor()` function to round the age to the nearest month, and then group by the child's name and age bracket to calculate the average of the `is_valid` variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvalid_tokens <- tokens |> \n  mutate(is_valid = gloss %in% GradyAugmented,\n         age = floor(target_child_age)) |> \n  group_by(target_child_name, age) %>%\n  summarise(valid_share = mean(is_valid))\n```\n:::\n\n\nOnce we have the data where each row corresponds to one child in one month, we can visualize the data with a line chart, coloring the lines by child name.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvalid_tokens |> \n  ggplot(aes(x = age, y = valid_share, color = target_child_name)) +\n  geom_line() + \n  labs(title = \"Development of Average Valid Word Usage Over Time\",\n       x = \"Age (months)\",\n       y = \"Average Valid Word Usage\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](workshop2_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nAfter looking at the data we can see that initially, there is an increase of average valid word usage, especially for Sarah. However, there does not seem to be a stable trend, except for Eve, where there is a stable increase for period of 5 months. Her speech was recorded for shorter period of time than Adam and Sarah and at a younger age, so it's not an entirely fair comparison with the other two. When we compare only Adam and Sarah, we can see that Adam started with a much higher average valid word usage, but Sarah caught up to Adam over time, and from about the age of 50 months, they have similar average valid word usage (85--90% of all tokens).\n\nThis way of analyzing tokens may be more suitable for children that are just learning how to speak, as the figure suggests that there is a certain development period of life when there is a rise of average valid use of words, and then the trends flatten out as the vast majority of spoken tokens are valid words beyind a certain age.\n\n# Share of uncommon words\n\nAnother way we can analyse and compare their vocabulary development is to look into the uniqueness of words the children use. For this we will track \"valid\" words from their utterances, but compare it against the 500 most common words in the English language to see how many words are NOT from this list (while still being valid words). So instead of calculating what share of total tokens are valid words, we can calculate what share of total tokens are valid but not common words.\n\nFirst, let's create a vector with the most common words according to english4today.com. There is no pre-defined list of the most common words in R, so we have to create it ourselves.\n\nUseful tip: AI tools such as ChatGPT can help with formatting words/numbers into data sets and vectors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a vector with the provided list of common words\ncommon_words <- c(\"the\", \"of\", \"to\", \"and\", \"a\", \"in\", \"is\", \"it\", \"you\", \"that\", \"he\", \n                  \"was\", \"for\", \"on\", \"are\", \"with\", \"as\", \"I\", \"his\", \"they\", \"be\", \n                  \"at\", \"one\", \"have\", \"this\", \"from\", \"or\", \"had\", \"by\", \"hot\", \"but\", \n                  \"some\", \"what\", \"there\", \"we\", \"can\", \"out\", \"other\", \"were\", \"all\", \n                  \"your\", \"when\", \"up\", \"use\", \"word\", \"how\", \"said\", \"an\", \"each\", \n                  \"she\", \"which\", \"do\", \"their\", \"time\", \"if\", \"will\", \"way\", \"about\", \n                  \"many\", \"then\", \"them\", \"would\", \"write\", \"like\", \"so\", \"these\", \n                  \"her\", \"long\", \"make\", \"thing\", \"see\", \"him\", \"two\", \"has\", \"look\", \n                  \"more\", \"day\", \"could\", \"go\", \"come\", \"did\", \"my\", \"sound\", \"no\", \n                  \"most\", \"number\", \"who\", \"over\", \"know\", \"water\", \"than\", \"call\", \n                  \"first\", \"people\", \"may\", \"down\", \"side\", \"been\", \"now\", \"find\", \n                  \"any\", \"new\", \"work\", \"part\", \"take\", \"get\", \"place\", \"made\", \"live\", \n                  \"where\", \"after\", \"back\", \"little\", \"only\", \"round\", \"man\", \"year\", \n                  \"came\", \"show\", \"every\", \"good\", \"me\", \"give\", \"our\", \"under\", \"name\", \n                  \"very\", \"through\", \"just\", \"form\", \"much\", \"great\", \"think\", \"say\", \n                  \"help\", \"low\", \"line\", \"before\", \"turn\", \"cause\", \"same\", \"mean\", \n                  \"differ\", \"move\", \"right\", \"boy\", \"old\", \"too\", \"does\", \"tell\", \n                  \"sentence\", \"set\", \"three\", \"want\", \"air\", \"well\", \"also\", \"play\", \n                  \"small\", \"end\", \"put\", \"home\", \"read\", \"hand\", \"port\", \"large\", \n                  \"spell\", \"add\", \"even\", \"land\", \"here\", \"must\", \"big\", \"high\", \"such\", \n                  \"follow\", \"act\", \"why\", \"ask\", \"men\", \"change\", \"went\", \"light\", \n                  \"kind\", \"off\", \"need\", \"house\", \"picture\", \"try\", \"us\", \"again\", \n                  \"animal\", \"point\", \"mother\", \"world\", \"near\", \"build\", \"self\", \n                  \"earth\", \"father\", \"head\", \"stand\", \"own\", \"page\", \"should\", \"country\", \n                  \"found\", \"answer\", \"school\", \"grow\", \"study\", \"still\", \"learn\", \n                  \"plant\", \"cover\", \"food\", \"sun\", \"four\", \"thought\", \"let\", \"keep\", \n                  \"eye\", \"never\", \"last\", \"door\", \"between\", \"city\", \"tree\", \"cross\", \n                  \"since\", \"hard\", \"start\", \"might\", \"story\", \"saw\", \"far\", \"sea\", \n                  \"draw\", \"left\", \"late\", \"run\", \"don't\", \"while\", \"press\", \"close\", \n                  \"night\", \"real\", \"life\", \"few\", \"stop\", \"open\", \"seem\", \"together\", \n                  \"next\", \"white\", \"children\", \"begin\", \"got\", \"walk\", \"example\", \"ease\", \n                  \"paper\", \"often\", \"always\", \"music\", \"those\", \"both\", \"mark\", \"book\", \n                  \"letter\", \"until\", \"mile\", \"river\", \"car\", \"feet\", \"care\", \"second\", \n                  \"group\", \"carry\", \"took\", \"rain\", \"eat\", \"room\", \"friend\", \"began\", \n                  \"idea\", \"fish\", \"mountain\", \"north\", \"once\", \"base\", \"hear\", \"horse\", \n                  \"cut\", \"sure\", \"watch\", \"color\", \"face\", \"wood\", \"main\", \"enough\", \n                  \"plain\", \"girl\", \"usual\", \"young\", \"ready\", \"above\", \"ever\", \"red\", \n                  \"list\", \"though\", \"feel\", \"talk\", \"bird\", \"soon\", \"body\", \"dog\", \n                  \"family\", \"direct\", \"pose\", \"leave\", \"song\", \"measure\", \"state\", \n                  \"product\", \"black\", \"short\", \"numeral\", \"class\", \"wind\", \"question\", \n                  \"happen\", \"complete\", \"ship\", \"area\", \"half\", \"rock\", \"order\", \"fire\", \n                  \"south\", \"problem\", \"piece\", \"told\", \"knew\", \"pass\", \"farm\", \"top\", \n                  \"whole\", \"king\", \"size\", \"heard\", \"best\", \"hour\", \"better\", \"true\", \n                  \"during\", \"hundred\", \"am\", \"remember\", \"step\", \"early\", \"hold\", \"west\", \n                  \"ground\", \"interest\", \"reach\", \"fast\", \"five\", \"sing\", \"listen\", \"six\", \n                  \"table\", \"travel\", \"less\", \"morning\", \"ten\", \"simple\", \"several\", \n                  \"vowel\", \"toward\", \"war\", \"lay\", \"against\", \"pattern\", \"slow\", \"center\", \n                  \"love\", \"person\", \"money\", \"serve\", \"appear\", \"road\", \"map\", \"science\", \n                  \"rule\", \"govern\", \"pull\", \"cold\", \"notice\", \"voice\", \"fall\", \"power\", \n                  \"town\", \"fine\", \"certain\", \"fly\", \"unit\", \"lead\", \"cry\", \"dark\", \n                  \"machine\", \"note\", \"wait\", \"plan\", \"figure\", \"star\", \"box\", \"noun\", \n                  \"field\", \"rest\", \"correct\", \"able\", \"pound\", \"done\", \"beauty\", \"drive\", \n                  \"stood\", \"contain\", \"front\", \"teach\", \"week\", \"final\", \"gave\", \"green\", \n                  \"oh\", \"quick\", \"develop\", \"sleep\", \"warm\", \"free\", \"minute\", \"strong\", \n                  \"special\", \"mind\", \"behind\", \"clear\", \"tail\", \"produce\", \"fact\", \n                  \"street\", \"inch\", \"lot\", \"nothing\", \"course\", \"stay\", \"wheel\", \"full\", \n                  \"force\", \"blue\", \"object\", \"decide\", \"surface\", \"deep\", \"moon\", \"island\", \n                  \"foot\", \"yet\", \"busy\", \"test\", \"record\", \"boat\", \"common\", \"gold\", \n                  \"possible\", \"plane\", \"age\", \"dry\", \"wonder\", \"laugh\", \"thousand\", \"ago\", \n                  \"ran\", \"check\", \"game\", \"shape\", \"yes\", \"hot\", \"miss\", \"brought\", \"heat\", \n                  \"snow\", \"bed\", \"bring\", \"sit\", \"perhaps\", \"fill\", \"east\", \"weight\", \n                  \"language\", \"among\")\n```\n:::\n\n\nNow let's create a new variable in the tokens data that counts the number of less common valid words. We can do this step by step: first, we create a variable that checks if the word is in the vector of valid words, another variable that checks if the word is in the vector of common words, and finally a variable that checks if the word is valid but not common. The `!` operator negates the condition, so `!is_common` is `TRUE` when `is_common` is `FALSE`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create the 'is_special' variable\nspecial_tokens <- tokens |> \n  mutate(is_valid = gloss %in% GradyAugmented,\n         is_common = gloss %in% common_words,\n         is_special = is_valid & !is_common)\n```\n:::\n\n\nNow we can aggregate and plot the data the same way as before, this time calculating the share of special words per month for each child.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# share of special words per month\nspecial_tokens <- tokens |> \n  mutate(is_valid = gloss %in% GradyAugmented,\n         is_common = gloss %in% common_words,\n         is_special = is_valid & !is_common,\n         age = floor(target_child_age)) |> \n  group_by(target_child_name, age) |> \n  summarize(special_share = mean(is_special))\n\n# create the plot\nspecial_tokens |> \n  ggplot(aes(x = age, y = special_share, color = target_child_name)) +\n  geom_line() +\n  labs(title = \"Share of Special Words per Age Bracket\",\n       x = \"Age (months)\",\n       y = \"Share of Special Words\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](workshop2_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThisp plot shows us that the share of special words is quite volatile over time, with little clear trends. For Eve, the share of special words decreases over time from above 30% to 25%, while for Adam there is a peak above 45% at around 30 months, followed by fluctuations around 25%. Sarah's special word usage also fluctuates around 25%, with a slight dip to 20% at around 33 months. This result is not what we might have expected, as we might have thought that the share of special words would increase over time as the children expand their vocabulary.\n\nA potential issue with this measure is that it counts every time a word is repeated. So we don't know if a child uses 100 different words or the same word 100 times.\n\nAs an alternative measure, we can consider the share of unique special words out of the total number of unique words. So e.g. if a child says 1000 unique tokens in a month, out of which 200 are valid but not common words, the share of unique special words would be 200/1000=20%, no matter how many times each unique token is used. We can calculate this alternative metric by adding one line of code that keeps only unique combinations of child name, age, token, and special word indicator.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# share of unique special words per month\nunique_special_tokens <- tokens |> \n  mutate(is_valid = gloss %in% GradyAugmented,\n         is_common = gloss %in% common_words,\n         is_special = is_valid & !is_common,\n         age = floor(target_child_age)) |> \n  # keep only unique tokens per child and age bracket\n  distinct(target_child_name, age, gloss, is_special) |> \n  group_by(target_child_name, age) |> \n  summarize(special_share = mean(is_special))\n\n# create the line plot\nunique_special_tokens |> \n  ggplot(aes(x = age, y = special_share, color = target_child_name)) +\n  geom_line() +\n  labs(title = \"Share of Special Words per Age Bracket\",\n       x = \"Age (months)\",\n       y = \"Share of Special Words\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](workshop2_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThis plot looks very different than the previous one, but it doesn't look much more informative. For all three children, the lines are very volatile with no clear trends over time. But if we compare the children, we see that up to the age of 50 months, Adam has a higher share of unique special words than Sarah, which is similar to the previous finding that in that age perid Adam also has a higher share of valid words.\n\nBoth of these methods have the limitation that we restrict the definition of common words to the 500 most common words. According to Linguisystems Milestones Guide the typical vocabulary size of a 5-year-old is between 2200-2500. So our definition of common words is quite restrictive, and we might miss some words that are common in the context of the children's environment. These differences in environment might also esxplain the large fluctuations between transcript recordings. Of course, you can always look for a more extensive list of common words, or even create your own list based on the data you have. You can try to redefine the list of common words, rerun the analysis. and see if the results change.\n\n# Conclusion\n\nIn this example we analyzed the speech patterns of three children on the token level. We looked at whether the words they use are valid English words, and whether they are more common or more unique. While we found some evidence that the children's language development progresses over time, the results are not as clear-cut as we might have expected, as all measures we tested are quite volatile. We also didn't do any analysis on the syntax of the sentences, which is also an important part of language development. For that, we would need to look at the utterance level, and analyze the structure of the sentences, the length of the utterances, the complexity of the utterances, etc.\n\n# References\n\nList of sources: Hsu, M.-L. (2013). Language play: The development of linguistic consciousness and creative speech in early childhood education. In Advances in early education and day care (Vol. 17, pp. 127â€“139). Emerald Group Publishing Limited. \\[<https://doi.org/10.1108/S0270-4021(2013)0000012007>\\]\\[<https://doi.org/10.1108/S0270-4021(2013)0000012007>\\]\n\nThe Education Hub. (n.d.). Effective vocabulary instruction. The Education Hub. <https://theeducationhub.org.nz/effective-vocabulary-instruction/>\n\nChildes-db. (2019). Childes-db: A flexible and reproducible interface to the child language data exchange system. Journal of Child Language, 51, 1928â€“1941. <https://doi.org/10.1017/s0305000900013866>\n\nWordReference. (n.d.). Top 2000 English words. WordReference. <https://lists.wordreference.com/show/Top-2000-English-words.1/>\n\nVocabularyFirst. (2019). How many words do I need to speak English language? VocabularyFirst. <https://www.vocabularyfirst.com/how-many-words-do-i-need-to-know/>\n\nYang, D. (2016). How many words do you need to know to be fluent in English? Day Translations. <https://www.daytranslations.com/blog/how-many-words-to-be-fluent-in-english/>\n\nSocial Sci LibreTexts. (2023). Language Development in Early Childhood.\n\nMichigan State University Extension. (2023). Language development â€“ Part 2: Principles that are the stem and branch of speech.\n",
    "supporting": [
      "workshop2_files\\figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}