{
  "hash": "c9cd464fb40b7b5ff5ad823e2dc6743a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data Center Apprenticeship:\\nText analysis in R\"\nsubtitle: \"June 2024\" \ndate: \"Last updated: 2025-08-09\"\n---\n\n\n\n\n# Introduction\n\nThis tutorial is based on multiple chapters of [\"Text Mining with R: A Tidy Approach\"](https://www.tidytextmining.com/) by Julia Silge and David Robinson.\n\nThroughout this tutorial, we will use the `tidytext` package to analyze text data, in particular the contents of Alice in Wonderland and Winnie-the-Pooh. This book and many others are available via the `gutenberg_download()` function in the `gutenbergr` package, which provides access to the Project Gutenberg collection of public domain books.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"tidytext\")\n# install.packages(\"gutenbergr\")\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(gutenbergr)\n\n# Download books based on their Gutenberg ID\n# https://gutenberg.org/ebooks/19033\n# https://gutenberg.org/ebooks/67098\nbooks <- gutenberg_download(c(19033, 67098))\n```\n:::\n\n\n# Representing text as data\n\n## Tidy text format\n\nCurrently in the `books` tibble, each row represents a line of text from one of the books. It is often useful to represent text data in a tidy format, where each row represents a word or token, as then we can apply data wrangling operations on the word level. We can use the `unnest_tokens()` function from the `tidytext` package to easily split the text into words, or into other levels of analysis such as characters, sentences or paragraphs. This function also takes care of removing punctuation, converting words to lowercase, and dropping empty rows. Before tokenizing, we may want to remove the contents of the titlepage, as the actual book contents only start on line 38 for Alice in Wonderland and line 79 for Winnie-the-Pooh.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remove title page and add book title variable\nbooks_content <- books |> \n  # add book title\n  mutate(book_title = ifelse(gutenberg_id == 19033, \"Alice\", \"Winnie\")) |> \n  # restart counting row numbers for each book\n  group_by(book_title) |> \n  filter((book_title == \"Alice\" & row_number() >= 38) |\n           (book_title == \"Winnie\" & row_number() >= 79)) |> \n  ungroup()\n\nwords <- books_content |> \n  # split books into words\n  unnest_tokens(output = word, input = text)\n```\n:::\n\n\nYou may notice that not all words are completely clean or relevant: some are surrounded by underscores and some are numbers. We can clean these up manually with regular expressions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwords <- words |> \n  mutate(word = str_remove_all(word, \"_\")) |>\n  filter(!str_detect(word, \"^\\\\d+$\"))\n```\n:::\n\n\n## Term frequency\n\nThe term frequency (tf) of a word is the number of times it appears in a document, divided by the total number of words in the document. It is simply the result of counting the number of times a word appears in the document.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf <- words |> \n  # count the number of times each word appears in each book\n  count(book_title, word) |>\n  # divide by number of words in each book\n  group_by(book_title) |> \n  mutate(tf = n / sum(n)) |> \n  ungroup()\n```\n:::\n\n\n## Term frequency - inverse document frequency (tf-idf)\n\nThe inverse document frequency (idf) of a word is the logarithm of the total number of documents divided by the number of documents that contain the word. It is a measure of how unique or rare a word is across the entire corpus. The intuition for why idf matters is that words that appear in many documents are less informative than words that appear in only a few documents. Therefore we often combine tf and idf into a single metric called term frequency-inverse document frequency (tf-idf), which is the product of tf and idf. The `bind_tf_idf()` function from the `tidytext` package can be used to calculate tf-idf values (the function also generates tf and idf separately).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf_idf <- words |> \n  count(book_title, word) |>\n  bind_tf_idf(word, book_title, n)\n```\n:::\n\n\n## Document-term matrix\n\nDocuments (in out case, books) can be represented as a document-term matrix, where each row represents a document and each column represents a word. The value in each cell is equal to the frequency of the word in the document. This matrix is sometimes called the bag-of-words representation of the text data (although sometimes that contains 0/1 values based on whether the word appears in the document), because it ignores the ordering of the words in the text. These matrices can be created using the `cast_dtm()` function from the `tidytext` package. Note that these matrices can get very large depending on the size of the vocabulary and the number of documents. Our data has less than 3000 unique words and 2 documents, so it is manageable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# DTM with pivot_wider() (generic tibble)\ndtm <- words |> \n  count(book_title, word) |>\n  pivot_wider(names_from = word, values_from = n, values_fill = 0)\n\n# DTM with cast_dtm() (DocumentTermMatrix object)\ndtm <- words |> \n  count(book_title, word) |>\n  cast_dtm(document = book_title, term = word, value = n)\n```\n:::\n\n\n# Text analysis\n\n## Visualize word frequencies\n\nThe easiest way to represent the contents of a document is to show the most frequent words. We can use a bar chart to show the words with highest term frequency in each book, or use a word cloud where the size of the word is proportional to its frequency (using the `ggwordcloud` package).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggwordcloud)\n\n# Most frequent words\ntf_idf |> \n  group_by(book_title) |> \n  slice_max(tf, n = 20) |> \n  ggplot(aes(tf, reorder_within(word, tf, book_title))) +\n  geom_col() +\n  facet_wrap(~book_title, scales = \"free\") +\n  scale_y_reordered() +\n  labs(x = \"Term frequency\", y = NULL, title = \"Most frequent words\") + \n  theme_minimal()\n  \n# Word cloud\ntf |> \n  group_by(book_title) |> \n  top_n(50, tf) |> \n  ggplot(aes(label = word, size = tf)) +\n  geom_text_wordcloud() +\n  facet_wrap(~book_title) +\n  theme_minimal()\n```\n:::\n\n\nUsing simple word frequencies is often uninformative because common words like \"the\" or \"and\" will dominate the results. One way to address this problem is to display the words with highest tf-idf values: in the case of two books, this will show the words that are unique to each book, as words that show up in all document have idf=tf-idf=0.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf_idf |> \n  group_by(book_title) |> \n  slice_max(tf_idf, n = 20) |> \n  ggplot(aes(tf_idf, reorder_within(word, tf_idf, book_title))) +\n  geom_col() +\n  facet_wrap(~book_title, scales = \"free\") +\n  scale_y_reordered() +\n  labs(x = \"tf-idf\", y = NULL, title = \"Highest tf-idf words\") + \n  theme_minimal()\n```\n:::\n\n\nAn alternative method is to remove common words (called stopwords) from the analysis, using a stopword list. `tidytext` provides a list of stopwords with the `get_stopwords()` function, which can be used to filter out common words from the analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstopwords <- get_stopwords() |> pull(word)\n\ntf_idf |> \n  group_by(book_title) |> \n  # remove stopwords\n  filter(!word %in% stopwords) |>\n  slice_max(tf_idf, n = 20) |> \n  ggplot(aes(tf, reorder_within(word, tf, book_title))) +\n  geom_col() +\n  facet_wrap(~book_title, scales = \"free\") +\n  scale_y_reordered() +\n  labs(x = \"Term frequency\", y = NULL, title = \"Most frequent words (excl. stopwords)\") + \n  theme_minimal()\n```\n:::\n\n\n## Bigrams, n-grams\n\nBigrams are pairs of words that appear next to each other in a document; n-grams are sequences of n words. They can be useful to capture the context in which words appear, as the meaning of a word can depend on the words that surround it. By specifying the `token` argument in the `unnest_tokens()` function, we can split the text into bigrams or n-grams.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbigrams <- books_content |> \n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n```\n:::\n\n\nWe can visualize the most common bigrams the same way we did for unigrams (single words).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbigrams |> \n  drop_na() |> \n  count(book_title, bigram) |> \n  group_by(book_title) |> \n  slice_max(n, n = 20) |> \n  ggplot(aes(n, reorder_within(bigram, n, book_title))) +\n  geom_col() +\n  facet_wrap(~book_title, scales = \"free\") +\n  scale_y_reordered() +\n  labs(x = \"Frequency\", y = NULL, title = \"Most frequent bigrams\") + \n  theme_minimal()\n```\n:::\n\n\nIn addition, we can make use of the extra context information provided by bigrams, and visualize which words are most likely to appear after a given word. For that, we need to separate the bigrams into two columns, one for the first word and one for the second word. To keep the vocabulary relatively small, we will only consider bigrams where neither of the words is a stopword. We can use these frequencies to create a network visualization of the most common bigrams with the `igraph` and `ggraph` packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(igraph)\nlibrary(ggraph)\n\n# create graph object\nbigram_graph <- bigrams |> \n  drop_na() |> \n  # separate bigrams into two columns\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") |> \n  # remove stopwords\n  filter(!word1 %in% stopwords & !word2 %in% stopwords) |> \n  # count word frequencies\n  count(word1, word2) |> \n  # remove bigrams that appear less than 5 times\n  filter(n > 5) |> \n  # create graph object\n  graph_from_data_frame()\n\n# plot graph\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link(aes(edge_width = n), show.legend = FALSE) +\n  geom_node_point() +\n  geom_node_text(aes(label = name), repel = TRUE) +\n  scale_edge_width(range = c(0.1, 2)) +\n  theme_void()\n```\n:::\n\n\n## Sentiment analysis\n\nSentiment analysis is the process of determining the sentiment of a piece of text, i.e. whether it is positive, negative, or neutral. One way to do this is to use a sentiment lexicon, which is a list of words and their associated sentiment scores. There are multiple different sentiment lexicons available, such as Bing, AFINN, and NRC. These differ in their training data and the sentiment categories they use, but are all available with the `get_sentiments()` function. So we can use the tidy words tibble and merge it with the sentiment lexicon to assign sentiment scores to each word.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get sentiment lexicons\nbing <- get_sentiments(\"bing\")\nafinn <- get_sentiments(\"afinn\")\n\n# plot the most common positive and negative words with the Bing lexicon\nwords |> \n  inner_join(bing, by = \"word\") |> \n  count(book_title, word, sentiment) |>\n  group_by(book_title, sentiment) |> \n  slice_max(n, n = 10) |>\n  ggplot(aes(n, reorder_within(word, n, book_title), fill = sentiment)) +\n  geom_col() +\n  scale_y_reordered() +\n  labs(x = \"Frequency\", y = NULL) +\n  facet_wrap(~book_title, scales = \"free\") +\n  theme_minimal()\n\n# calculate sentiment scores per book with the AFINN lexicon\nwords |> \n  count(book_title, word) |>\n  inner_join(afinn, by = \"word\") |> \n  # calculate each word's contribution to the sentiment score\n  mutate(value_n = value * n) |>\n  group_by(book_title) |> \n  # calculate the sentiment score for each book (sum of sentiment scores / number of words)\n  summarize(score = sum(value_n) / sum(n))\n```\n:::\n\n\n## Topic modelling\n\nTopic modelling is a method to discover the topics that are present in a collection of documents. It is an unsupervised learning method, meaning that it does not require labeled data. One popular topic modelling method is latent Dirichlet allocation (LDA), which assumes that each document is a mixture of different topics, and each topic is a mixture of words. The `LDA()` function from the `topicmodels` package can be used to fit an LDA model to a document-term matrix. The function requires a document-term matrix as created by `cast_dtm()`, so first we should create a clean version of our previous `dtm` object (remove stopwords).\n\nBefore we fit a model, we need to decide how many topics to use. If we have previous expectations about what results we want to see, we can choose a specific number of topics, otherwise we can try multiple values until we find sensible results. The model also includes a random initialization step, so it is a good idea to set a seed to ensure that we get the same results every time. In this case, we might try to fit a model with 2 topics if we hope that the model can separate the topics of the two documents.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(topicmodels)\n\ndtm <- words |> \n  count(book_title, word) |>\n  filter(!word %in% stopwords) |>\n  cast_dtm(document = book_title, term = word, value = n)\n\n# fit LDA model with 2 topics\nlda <- LDA(dtm, k = 2, control = list(seed = 1))\n```\n:::\n\n\nWhen interpreting LDA results, we consider two sets of parameters: the document-topic matrix and the topic-word matrix. The document-topic matrix tells us how much of each topic is present in each document, while the topic-word matrix tells us which words are associated with each topic. We can use the `tidy()` function from the `topicmodels` package to extract these matrices into a tidy format, specifying `matrix = \"beta\"` for the topic-word matrix and `matrix = \"gamma\"` for the document-topic matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntopic_word <- tidy(lda, matrix = \"beta\")\ndocument_topic <- tidy(lda, matrix = \"gamma\")\n```\n:::\n\n\nThe topic-word matrix helps us give meaning to the topics by showing which words are the most strongly associated with each topic. We can plot these word probabilities to visualize the topics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntopic_word |> \n  group_by(topic) |> \n  slice_max(beta, n = 10) |> \n  ggplot(aes(beta, reorder_within(term, beta, topic))) +\n  geom_col() +\n  facet_wrap(~topic, scales = \"free\") +\n  scale_y_reordered() +\n  labs(x = \"Per-topic-per-word probability\", y = NULL) + \n  theme_minimal()\n```\n:::\n\n\nIt seems like the topics can separate the two books well, although it might not succeed with a different random seed.\n\nThe per-document-per-topic probabilities confirm that the the documents are clearly split into topics, with each having a near-1 probability for one topic and near-0 for the other.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndocument_topic |> \n  pivot_wider(names_from = topic, values_from = gamma)\n```\n:::\n\n\nNevertheless, topic modelling can be very useful for larger collections of documents, where it can help to identify the main themes present in the corpus.\n\n## Word embeddings\n\nWord embeddings are a way to represent words as vectors in a high-dimensional space, where words with similar meanings are close to each other. There are pre-trained models (such as GloVe or BERT), that were trained on large corpora of text data, but we can also create our own word embeddings from our own data, which will be specific to the context at hand. One popular method to create our own word embeddings is word2vec, which is implemented in the `word2vec` package. word2vec is the simplest type of embedding model, but more complex, more contextualized embeddings form the basis of the current large language models.\n\n`word2vec()` takes a character vector containing the full text, so let's create a 2-element vector where each element corresponds to the full text of one of the books, using the cleaned version of the text from `words`. We can specify a lot of model parameters, such as the dimension of the word vectors (the length of the vector associated with a word) or the context window (the number of words around each word to consider as the context), but we can also go with the default settings. The numerical values of the embeddings aren't informative, the information is in the similarities and differences between different words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(word2vec)\n\ntext <- words |> \n  group_by(book_title) |>\n  # collapse words into a single string\n  summarize(text = paste(word, collapse = \" \")) |> \n  # extract the text as a vector\n  pull(text)\n\n# create word embeddings\nembeddings <- word2vec(text)\n\n# view word embeddings\npredict(embeddings, words$word, type = \"embedding\")[1:6, 1:6]\n```\n:::\n\n\nEmbeddings are useful for evaluating which words are the most similar to a particular word. In this case, similarity doesn't necessarily mean similarity in meaning, but rather that the words could replace each other in a sentence or occur near each other. We can use the generic `predict()` function to find the most similar words to a given word.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# most similar words to \"alice\" and \"pooh\"\npredict(embeddings, c(\"alice\", \"pooh\"), type = \"nearest\", top_n = 5) |> \n  bind_rows()\n```\n:::\n\n\nA more systematic way to evaluate the embeddings is to use them to visualize the words in a lower-dimensional space. We can use principal component analysis (PCA) to reduce the dimensionality of the embeddings to 2 dimensions, and then plot the words in this space. You don't need to understand how PCA works, other than that it takes the initial high-dimensional data, and basically tries to find the directions in which the data varies the most, by looking for combinations of the original variables. So the first two dimensions capture as much variation in the embedding space as possible in two dimensions.\n\nTo get the visualization, we first get all the embedding vectors with the `predict()` function used above, then use the `prcomp()` function to perform PCA, and predict the first two components with the `predict()` function again. We can limit the visualization to the 100 words with the highest tf-idf values to keep the plot readable and remove overlapping to keep the plot readable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get embedding vectors\nvectors <- predict(embeddings, words$word, type = \"embedding\") |> \n  as.data.frame() |> \n  drop_na()\n\n# get PCA dimensions\nprcomp(vectors) |> \n  # predict the first two components\n  predict() |> \n  as.data.frame() |> \n  rownames_to_column(\"word\") |> \n  # keep only the 100 words with the highest tf-idf values\n  filter(word %in% slice_max(tf_idf, tf_idf, n = 100)$word) |>\n  ggplot(aes(PC1, PC2, label = word)) +\n  geom_text(size = 3, check_overlap = TRUE) +\n  theme_void()\n```\n:::\n\n\nApparently, the most different words in the two books are \"balloon\" and \"forest\" along one dimension, while \"christopher\" and \"robin\" are very different from all other words. It doesn't seem like the results make much sense, which is probably because we trained the model on a small sample. The model can perform much better on larger datasets, and indeed much of the power of state-of-the-art language models comes from the large amounts of data they are trained on (together with a very large number of model parameters).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}