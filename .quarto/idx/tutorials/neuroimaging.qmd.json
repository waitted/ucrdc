{"title":"Neuroimaging in R","markdown":{"yaml":{"layout":"page","title":"Neuroimaging in R","date":"Last updated: `r Sys.Date()`"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, error = FALSE)\n```\n\n\nThis document gives a basic tutorial on how to work with structural MRI data stored in NIfTI files. The first half gives a recap of the theory behind MRI machines and what kind of data we are actually working with. The second half is about visualizing neuroimaging data and manipulating it to draw correlation between disease severity and pathological manifestation (more specifically lesion size), by using the dataset including people with post-stroke aphasia. Hope you find it interesting and this tutorial inspires you to explore the topic more! ☺️\n\n## Before we begin: A little recap on MRI\n\nTo start working with neuroimaging, it’s important to understand the basics of MRI and what the images we see actually represent. The following text is based on [Berger’s](https://pmc.ncbi.nlm.nih.gov/articles/PMC1121941/) explanation and [Azhar & Chong’s](https://academic-oup-com.utrechtuniversity.idm.oclc.org/pmj/article/99/1174/894/7148067) paper on MRI imaging (Azhar & Chong, 2023; *Magnetic Resonance Imaging - PMC*, n.d.). Feel free to check these out if something remains unclear!\n\nMagnetic Resonance Imaging (MRI) is a technique used to visualize the internal structures of the body, and in our case, the brain. What we’re trying to do is quantify the properties of different tissues, such as gray matter, white matter, cerebrospinal fluid (CSF), or lesions. To achieve this, we use weighted MRI images. These images don’t have absolute or objective brightness values; instead, their intensities are relative and depend on how the MRI sequence is set up. By using several types of weighted images, we can extract meaningful information.\n\nMRI works because water is magnetic, and our bodies are about 60% water. Each water molecule has two hydrogen atoms, and each hydrogen atom has a single proton at its nucleus, surrounded by one electron. These protons have a property called “spin” which gives a tiny magnetic field – like a miniature bar magnet.\n\nUnder normal conditions, the magnetic fields of these protons are randomly oriented. But when someone is placed in the strong magnetic field of an MRI scanner, more of these protons align with or against the field. The protons that align with the magnetic field are said to be in a “low-energy state”, while the protons that align against the magnetic field are said to be in a “high-energy state”. However, there is an imbalance, as more protons are in the low-energy state (because this is more energetically favorable), so the magnetic fields of the protons aligned with the field don’t get completely cancelled out by the protons aligned against it. What is left, creates a net magnetization vector in the direction of the MRI scanner’s magnetic field. This is what the MRI uses as a “starting point”.\n\nThe MRI scanner then uses a technique called resonance to gather information. It sends out radiofrequency (RF) pulses to the low-energy protons, causing them to flip to the high-energy state. When the RF pulse is turned off, the protons gradually return to their original alignment. As they do, they release a tiny amount of electromagnetic energy, which is picked up by RF coils placed around the body. These signals differ based on the type of tissue, the amount of water or fat it contains, and how the molecules behave. Stronger signals are associated with tissues that have more hydrogen atoms, like in areas of edema or inflammation, while the timing of the signal tells us about the environment and state of the tissue.\n\nTwo important measurements in MRI are T1 and T2 relaxation times, which create contrast in the MRI image:\n\n-   T1 relaxation – How quickly protons realign with the magnetic field\n\n-   T2 relaxation – How quickly protons fall out of sync (lose phase coherence) with each other after the RF pulse, when protons are spinning together in coherence\n\nThese two properties are used to generate T1-weighted and T2-weighted images, which emphasize different tissue characteristics.\n\n**T1-weighted** imagesprovide a clear picture of brain anatomy and is excellent for structural imaging. When a contrast agent is used, active or inflamed lesions can become more visible. On T1 images, fat and suba-cute blood typically appear bright, while fluids like CSF, edema, and lesions appear dark. You can think of T1 like viewing the brain under regular lighting—it shows you structure and clarity.\n\n**T2-weighted** images aremore sensitive to water content and is particularly good for spotting fluid, inflammation, or other pathological changes. In T2 images, things like CSF, edema, cysts, and lesions appear bright, while fat, calcifications, and bone tend to appear dark. You might imagine T2 as looking at the brain under blacklight—suddenly all the ‘wet’ or abnormal areas light up.\n\nSometimes, T2 images are modified using a sequence called **FLAIR** (Fluid-Attenuated Inversion Recovery). CSF appears very bright on T2, which can make it hard to see subtle lesions near the ventricles or cortex. FLAIR solves this by suppressing the CSF signal, allowing small lesions to stand out while maintaining the T2 sensitivity of surrounding tissues. This makes FLAIR ideal for spotting MS plaques, small metastases, or cortical strokes. On FLAIR images, edema, demyelination, tumors, and strokes appear bright, while normal CSF appears dark—like turning off the glow from water to focus on hidden areas.\n\nSee Figure 1. for a visual representation on how T1, T2 and FLAIR images appear from an MRI scan.\n\nThere are of course also other types of sequences, but in this tutorial we will be mainly focusing on structural MRI and more specifically the T1 and T2 weighted images.\n\n![**Figure 1. MRI of a patient with acute hypoglycemia that mimicked symptoms of acute ischemic stroke, but did not show the usual pathologies on the brain scan.** (A) T1 weighted image. Note that the CSF, appearing dark in the ventricles and between the meninges (B) T2 weighted image. Note the CSF appearing bright. (C) FLAIR image. Note that the CSF no longer appears bright, but dark and the signal is less exaggerated.](../../assets/img/neuroimaging/MRI_T1_T2_FLAIR.png)\n\n*Note*: the picture was taken from: Rodriguez-Hernandez A, Babici D, Campbell M, Carranza-Reneteria O, Hammond T. Hypoglycemic hemineglect a stroke mimic. eNeurologicalSci. 2023 Jan 17;30:100444.\n\nNow that we have the basics, let’s try to understand what kind of data is actually portrayed on these MRI scans and what we can do with them.\n\nAn MRI picture is like a 3D map of our brain, which is built from approx. 7 million of tiny building blocks, called voxels. A voxel is a 3D pixel. It’s a little cube of brain tissue, and the MRI machine measures how water behaves in it and assigns it a number. That number becomes the shade of gray in the picture. **Dark areas** have **low numbers** (less signal), while **bright areas** have **high numbers** (more signal). \n\nHowever, whether something appears bright or dark depends on the contrast type (so if it’s T1 or T2 for example). What changes between T1 and T2 is what kind of tissue gives off a strong or weak signal. *For example*: in T1 the CSF appears dark, meaning it has a lower signal, and a lower number assigned. On the other hand, in the T2 the CSF appears bright, meaning it has a higher signal, and a higher number assigned. See Figure 2. of a single, axial slice of the brain, showing how to imagine the portrayal of the “data” on these MRI pictures. Although, here a single slice is presented for better understanding, keep in mind that we usually look at the whole of the brain, which is made up of many-many different slices. \n\n![**Figure 2. Voxels with assigned numbers corresponding to the highlighted area on an MRI axial scan of the brain.** Note, that the areas appearing darker on the image have lower numbers, while areas that appear lighter have higher numbers.](../../assets/img/neuroimaging/Voxels_dark_light_nmbrs.png)\n\n*Note*: this picture was taken from the video: Elizabeth Sweeney -  Neuroimaging Analysis in R \\[Internet\\]. 2019 \\[cited 2025 Apr 18\\]. Available from: <https://www.youtube.com/watch?v=9HkEq01nrco>\n\nIn this tutorial we will be working with **NIfTI files** (in 'nii.' or 'nii.gz' formats), which is the standard file format for storing MRI brain imaging data (both the image data and metadata, like voxel size, orientation etc.). It is a **3D array** (X, Y, Z), where:\n\n-   X is the width (left to right in the brain)\n-   Y is the depth (front/back (anterior/posterior))\n-   and Z is the height (up/down (superior/inferior).\n\nLet’s say that we want to take a slice of the brain at the 125th index “deep” in the brain, including its whole width and height (creating a coronal view). Figure 3. shows how we can do this, and how you should imagine this in the 3D plane. \n\n![**Figure 3. Visual representation about the different axes of the 3D arrays of NIfTI files.** The axes are denoted in black. When we call these different slices we use a format like this: data \\[X, Y, Z\\]. If we want to include the whole width and height, we can just leave a comma at the place of X and Z. So, the red section represents the 125th slice along the Y axis (which is the axis of anterior-to-posterior), including its whole width and whole height (so represented as: \\[, 125 ,\\])](../../assets/img/neuroimaging/3D_Brain_Axes.png)\n\nFor future reference, the different axes of the brain are also visualized in the figure below. It’s good to have this on hand when working with neuroimaging.\n\n![](../../assets/img/neuroimaging/brain_axes.png)\n\n## Description of the ARC dataset\n\nFor this tutorial we will be using the [Aphasia Recovery Cohort (ARC) Dataset](https://openneuro.org/datasets/ds004884/versions/1.0.1), which is a collection of data from multiple studies conducted over several years. This is a large, open-access neuroimaging dataset, that contains longitudinal data from 230 individuals with post-stroke aphasia. The dataset includes multimodal MRI data (T1, T2, FLAIR, fMRI, DWI),  detailed behavioral assessments (via the Western Aphasia Battery (WAB)) and demographic information. The dataset contains scanning sessions across different time points, ranging from days to years post-stroke. \n\nEnrollment requirements included individuals who had experienced a left-hemisphere stroke at least 6 months (or 12 months for some studies) prior to enrollment, between the ages of 21 and 80 years old, with no contraindications to MRI or additional neurological impairments (such as multiple sclerosis, Parkinson’s, dementia, etc). It is important to note that many individuals participated in multiple studies. \n\nIf you would like to learn more about a specific aspect of the study, see the [link](https://www-nature-com.utrechtuniversity.idm.oclc.org/articles/s41597-024-03819-7) taking you to the publication. \n\nHowever, the scans that are included in the ARC dataset are raw MRI scans. This means that the scans came directly from the scanner, with no preprocessing applied. This means that the images are still in the original orientation and the position that the person’s head was during the scan. Because of this and also the variability of brains between individuals makes it difficult to compare multiple brains. Therefore, the MRI scans have to be “spatially normalized”, meaning that they have to be transformed to match a standard template (a common reference brain). Additionally, these images still include non-brain structures like the skull, which provide unnecessary data and thus interfere with our data analysis.\n\nIf you are new to neuroimaging, working with raw data can be tricky, because it often need a lot of pre-processing before it can used in analysis or visualization.\n\nLuckily, the researchers behind the ARC dataset provided post-processed images for educational purposes that can be accessed through this [link](https://github.com/neurolabusc/AphasiaRecoveryCohortDemo). These images include:\n\n-   normalized brain scans, where each scan has been aligned to a standard template,\n-   lesion masks, which highlight the damaged brain regions in each participant’s brain,\n-   lesion incidence maps, which show which brain regions are most affected across the whole group.\n\nThese post-processed images are what we’ll be using for the next steps of the tutorial, as they let us focus on interpreting results rather than struggling with preprocessing.\n\n## Description of the libraries and packages.\n\nWe will be using these packages:\n\n-   `oro.nifti` - This package is the core R package for working with NifTI files. It writes and loads NifTI files into R as 3D or 4D arrays. It’s basic functions include: `readNIfTI()` and `writeNifTI()`\n\n-   `neurobase` - This package is a part of the [Neuroconductor](https://neuroconductor.org/) project, which hosts a collection of R packages specifically for neuroimaging analysis, building on R’s existing ecosystem. It builds on oro.nift, and adds helpful functions for image math, masks, plotting, etc. \n\nThe earlier mentioned Neuroconductor system can also come in handy later for your future projects. If you type this in R you will be able to download and run an R script hosted on the Neuroconductor website:\n\n```{r, eval=FALSE}\nsource(\"https://neuroconductor.org/neurocLite.R\")\n```\n\nThat script defines the `neuro_install()` function, which can be used to install any Neuroconductor package like this:\n\n```{r, eval=FALSE}\nneuro_install(\"neurobase\")\n```\n\nSo, let’s get started!\n\nAs stated earlier, we will be using the post-processed images. For now let’s use the NifTi file that is called “wbsub-M2001_ses-1253x1076_T1w.nii.gz” and you will find it under 'files' within the project's folder.\n\nHere is what each part of this file name means:\n\n-   'sub-M2001' corresponds to the subject ID\n\n-   'ses-1253x1076' means that they likely combined scans from session 1253 and session 1076. The numbers refer to how many days after the stroke the given imaging session took place, so in this case it was 1076 and 1253 days post-stroke\n\n-   'T1w' means the scan type.\n\n# Getting Started\n\n## Loading our packages and files\n\nFirst you will have to install the packages and only then will you be able to load them in from the library:\n\n```{r, eval=FALSE}\ninstall.packages(\"oro.nifti\")\ninstall.packages(\"neurobase\")\ninstall.packages(\"tidyverse\") \n\n```\n\n```{r, eval = FALSE}\nlibrary(oro.nifti) # handling NifTI images\nlibrary(neurobase) # has additional neuroimaging analysis tools\nlibrary(tidyverse) \n\n```\n\nNext we will load in our NIfTI file from patient M2001. Since we are using one file for now, it is easier if you download it into your local folder, where your R file is also saved (! this is important). To load our file we will use the read NIfTI command from the `oro.nifti` package. You can download the file using [this link](https://github.com/ucrdatacenter/projects/raw/refs/heads/main/misc/neuroimaging_tutorial/lesion_masks/wsub-M2069_ses-5818_lesion.nii).\n\n```{r, echo=FALSE, eval = FALSE}\nt1_img_M2001 <- readNIfTI(\"../assets/neuroimaging_data/wbsub-M2001_ses-1253x1076_T1w.nii\", reorient = TRUE)\n```\n\n```{r, eval=FALSE}\nt1_img_M2001 <- readNIfTI(\"wbsub-M2001_ses-1253x1076_T1w.nii\", reorient = TRUE)\n```\n\nThe `reorient = TRUE` ensures that the image is aligned properly in a standard orientation, which is useful for consistency.\n\n## Checking image properties\n\nBefore we can do anything with pretty pictures, we have to inspect the properties of our NIfTI file. It’s good to know the physical size of the voxels, for accurate measurements of brain structures as well as understanding the orientation, so that the anatomical structures are correctly identified and the analyses are consistent across datasets.\n\nFirst, we can check the number of voxels along each axis. This helps us understand the resolution and size of the 3D image. Run this chunk of code and see the output:\n\n```{r, eval = FALSE}\ndim(t1_img_M2001) \n```\n\nHowever, maybe it is smarter to have a comprehensive summary of the NIfTI object, including data type (each voxel’s intensity is stored as a 16-bit signed integer), dimensions, (number of voxels in X,Y and Z planes), pixel dimension (indicates how much each voxel measures) and voxel units.This provides a quick overview of the image’s metadata and structure.\n\n```{r, eval = FALSE}\nt1_img_M2001\n```\n\nNext, we can also list all the metadata slots available in the NIfTI object, which allows us to see what additional information is stored and is accessible within the object. We can do this by typing this:\n\n```{r, eval = FALSE}\nslotNames(t1_img_M2001)\n```\n\nFrom all of this the most important thing is that we know the dimension of the images: 157 × 189 × 156. This means that it comprises 157 voxels in the x-axis, 189 in the y-axis, and 156 in the z-axis.\n\n## Visualizing our brain images\n\nNow let’s look at our image. We can do this two ways. If we want a quick visualization of our image, the `orthographic()` function from the oro.nift package displays NIfTI objects in an orthogonal view, across 3 different planes (axial, sagittal and coronal). This way of looking at our data is more neuroimaging-specific and offers a convenient way to inspect 3D images. The picture is also very cool, and could even be published! You will see the image under “Plots” in R. Run this code to see the visual output we get:\n\n```{r, eval = FALSE}\northographic(t1_img_M2001)\n```\n\nThe other option is to use the `image()` function from the `oro.nifti` package. This shows a mosaic view with all slices in a given orientation from a brain MRI. It might take a few minutes to run the code below. Can you recognize which view these are oriented in?\n\n```{r, eval = FALSE}\nimage(t1_img_M2001)\n```\n\nHere we can see that it portrays consecutive slices row-wise from inferior to superior. We can see that the brain has been well pre-processed. The brain structures are symmetrical and clean, and there is no visible skull tissue, indicating successful brain extraction. There is also a good contrast between the white and gray matter. Keep in mind that this simplifies the portrayal of the brain into axial slices, but in reality we have a 3D array of voxels from the MRI scan.\n\nIf we would like to extract a specific slice from the array we have to specify its position along the 3 axes. We can do this by using the `image()` function from the `oro.nifti` package.\n\nThis line of code displays the sagittal slice at index 125 (so we have to specify which slice we want to get along the X axis, representing the left-to-right dimension). See the description in the beginning with the 3D picture of the brain and the annotations (Figure 3). This can be especially useful for examining structures along the mid-line of the brain or assessing lateralized features.\n\n```{r, eval = FALSE}\nimage(t1_img_M2001[125,,]) \n# the commas represent that we include all indexes along X and Y axes\n\n```\n\nThe X and Y axis on this picture that is produced actually translates as:\n\n-   X axis (on output) is actually the Y plane of the brain (so left-to-right)\n\n-   Y axis (on output) is actually the Z plane of the brain (so front-to-back)\n\n## Lesion volume calculation\n\nLet’s say we would like to calculate the volume of the lesion this patient has. \n\nTo do this, we would need a lesion mask. A lesion mask is a binary image used in neuroimaging to identify and isolate regions of brain tissue that have been damaged due to conditions such as stroke. In this mask, each voxel (a 3D pixel) is assigned a value:​\n\n-   **1** indicates the presence of a lesion\n\n-   **0** indicates healthy or unaffected tissue\n\nThis allows for the calculation of lesion volume by summing the number of voxels labeled as lesions (so 1) and multiplying by the volume of each voxel.\n\nLesion masks can be created from the MRI images, however it has to be done manually with external tools, like FSLeyes. Unfortunately, R does not provide built-in tools for manual lesion delineation.\n\nAlthough it is very interesting to learn how to use these tools, we will be working with a lesion mask that is already provided for subject M2001. This is also available on the GitHub page with the processed images (in the ‘NIfTI’ folder, if you search for your subject with Ctrl+F there will be two files corresponding, one is the MRI scan and one is the extracted brain lesion).\n\nWe read this lesion mask, similar to how we did the MRI scan. You can download all lesion masks files we will work with from [GitHub](https://github.com/ucrdatacenter/projects/tree/main/misc/neuroimaging_tutorial/lesion_masks). In the same folder as your R script, create a folder called \"lesion_masks\"/. Then click on each file in the GitHub folder, and find the \"Download raw file\" button in the top right. Make sure to move the downloaded files into your lesion_masks folder. Then the following file path will work.\n\n```{r, echo=FALSE, eval = FALSE}\nlesion_mask <- readNIfTI(\"../assets/neuroimaging_data/lesion_masks/wsub-M2001_ses-1253x1076_lesion.nii\", reorient = TRUE)\n```\n\n```{r, eval=FALSE}\nlesion_mask <- readNIfTI(\"lesion_masks/wsub-M2001_ses-1253x1076_lesion.nii\", reorient = TRUE)\n```\n\nThen we look at a summary of this NIfTI object. We can do this by simply running the variable we stored the lesion mask in. Keep an eye out on the ‘Pixel Dimension’ as we will need to use this later to calculate the lesion volume:\n\n```{r, eval = FALSE}\nlesion_mask\n```\n\nIf you are curious what this looks like just run `image(lesion_mask)` or `orthographic(lesion_mask)`.\n\nOkay now the calculating begins. First we will count the number of voxels labeled as 1 (remember that 1 indicates the presence of a lesion).\n\n```{r, eval = FALSE}\northographic(lesion_mask)\n\nlesion_voxels <- sum(lesion_mask == 1)\n```\n\nThen, we retrieve voxel dimensions (in mm) via the `pixdim()` function. The array in the NIfTI header contains scaling information for each dimension of the image data (you can get this information by running `lesion_mask@pixdim`).\n\n(The output if we run `lesion_mask\\@pixdim` is : `-1 1 1 1 0 0 0 0`)\n\n-   The first value indicates the orientation of the image axes. For this example it would be -1, which suggests left-handed coordinate system (while 1 would indicate a right-handed system).\n\n-   The second, third and fourth value in this array indicates the voxel dimensions along the x, y, and z axes, respectively. In our case these are all 1.\n\n-   The rest of the values are usually used for other dimensions like time. In our case they are 0, indicating that they are not used in our dataset.\n\nSo we are only interested in the second to fourth value from the ‘pixdim’ array, meaning we will have to filter these out with the use of the square-brackets.\n\n```{r, eval = FALSE}\nvoxel_dims <- pixdim(lesion_mask)[2:4] \n```\n\nThen we calculate the volume of a single voxel in mm³. We will be doing this via the `prod()` function, which is part of R’s base package.\n\n```{r, eval = FALSE}\nvoxel_volume_mm3 <- prod(voxel_dims)\n```\n\nNow we calculate the product of the number of voxels (`lesion_voxels`) and the volume of a single voxel (`voxel_volume_mm3`) to get the total lesion volume.  \n\n```{r, eval = FALSE}\nlesion_volume_mm3 <- lesion_voxels * voxel_volume_mm3\n```\n\nNow we can have a nice output of our result by using the `cat()` base R function, which concatenates and outputs its arguments:\n\n```{r, eval = FALSE}\ncat(\"Lesion Volume:\", lesion_volume_mm3, \"mm³\")\n```\n\nThere is a table also linked to this dataset that contains participants’ information such as sex, age at stroke, race, the days after the stroke when participant took the WAB (Western Aphasia Battery) test, the Aphasia Quotient (AQ) from this test (which is a score out of 100, measuring language function in people with brain injury (like stroke) - see Table 1. on how to interpret the scores for the AQ part of the WAB) and the type of aphasia.\n\n**Table 1. WAB-AQ scores**\n\n| **AQ Score** | **Severity** |\n|--------------|--------------|\n| 0-25         | Very severe  |\n| 26-50        | Severe       |\n| 51-75        | Moderate     |\n| 76+          | Mild         |\n\n*Note*: The information for this table was taken from: [Western Aphasia Battery (WAB) – Strokengine](https://strokengine.ca/en/assessments/western-aphasia-battery-wab/#Compositescores:)\n\nWe can load the table containing all this information by directly from GitHub with the following code. Hit `View(df)` to see the data frame:\n\n```{r, eval = FALSE}\ndf <- read_tsv(\"https://github.com/ucrdatacenter/projects/raw/refs/heads/main/misc/neuroimaging_tutorial/participants.tsv\")\nView(df)\n\n```\n\nLet’s put to use what we learnt. Let’s say we want to see how lesion size and WAB-AQ scores correlate. We can do this by following these steps:\n\nLet’s pick subjects M2001, M2004, M2060 and M2069 who all have the same type of aphasia (anemic, which is a type of aphasia where patients find it hard to find words, but have near-normal speech). \n\nLet’s make a function to calculate the lesion volume using what we did for M2001. We can do this by: \n\n```{r, eval = FALSE}\ncalculate_lesion_volume <- function(mask_path) {\n  lesion_mask <- readNIfTI(mask_path, reorient = TRUE)\n  lesion_voxels <- sum(lesion_mask == 1)\n  voxel_dims <- pixdim(lesion_mask)[2:4]\n  voxel_volume_mm3 <- prod(voxel_dims)\n  lesion_voxels * voxel_volume_mm3\n}\n```\n\nHow this works is the following:\n\n-   the function is called: `calculate_lesion_volume`\n\n-   this function will take a variable that is the path to the lesion mask: `function(mask_path)`\n\n-   the following are just copied from what we did earlier:\n\n```         \nlesion_mask <- readNIfTI(mask_path, reorient = TRUE)\nlesion_voxels <- sum(lesion_mask == 1)\nvoxel_dims <- pixdim(lesion_mask)[2:4]\nvoxel_volume_mm3 <- prod(voxel_dims)\nlesion_volume <- lesion_voxels * voxel_volume_mm3\n```\n\nNow we will use these functions to compute lesion volume per subject. We call function for each subject’s lesion mask files (M2001, M2004, M2060, and M2069). This will return a number (volume in mm³ for each participant), which will be stored in named variables (lesion_volume_2001, lesion_volume_2004 etc. - see on the right under ‘Values’):\n\n```{r, echo=FALSE, eval = FALSE}\nlesion_volume_M2001 <- calculate_lesion_volume(\"../assets/neuroimaging_data/lesion_masks/wsub-M2001_ses-1253x1076_lesion.nii\")\nlesion_volume_M2004 <- calculate_lesion_volume(\"../assets/neuroimaging_data/lesion_masks/wsub-M2006_ses-2381x1773_lesion.nii\")\nlesion_volume_M2060 <- calculate_lesion_volume(\"../assets/neuroimaging_data/lesion_masks/wsub-M2060_ses-220_lesion.nii\")\nlesion_volume_M2069 <- calculate_lesion_volume(\"../assets/neuroimaging_data/lesion_masks/wsub-M2069_ses-5818_lesion.nii\")\n\n```\n\n```{r, eval=FALSE, eval = FALSE}\nlesion_volume_M2001 <- calculate_lesion_volume(\"lesion_masks/wsub-M2001_ses-1253x1076_lesion.nii\")\nlesion_volume_M2004 <- calculate_lesion_volume(\"lesion_masks/wsub-M2006_ses-2381x1773_lesion.nii\")\nlesion_volume_M2060 <- calculate_lesion_volume(\"lesion_masks/wsub-M2060_ses-220_lesion.nii\")\nlesion_volume_M2069 <- calculate_lesion_volume(\"lesion_masks/wsub-M2069_ses-5818_lesion.nii\")\n\n```\n\nNow we will use the table with the participant information.\n\nWe add the lesion volume variable to this table, but first we create a data frame where we store the participant id with the matching lesion volume. We create the new table like this:\n\n```{r, eval = FALSE}\nlesion_volumes <- tibble(\n  participant_id =c(\"sub-M2001\", \"sub-M2004\", \"sub-M2060\", \"sub-M2069\"),\n  lesion_volume_mm3 = c(lesion_volume_M2001, lesion_volume_M2004, lesion_volume_M2060, lesion_volume_M2069)\n)\n\n```\n\nNow we can merge this table with the bigger participants table (`df`) to do analysis. We do so by using the `inner_join()` function, which merges rows from the df and `lesion_volumes` table only where `participant_id` matches in both:\n\n```{r, eval = FALSE}\nmerged_data <- inner_join(df, lesion_volumes, by = \"participant_id\")\n```\n\nFinally, we can do our analysis. Let’s do a correlation analysis to see whether higher WAB-AQ scores correlate positively to higher lesion volumes. The `cor.test()` function performs a Pearson’s correlation test, using the lesion volume and WAB-AQ scores as arguments. This outputs the correlation coefficient, p-value and other statistics.\n\n```{r, eval = FALSE}\ncor.test(merged_data$lesion_volume_mm3, as.numeric(merged_data$wab_aq))\n```\n\nSo the correlation is a negative value (-0.5635701), indicating a trend where larger lesion volume indicates lower WAB-AQ scores. The p-value is \\> 0.05 (it is 0.4364), meaning that this is not statistically significant. However, we have a very small sample size (4 participants) so that can explain why our results are so insignificant.\n\nHowever, it would be nice to visualize this correlation. Let’s create a scatter plot, using ggplot, with the lesion volume on the X-axis, and the WAB-AQ score on the Y-axis.\n\nTo add a regression line we use geom_smooth, where we set method = “lm” to specify the linear model, and additionally set se=FALSE, to hide the shaded confidence interval around the line (optional).\n\n```{r, eval = FALSE}\nggplot(merged_data, aes(x = lesion_volume_mm3, y = wab_aq, label = participant_id)) + \n  geom_point(size = 3, color=\"black\") +  \n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", size=0.6) +  \n  geom_text(vjust = -0.5, size = 3) +  # participant labels\n  labs(\n    title = \"Lesion Volume vs WAB-AQ\",\n    x = \"Lesion Volume (mm³)\",\n    y = \"WAB-AQ Score\"\n  ) +\n  theme_minimal()\n\n```\n\nWe can see a trend that could be negative correlation between higher WAB-AQ scores and larger lesion volumes. Is this a result you expected? You can also explore different aphasia types and see whether this correlation is also present in those.\n\nHope you enjoyed this tutorial and you feel more inspired to explore its potential more in R.\n\nSome other useful resources if you are interested in neuroimaging:\n\n-   The Neuroconductor webpage has three online-courses: [Courses \\| Neuroconductor](https://neuroconductor.org/courses). I highly recommend the one called “Imaging in R”.\n\n-   There are two videos from Elizabeth Sweeney that give a very nice insight into what is possible with Neuroimaging in R. I recommend checking it out if you are interested:\n\n    -   [Elizabeth Sweeney - Neuroimaging Analysis in R](https://www.youtube.com/watch?v=9HkEq01nrco&t=182s) (16:45 minutes)\n\n    -   [Neuroimaging Analysis in R: Image Preprocessing](https://www.youtube.com/watch?v=6tDbdNTwEuA&t=2288s) (51:12 minutes)\n\n# References\n\nAzhar, S., & Chong, L. R. (2023). Clinician’s guide to the basic principles of MRI. *Postgraduate Medical Journal*, *99*(1174), 894–903. <https://doi.org/10.1136/pmj-2022-141998>\n\n*Magnetic resonance imaging—PMC*. (n.d.). Retrieved 14 May 2025, from <https://pmc.ncbi.nlm.nih.gov/articles/PMC1121941/>\n\nRodriguez-Hernandez A, Babici D, Campbell M, Carranza-Reneteria O, Hammond T. Hypoglycemic hemineglect a stroke mimic. eNeurologicalSci. 2023 Jan 17;30:100444.\n\nElizabeth Sweeney - Neuroimaging Analysis in R \\[Internet\\]. 2019 \\[cited 2025 Apr 18\\]. Available from: <https://www.youtube.com/watch?v=9HkEq01nrco>\n\n*Neuroimaging Analysis in R: Image Preprocessing—YouTube*. (n.d.). Retrieved 2 May 2025, from <https://www.youtube.com/watch?v=6tDbdNTwEuA&t=2288s>\n\n*APROCSA dataset*. (n.d.). Retrieved 2 May 2025, from <https://langneurosci.org/aprocsa-dataset/>\n\n*Courses \\| Neuroconductor*. (n.d.). Retrieved 2 May 2025, from <https://neuroconductor.org/courses>\n\n*John Muschelli—About*. (n.d.). Retrieved 2 May 2025, from <https://johnmuschelli.com/>\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, error = FALSE)\n```\n\n# Introduction\n\nThis document gives a basic tutorial on how to work with structural MRI data stored in NIfTI files. The first half gives a recap of the theory behind MRI machines and what kind of data we are actually working with. The second half is about visualizing neuroimaging data and manipulating it to draw correlation between disease severity and pathological manifestation (more specifically lesion size), by using the dataset including people with post-stroke aphasia. Hope you find it interesting and this tutorial inspires you to explore the topic more! ☺️\n\n## Before we begin: A little recap on MRI\n\nTo start working with neuroimaging, it’s important to understand the basics of MRI and what the images we see actually represent. The following text is based on [Berger’s](https://pmc.ncbi.nlm.nih.gov/articles/PMC1121941/) explanation and [Azhar & Chong’s](https://academic-oup-com.utrechtuniversity.idm.oclc.org/pmj/article/99/1174/894/7148067) paper on MRI imaging (Azhar & Chong, 2023; *Magnetic Resonance Imaging - PMC*, n.d.). Feel free to check these out if something remains unclear!\n\nMagnetic Resonance Imaging (MRI) is a technique used to visualize the internal structures of the body, and in our case, the brain. What we’re trying to do is quantify the properties of different tissues, such as gray matter, white matter, cerebrospinal fluid (CSF), or lesions. To achieve this, we use weighted MRI images. These images don’t have absolute or objective brightness values; instead, their intensities are relative and depend on how the MRI sequence is set up. By using several types of weighted images, we can extract meaningful information.\n\nMRI works because water is magnetic, and our bodies are about 60% water. Each water molecule has two hydrogen atoms, and each hydrogen atom has a single proton at its nucleus, surrounded by one electron. These protons have a property called “spin” which gives a tiny magnetic field – like a miniature bar magnet.\n\nUnder normal conditions, the magnetic fields of these protons are randomly oriented. But when someone is placed in the strong magnetic field of an MRI scanner, more of these protons align with or against the field. The protons that align with the magnetic field are said to be in a “low-energy state”, while the protons that align against the magnetic field are said to be in a “high-energy state”. However, there is an imbalance, as more protons are in the low-energy state (because this is more energetically favorable), so the magnetic fields of the protons aligned with the field don’t get completely cancelled out by the protons aligned against it. What is left, creates a net magnetization vector in the direction of the MRI scanner’s magnetic field. This is what the MRI uses as a “starting point”.\n\nThe MRI scanner then uses a technique called resonance to gather information. It sends out radiofrequency (RF) pulses to the low-energy protons, causing them to flip to the high-energy state. When the RF pulse is turned off, the protons gradually return to their original alignment. As they do, they release a tiny amount of electromagnetic energy, which is picked up by RF coils placed around the body. These signals differ based on the type of tissue, the amount of water or fat it contains, and how the molecules behave. Stronger signals are associated with tissues that have more hydrogen atoms, like in areas of edema or inflammation, while the timing of the signal tells us about the environment and state of the tissue.\n\nTwo important measurements in MRI are T1 and T2 relaxation times, which create contrast in the MRI image:\n\n-   T1 relaxation – How quickly protons realign with the magnetic field\n\n-   T2 relaxation – How quickly protons fall out of sync (lose phase coherence) with each other after the RF pulse, when protons are spinning together in coherence\n\nThese two properties are used to generate T1-weighted and T2-weighted images, which emphasize different tissue characteristics.\n\n**T1-weighted** imagesprovide a clear picture of brain anatomy and is excellent for structural imaging. When a contrast agent is used, active or inflamed lesions can become more visible. On T1 images, fat and suba-cute blood typically appear bright, while fluids like CSF, edema, and lesions appear dark. You can think of T1 like viewing the brain under regular lighting—it shows you structure and clarity.\n\n**T2-weighted** images aremore sensitive to water content and is particularly good for spotting fluid, inflammation, or other pathological changes. In T2 images, things like CSF, edema, cysts, and lesions appear bright, while fat, calcifications, and bone tend to appear dark. You might imagine T2 as looking at the brain under blacklight—suddenly all the ‘wet’ or abnormal areas light up.\n\nSometimes, T2 images are modified using a sequence called **FLAIR** (Fluid-Attenuated Inversion Recovery). CSF appears very bright on T2, which can make it hard to see subtle lesions near the ventricles or cortex. FLAIR solves this by suppressing the CSF signal, allowing small lesions to stand out while maintaining the T2 sensitivity of surrounding tissues. This makes FLAIR ideal for spotting MS plaques, small metastases, or cortical strokes. On FLAIR images, edema, demyelination, tumors, and strokes appear bright, while normal CSF appears dark—like turning off the glow from water to focus on hidden areas.\n\nSee Figure 1. for a visual representation on how T1, T2 and FLAIR images appear from an MRI scan.\n\nThere are of course also other types of sequences, but in this tutorial we will be mainly focusing on structural MRI and more specifically the T1 and T2 weighted images.\n\n![**Figure 1. MRI of a patient with acute hypoglycemia that mimicked symptoms of acute ischemic stroke, but did not show the usual pathologies on the brain scan.** (A) T1 weighted image. Note that the CSF, appearing dark in the ventricles and between the meninges (B) T2 weighted image. Note the CSF appearing bright. (C) FLAIR image. Note that the CSF no longer appears bright, but dark and the signal is less exaggerated.](../../assets/img/neuroimaging/MRI_T1_T2_FLAIR.png)\n\n*Note*: the picture was taken from: Rodriguez-Hernandez A, Babici D, Campbell M, Carranza-Reneteria O, Hammond T. Hypoglycemic hemineglect a stroke mimic. eNeurologicalSci. 2023 Jan 17;30:100444.\n\nNow that we have the basics, let’s try to understand what kind of data is actually portrayed on these MRI scans and what we can do with them.\n\nAn MRI picture is like a 3D map of our brain, which is built from approx. 7 million of tiny building blocks, called voxels. A voxel is a 3D pixel. It’s a little cube of brain tissue, and the MRI machine measures how water behaves in it and assigns it a number. That number becomes the shade of gray in the picture. **Dark areas** have **low numbers** (less signal), while **bright areas** have **high numbers** (more signal). \n\nHowever, whether something appears bright or dark depends on the contrast type (so if it’s T1 or T2 for example). What changes between T1 and T2 is what kind of tissue gives off a strong or weak signal. *For example*: in T1 the CSF appears dark, meaning it has a lower signal, and a lower number assigned. On the other hand, in the T2 the CSF appears bright, meaning it has a higher signal, and a higher number assigned. See Figure 2. of a single, axial slice of the brain, showing how to imagine the portrayal of the “data” on these MRI pictures. Although, here a single slice is presented for better understanding, keep in mind that we usually look at the whole of the brain, which is made up of many-many different slices. \n\n![**Figure 2. Voxels with assigned numbers corresponding to the highlighted area on an MRI axial scan of the brain.** Note, that the areas appearing darker on the image have lower numbers, while areas that appear lighter have higher numbers.](../../assets/img/neuroimaging/Voxels_dark_light_nmbrs.png)\n\n*Note*: this picture was taken from the video: Elizabeth Sweeney -  Neuroimaging Analysis in R \\[Internet\\]. 2019 \\[cited 2025 Apr 18\\]. Available from: <https://www.youtube.com/watch?v=9HkEq01nrco>\n\nIn this tutorial we will be working with **NIfTI files** (in 'nii.' or 'nii.gz' formats), which is the standard file format for storing MRI brain imaging data (both the image data and metadata, like voxel size, orientation etc.). It is a **3D array** (X, Y, Z), where:\n\n-   X is the width (left to right in the brain)\n-   Y is the depth (front/back (anterior/posterior))\n-   and Z is the height (up/down (superior/inferior).\n\nLet’s say that we want to take a slice of the brain at the 125th index “deep” in the brain, including its whole width and height (creating a coronal view). Figure 3. shows how we can do this, and how you should imagine this in the 3D plane. \n\n![**Figure 3. Visual representation about the different axes of the 3D arrays of NIfTI files.** The axes are denoted in black. When we call these different slices we use a format like this: data \\[X, Y, Z\\]. If we want to include the whole width and height, we can just leave a comma at the place of X and Z. So, the red section represents the 125th slice along the Y axis (which is the axis of anterior-to-posterior), including its whole width and whole height (so represented as: \\[, 125 ,\\])](../../assets/img/neuroimaging/3D_Brain_Axes.png)\n\nFor future reference, the different axes of the brain are also visualized in the figure below. It’s good to have this on hand when working with neuroimaging.\n\n![](../../assets/img/neuroimaging/brain_axes.png)\n\n## Description of the ARC dataset\n\nFor this tutorial we will be using the [Aphasia Recovery Cohort (ARC) Dataset](https://openneuro.org/datasets/ds004884/versions/1.0.1), which is a collection of data from multiple studies conducted over several years. This is a large, open-access neuroimaging dataset, that contains longitudinal data from 230 individuals with post-stroke aphasia. The dataset includes multimodal MRI data (T1, T2, FLAIR, fMRI, DWI),  detailed behavioral assessments (via the Western Aphasia Battery (WAB)) and demographic information. The dataset contains scanning sessions across different time points, ranging from days to years post-stroke. \n\nEnrollment requirements included individuals who had experienced a left-hemisphere stroke at least 6 months (or 12 months for some studies) prior to enrollment, between the ages of 21 and 80 years old, with no contraindications to MRI or additional neurological impairments (such as multiple sclerosis, Parkinson’s, dementia, etc). It is important to note that many individuals participated in multiple studies. \n\nIf you would like to learn more about a specific aspect of the study, see the [link](https://www-nature-com.utrechtuniversity.idm.oclc.org/articles/s41597-024-03819-7) taking you to the publication. \n\nHowever, the scans that are included in the ARC dataset are raw MRI scans. This means that the scans came directly from the scanner, with no preprocessing applied. This means that the images are still in the original orientation and the position that the person’s head was during the scan. Because of this and also the variability of brains between individuals makes it difficult to compare multiple brains. Therefore, the MRI scans have to be “spatially normalized”, meaning that they have to be transformed to match a standard template (a common reference brain). Additionally, these images still include non-brain structures like the skull, which provide unnecessary data and thus interfere with our data analysis.\n\nIf you are new to neuroimaging, working with raw data can be tricky, because it often need a lot of pre-processing before it can used in analysis or visualization.\n\nLuckily, the researchers behind the ARC dataset provided post-processed images for educational purposes that can be accessed through this [link](https://github.com/neurolabusc/AphasiaRecoveryCohortDemo). These images include:\n\n-   normalized brain scans, where each scan has been aligned to a standard template,\n-   lesion masks, which highlight the damaged brain regions in each participant’s brain,\n-   lesion incidence maps, which show which brain regions are most affected across the whole group.\n\nThese post-processed images are what we’ll be using for the next steps of the tutorial, as they let us focus on interpreting results rather than struggling with preprocessing.\n\n## Description of the libraries and packages.\n\nWe will be using these packages:\n\n-   `oro.nifti` - This package is the core R package for working with NifTI files. It writes and loads NifTI files into R as 3D or 4D arrays. It’s basic functions include: `readNIfTI()` and `writeNifTI()`\n\n-   `neurobase` - This package is a part of the [Neuroconductor](https://neuroconductor.org/) project, which hosts a collection of R packages specifically for neuroimaging analysis, building on R’s existing ecosystem. It builds on oro.nift, and adds helpful functions for image math, masks, plotting, etc. \n\nThe earlier mentioned Neuroconductor system can also come in handy later for your future projects. If you type this in R you will be able to download and run an R script hosted on the Neuroconductor website:\n\n```{r, eval=FALSE}\nsource(\"https://neuroconductor.org/neurocLite.R\")\n```\n\nThat script defines the `neuro_install()` function, which can be used to install any Neuroconductor package like this:\n\n```{r, eval=FALSE}\nneuro_install(\"neurobase\")\n```\n\nSo, let’s get started!\n\nAs stated earlier, we will be using the post-processed images. For now let’s use the NifTi file that is called “wbsub-M2001_ses-1253x1076_T1w.nii.gz” and you will find it under 'files' within the project's folder.\n\nHere is what each part of this file name means:\n\n-   'sub-M2001' corresponds to the subject ID\n\n-   'ses-1253x1076' means that they likely combined scans from session 1253 and session 1076. The numbers refer to how many days after the stroke the given imaging session took place, so in this case it was 1076 and 1253 days post-stroke\n\n-   'T1w' means the scan type.\n\n# Getting Started\n\n## Loading our packages and files\n\nFirst you will have to install the packages and only then will you be able to load them in from the library:\n\n```{r, eval=FALSE}\ninstall.packages(\"oro.nifti\")\ninstall.packages(\"neurobase\")\ninstall.packages(\"tidyverse\") \n\n```\n\n```{r, eval = FALSE}\nlibrary(oro.nifti) # handling NifTI images\nlibrary(neurobase) # has additional neuroimaging analysis tools\nlibrary(tidyverse) \n\n```\n\nNext we will load in our NIfTI file from patient M2001. Since we are using one file for now, it is easier if you download it into your local folder, where your R file is also saved (! this is important). To load our file we will use the read NIfTI command from the `oro.nifti` package. You can download the file using [this link](https://github.com/ucrdatacenter/projects/raw/refs/heads/main/misc/neuroimaging_tutorial/lesion_masks/wsub-M2069_ses-5818_lesion.nii).\n\n```{r, echo=FALSE, eval = FALSE}\nt1_img_M2001 <- readNIfTI(\"../assets/neuroimaging_data/wbsub-M2001_ses-1253x1076_T1w.nii\", reorient = TRUE)\n```\n\n```{r, eval=FALSE}\nt1_img_M2001 <- readNIfTI(\"wbsub-M2001_ses-1253x1076_T1w.nii\", reorient = TRUE)\n```\n\nThe `reorient = TRUE` ensures that the image is aligned properly in a standard orientation, which is useful for consistency.\n\n## Checking image properties\n\nBefore we can do anything with pretty pictures, we have to inspect the properties of our NIfTI file. It’s good to know the physical size of the voxels, for accurate measurements of brain structures as well as understanding the orientation, so that the anatomical structures are correctly identified and the analyses are consistent across datasets.\n\nFirst, we can check the number of voxels along each axis. This helps us understand the resolution and size of the 3D image. Run this chunk of code and see the output:\n\n```{r, eval = FALSE}\ndim(t1_img_M2001) \n```\n\nHowever, maybe it is smarter to have a comprehensive summary of the NIfTI object, including data type (each voxel’s intensity is stored as a 16-bit signed integer), dimensions, (number of voxels in X,Y and Z planes), pixel dimension (indicates how much each voxel measures) and voxel units.This provides a quick overview of the image’s metadata and structure.\n\n```{r, eval = FALSE}\nt1_img_M2001\n```\n\nNext, we can also list all the metadata slots available in the NIfTI object, which allows us to see what additional information is stored and is accessible within the object. We can do this by typing this:\n\n```{r, eval = FALSE}\nslotNames(t1_img_M2001)\n```\n\nFrom all of this the most important thing is that we know the dimension of the images: 157 × 189 × 156. This means that it comprises 157 voxels in the x-axis, 189 in the y-axis, and 156 in the z-axis.\n\n## Visualizing our brain images\n\nNow let’s look at our image. We can do this two ways. If we want a quick visualization of our image, the `orthographic()` function from the oro.nift package displays NIfTI objects in an orthogonal view, across 3 different planes (axial, sagittal and coronal). This way of looking at our data is more neuroimaging-specific and offers a convenient way to inspect 3D images. The picture is also very cool, and could even be published! You will see the image under “Plots” in R. Run this code to see the visual output we get:\n\n```{r, eval = FALSE}\northographic(t1_img_M2001)\n```\n\nThe other option is to use the `image()` function from the `oro.nifti` package. This shows a mosaic view with all slices in a given orientation from a brain MRI. It might take a few minutes to run the code below. Can you recognize which view these are oriented in?\n\n```{r, eval = FALSE}\nimage(t1_img_M2001)\n```\n\nHere we can see that it portrays consecutive slices row-wise from inferior to superior. We can see that the brain has been well pre-processed. The brain structures are symmetrical and clean, and there is no visible skull tissue, indicating successful brain extraction. There is also a good contrast between the white and gray matter. Keep in mind that this simplifies the portrayal of the brain into axial slices, but in reality we have a 3D array of voxels from the MRI scan.\n\nIf we would like to extract a specific slice from the array we have to specify its position along the 3 axes. We can do this by using the `image()` function from the `oro.nifti` package.\n\nThis line of code displays the sagittal slice at index 125 (so we have to specify which slice we want to get along the X axis, representing the left-to-right dimension). See the description in the beginning with the 3D picture of the brain and the annotations (Figure 3). This can be especially useful for examining structures along the mid-line of the brain or assessing lateralized features.\n\n```{r, eval = FALSE}\nimage(t1_img_M2001[125,,]) \n# the commas represent that we include all indexes along X and Y axes\n\n```\n\nThe X and Y axis on this picture that is produced actually translates as:\n\n-   X axis (on output) is actually the Y plane of the brain (so left-to-right)\n\n-   Y axis (on output) is actually the Z plane of the brain (so front-to-back)\n\n## Lesion volume calculation\n\nLet’s say we would like to calculate the volume of the lesion this patient has. \n\nTo do this, we would need a lesion mask. A lesion mask is a binary image used in neuroimaging to identify and isolate regions of brain tissue that have been damaged due to conditions such as stroke. In this mask, each voxel (a 3D pixel) is assigned a value:​\n\n-   **1** indicates the presence of a lesion\n\n-   **0** indicates healthy or unaffected tissue\n\nThis allows for the calculation of lesion volume by summing the number of voxels labeled as lesions (so 1) and multiplying by the volume of each voxel.\n\nLesion masks can be created from the MRI images, however it has to be done manually with external tools, like FSLeyes. Unfortunately, R does not provide built-in tools for manual lesion delineation.\n\nAlthough it is very interesting to learn how to use these tools, we will be working with a lesion mask that is already provided for subject M2001. This is also available on the GitHub page with the processed images (in the ‘NIfTI’ folder, if you search for your subject with Ctrl+F there will be two files corresponding, one is the MRI scan and one is the extracted brain lesion).\n\nWe read this lesion mask, similar to how we did the MRI scan. You can download all lesion masks files we will work with from [GitHub](https://github.com/ucrdatacenter/projects/tree/main/misc/neuroimaging_tutorial/lesion_masks). In the same folder as your R script, create a folder called \"lesion_masks\"/. Then click on each file in the GitHub folder, and find the \"Download raw file\" button in the top right. Make sure to move the downloaded files into your lesion_masks folder. Then the following file path will work.\n\n```{r, echo=FALSE, eval = FALSE}\nlesion_mask <- readNIfTI(\"../assets/neuroimaging_data/lesion_masks/wsub-M2001_ses-1253x1076_lesion.nii\", reorient = TRUE)\n```\n\n```{r, eval=FALSE}\nlesion_mask <- readNIfTI(\"lesion_masks/wsub-M2001_ses-1253x1076_lesion.nii\", reorient = TRUE)\n```\n\nThen we look at a summary of this NIfTI object. We can do this by simply running the variable we stored the lesion mask in. Keep an eye out on the ‘Pixel Dimension’ as we will need to use this later to calculate the lesion volume:\n\n```{r, eval = FALSE}\nlesion_mask\n```\n\nIf you are curious what this looks like just run `image(lesion_mask)` or `orthographic(lesion_mask)`.\n\nOkay now the calculating begins. First we will count the number of voxels labeled as 1 (remember that 1 indicates the presence of a lesion).\n\n```{r, eval = FALSE}\northographic(lesion_mask)\n\nlesion_voxels <- sum(lesion_mask == 1)\n```\n\nThen, we retrieve voxel dimensions (in mm) via the `pixdim()` function. The array in the NIfTI header contains scaling information for each dimension of the image data (you can get this information by running `lesion_mask@pixdim`).\n\n(The output if we run `lesion_mask\\@pixdim` is : `-1 1 1 1 0 0 0 0`)\n\n-   The first value indicates the orientation of the image axes. For this example it would be -1, which suggests left-handed coordinate system (while 1 would indicate a right-handed system).\n\n-   The second, third and fourth value in this array indicates the voxel dimensions along the x, y, and z axes, respectively. In our case these are all 1.\n\n-   The rest of the values are usually used for other dimensions like time. In our case they are 0, indicating that they are not used in our dataset.\n\nSo we are only interested in the second to fourth value from the ‘pixdim’ array, meaning we will have to filter these out with the use of the square-brackets.\n\n```{r, eval = FALSE}\nvoxel_dims <- pixdim(lesion_mask)[2:4] \n```\n\nThen we calculate the volume of a single voxel in mm³. We will be doing this via the `prod()` function, which is part of R’s base package.\n\n```{r, eval = FALSE}\nvoxel_volume_mm3 <- prod(voxel_dims)\n```\n\nNow we calculate the product of the number of voxels (`lesion_voxels`) and the volume of a single voxel (`voxel_volume_mm3`) to get the total lesion volume.  \n\n```{r, eval = FALSE}\nlesion_volume_mm3 <- lesion_voxels * voxel_volume_mm3\n```\n\nNow we can have a nice output of our result by using the `cat()` base R function, which concatenates and outputs its arguments:\n\n```{r, eval = FALSE}\ncat(\"Lesion Volume:\", lesion_volume_mm3, \"mm³\")\n```\n\nThere is a table also linked to this dataset that contains participants’ information such as sex, age at stroke, race, the days after the stroke when participant took the WAB (Western Aphasia Battery) test, the Aphasia Quotient (AQ) from this test (which is a score out of 100, measuring language function in people with brain injury (like stroke) - see Table 1. on how to interpret the scores for the AQ part of the WAB) and the type of aphasia.\n\n**Table 1. WAB-AQ scores**\n\n| **AQ Score** | **Severity** |\n|--------------|--------------|\n| 0-25         | Very severe  |\n| 26-50        | Severe       |\n| 51-75        | Moderate     |\n| 76+          | Mild         |\n\n*Note*: The information for this table was taken from: [Western Aphasia Battery (WAB) – Strokengine](https://strokengine.ca/en/assessments/western-aphasia-battery-wab/#Compositescores:)\n\nWe can load the table containing all this information by directly from GitHub with the following code. Hit `View(df)` to see the data frame:\n\n```{r, eval = FALSE}\ndf <- read_tsv(\"https://github.com/ucrdatacenter/projects/raw/refs/heads/main/misc/neuroimaging_tutorial/participants.tsv\")\nView(df)\n\n```\n\nLet’s put to use what we learnt. Let’s say we want to see how lesion size and WAB-AQ scores correlate. We can do this by following these steps:\n\nLet’s pick subjects M2001, M2004, M2060 and M2069 who all have the same type of aphasia (anemic, which is a type of aphasia where patients find it hard to find words, but have near-normal speech). \n\nLet’s make a function to calculate the lesion volume using what we did for M2001. We can do this by: \n\n```{r, eval = FALSE}\ncalculate_lesion_volume <- function(mask_path) {\n  lesion_mask <- readNIfTI(mask_path, reorient = TRUE)\n  lesion_voxels <- sum(lesion_mask == 1)\n  voxel_dims <- pixdim(lesion_mask)[2:4]\n  voxel_volume_mm3 <- prod(voxel_dims)\n  lesion_voxels * voxel_volume_mm3\n}\n```\n\nHow this works is the following:\n\n-   the function is called: `calculate_lesion_volume`\n\n-   this function will take a variable that is the path to the lesion mask: `function(mask_path)`\n\n-   the following are just copied from what we did earlier:\n\n```         \nlesion_mask <- readNIfTI(mask_path, reorient = TRUE)\nlesion_voxels <- sum(lesion_mask == 1)\nvoxel_dims <- pixdim(lesion_mask)[2:4]\nvoxel_volume_mm3 <- prod(voxel_dims)\nlesion_volume <- lesion_voxels * voxel_volume_mm3\n```\n\nNow we will use these functions to compute lesion volume per subject. We call function for each subject’s lesion mask files (M2001, M2004, M2060, and M2069). This will return a number (volume in mm³ for each participant), which will be stored in named variables (lesion_volume_2001, lesion_volume_2004 etc. - see on the right under ‘Values’):\n\n```{r, echo=FALSE, eval = FALSE}\nlesion_volume_M2001 <- calculate_lesion_volume(\"../assets/neuroimaging_data/lesion_masks/wsub-M2001_ses-1253x1076_lesion.nii\")\nlesion_volume_M2004 <- calculate_lesion_volume(\"../assets/neuroimaging_data/lesion_masks/wsub-M2006_ses-2381x1773_lesion.nii\")\nlesion_volume_M2060 <- calculate_lesion_volume(\"../assets/neuroimaging_data/lesion_masks/wsub-M2060_ses-220_lesion.nii\")\nlesion_volume_M2069 <- calculate_lesion_volume(\"../assets/neuroimaging_data/lesion_masks/wsub-M2069_ses-5818_lesion.nii\")\n\n```\n\n```{r, eval=FALSE, eval = FALSE}\nlesion_volume_M2001 <- calculate_lesion_volume(\"lesion_masks/wsub-M2001_ses-1253x1076_lesion.nii\")\nlesion_volume_M2004 <- calculate_lesion_volume(\"lesion_masks/wsub-M2006_ses-2381x1773_lesion.nii\")\nlesion_volume_M2060 <- calculate_lesion_volume(\"lesion_masks/wsub-M2060_ses-220_lesion.nii\")\nlesion_volume_M2069 <- calculate_lesion_volume(\"lesion_masks/wsub-M2069_ses-5818_lesion.nii\")\n\n```\n\nNow we will use the table with the participant information.\n\nWe add the lesion volume variable to this table, but first we create a data frame where we store the participant id with the matching lesion volume. We create the new table like this:\n\n```{r, eval = FALSE}\nlesion_volumes <- tibble(\n  participant_id =c(\"sub-M2001\", \"sub-M2004\", \"sub-M2060\", \"sub-M2069\"),\n  lesion_volume_mm3 = c(lesion_volume_M2001, lesion_volume_M2004, lesion_volume_M2060, lesion_volume_M2069)\n)\n\n```\n\nNow we can merge this table with the bigger participants table (`df`) to do analysis. We do so by using the `inner_join()` function, which merges rows from the df and `lesion_volumes` table only where `participant_id` matches in both:\n\n```{r, eval = FALSE}\nmerged_data <- inner_join(df, lesion_volumes, by = \"participant_id\")\n```\n\nFinally, we can do our analysis. Let’s do a correlation analysis to see whether higher WAB-AQ scores correlate positively to higher lesion volumes. The `cor.test()` function performs a Pearson’s correlation test, using the lesion volume and WAB-AQ scores as arguments. This outputs the correlation coefficient, p-value and other statistics.\n\n```{r, eval = FALSE}\ncor.test(merged_data$lesion_volume_mm3, as.numeric(merged_data$wab_aq))\n```\n\nSo the correlation is a negative value (-0.5635701), indicating a trend where larger lesion volume indicates lower WAB-AQ scores. The p-value is \\> 0.05 (it is 0.4364), meaning that this is not statistically significant. However, we have a very small sample size (4 participants) so that can explain why our results are so insignificant.\n\nHowever, it would be nice to visualize this correlation. Let’s create a scatter plot, using ggplot, with the lesion volume on the X-axis, and the WAB-AQ score on the Y-axis.\n\nTo add a regression line we use geom_smooth, where we set method = “lm” to specify the linear model, and additionally set se=FALSE, to hide the shaded confidence interval around the line (optional).\n\n```{r, eval = FALSE}\nggplot(merged_data, aes(x = lesion_volume_mm3, y = wab_aq, label = participant_id)) + \n  geom_point(size = 3, color=\"black\") +  \n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", size=0.6) +  \n  geom_text(vjust = -0.5, size = 3) +  # participant labels\n  labs(\n    title = \"Lesion Volume vs WAB-AQ\",\n    x = \"Lesion Volume (mm³)\",\n    y = \"WAB-AQ Score\"\n  ) +\n  theme_minimal()\n\n```\n\nWe can see a trend that could be negative correlation between higher WAB-AQ scores and larger lesion volumes. Is this a result you expected? You can also explore different aphasia types and see whether this correlation is also present in those.\n\nHope you enjoyed this tutorial and you feel more inspired to explore its potential more in R.\n\nSome other useful resources if you are interested in neuroimaging:\n\n-   The Neuroconductor webpage has three online-courses: [Courses \\| Neuroconductor](https://neuroconductor.org/courses). I highly recommend the one called “Imaging in R”.\n\n-   There are two videos from Elizabeth Sweeney that give a very nice insight into what is possible with Neuroimaging in R. I recommend checking it out if you are interested:\n\n    -   [Elizabeth Sweeney - Neuroimaging Analysis in R](https://www.youtube.com/watch?v=9HkEq01nrco&t=182s) (16:45 minutes)\n\n    -   [Neuroimaging Analysis in R: Image Preprocessing](https://www.youtube.com/watch?v=6tDbdNTwEuA&t=2288s) (51:12 minutes)\n\n# References\n\nAzhar, S., & Chong, L. R. (2023). Clinician’s guide to the basic principles of MRI. *Postgraduate Medical Journal*, *99*(1174), 894–903. <https://doi.org/10.1136/pmj-2022-141998>\n\n*Magnetic resonance imaging—PMC*. (n.d.). Retrieved 14 May 2025, from <https://pmc.ncbi.nlm.nih.gov/articles/PMC1121941/>\n\nRodriguez-Hernandez A, Babici D, Campbell M, Carranza-Reneteria O, Hammond T. Hypoglycemic hemineglect a stroke mimic. eNeurologicalSci. 2023 Jan 17;30:100444.\n\nElizabeth Sweeney - Neuroimaging Analysis in R \\[Internet\\]. 2019 \\[cited 2025 Apr 18\\]. Available from: <https://www.youtube.com/watch?v=9HkEq01nrco>\n\n*Neuroimaging Analysis in R: Image Preprocessing—YouTube*. (n.d.). Retrieved 2 May 2025, from <https://www.youtube.com/watch?v=6tDbdNTwEuA&t=2288s>\n\n*APROCSA dataset*. (n.d.). Retrieved 2 May 2025, from <https://langneurosci.org/aprocsa-dataset/>\n\n*Courses \\| Neuroconductor*. (n.d.). Retrieved 2 May 2025, from <https://neuroconductor.org/courses>\n\n*John Muschelli—About*. (n.d.). Retrieved 2 May 2025, from <https://johnmuschelli.com/>\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"neuroimaging.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.555","editor":"visual","theme":"simplex","mainfont":"Cormorant SC","fontsize":"20px","layout":"page","title":"Neuroimaging in R","date":"Last updated: `r Sys.Date()`"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}