{"title":"SCICOGN302 workshop I:<br> Introduction to the CHILDES dataset","markdown":{"yaml":{"layout":"page","title":"SCICOGN302 workshop I:<br> Introduction to the CHILDES dataset","subtitle":"Fall 2024","date":"Last updated: `r Sys.Date()`"},"headingText":"Learning outcomes","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, error = FALSE)\n```\n\n\nIn this tutorial you learn the steps needed to conduct basic text analysis in R using the CHILDES database of child language development. You will need these skills in the small homework assignment following this workshop, and in case you choose to complete a data assignment instead of an experiment for your final poster.\n\nYou learn how to import data into R with the `childesr` package, how to use basic text analysis tools, and how to visualize your results using the `ggplot2` package.\n\n# First steps\n\nThis tutorial assumes that you completed the preparatory steps listed [here](../../SCICOGN302#first_workshop).\n\nIf you get stuck at any point, check the help files of functions (access by running `?functionname`), look at more extensive [Data Center tutorials](../../../tutorials), try googling your question, attend Data Center office hours (TBA) or email [datacenter\\@ucr.nl](mailto:datacenter@ucr.nl).\n\nThe code used in this tutorial is also available on [Github](https://github.com/ucrdatacenter/projects/blob/main/SCICOGN302/2024h2/workshop1code.R).\n\n# Getting data from the CHILDES database\n\nOpen a new script and load the `tidyverse`, `tidytext` and `childesr` packages. We use `tidyverse` for data cleaning and plotting, `tidytext` for text analysis tools, and `childesr` to access the CHILDES database from within R.\n\n```{r}\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(childesr)\n```\n\nThe CHILDES database contains over 50 000 transcripts, featuring children at various ages and speaking various languages. To get an idea of what data is available, have a look at their [searchable online database](https://sla.talkbank.org/TBB/childes).\n\nIn this tutorial use data from a single, English-speaking child called Amy, who has two transcripts available in the \"VanKleeck\" corpus. For your own project(s), you should explore the database further to find a suitable sample.\n\nIn the online database transcripts are available as single raw text files, which would require some data cleaning before you can conveniently work with them in R. If you use the `childesr` package to load the data, then the transcripts will already load as clean dataframes.\n\nYou have two options for how your data should look like:\n\n-   If you only conduct analysis on the word level, you should use the `get_tokens()` function. One row in your dataframe will correspond to one token (a token is generally a single word).\n-   If you want to analyse longer expressions (up to full sentences), you should use the `get_utterances()` function. One row in your dataframe will correspond to one utterance (an utterance is a phrase/expression/sentence (fragment))\n\nIn this tutorial we show examples of how to work with both data types. For the case of studying the transcripts of Amy, you can load the token-based and utterance-based datasets as follows:\n\n```{r}\ntokens <- get_tokens(token = \"*\", collection = \"Eng-NA\", target_child = \"Amy\",\n                     corpus = \"VanKleeck\", role = \"target_child\")\n\nutterances <- get_utterances(collection = \"Eng-NA\", target_child = \"Amy\",\n                             corpus = \"VanKleeck\", role = \"target_child\")\n```\n\nSome notes for using different samples:\n\n-   `token = \"*\"` means to download all words in the transcript, and is a necessary argument of `get_tokens()`.\n-   `collection` and `corpus` specify the location of the transcripts and correspond to the categories in the [online database](https://sla.talkbank.org/TBB/childes).\n-   `role = \"target_child\"` means to download only the speech of the child, and not the parents or researchers having a conversation with the child. For other filters you can use, check the help files of the functions.\n-   Don't try to download large sets of transcripts, such as an entire collection without adding filters. Check the [online database](https://sla.talkbank.org/TBB/childes) to see whether the sample that you are considering has a reasonable size.\n\nYou can view the downloaded dataframes by calling the `View()` function or clicking on the name of the dataframes in the Environment tab.\n\nThe key variables of `tokens` (token-based dataframe) are\n\n-   gloss: token(/word) as used in the speech\n-   stem: stem of the word in gloss\n-   part_of_speech: syntactic function of the word\n-   language, corpus\\_..., collection\\_... speaker\\_..., target_child\\_...: transcript metadata\n\nThe key variables of `utterances` (utterance-based dataframe) are\n\n-   gloss: utterance (built from tokens) as used in the speech\n-   stem: stem of each word in the utterance\n-   part_of_speech: syntactic structure of the utterance\n-   num_tokens: number of tokens in the utterance\n-   language, corpus\\_..., collection\\_... speaker\\_..., target_child\\_...: transcript metadata\n\n# Data manipulation\n\n## Counting frequencies\n\nLet's say you want to know what words Amy uses most frequently. To count how many times each word occurs, you can use the `count()` function. You need to specify the data and the variable that you'd like to count as the function arguments, and assign the result to a new object to store it in R's memory. We use the \"stem\" variable instead of \"gloss\" to ignore suffixes, conjugation, etc. and set `sort = TRUE` to display the results from most to least frequent. Then we can print the top 10 most common words.\n\n```{r}\nn_tokens <- count(tokens, stem, sort = TRUE)\nhead(n_tokens, 10)\n```\n\nYou can see that these results do not tell us much. All of these words are very common and generic, and most don't hold much meaning -- they are so-called stopwords. In addition, the second most common token is blank (i.e. no intelligible word).\n\n## Filtering the data\n\nTo make our results more useful, we should filter the dataset first to keep only relevant observations. One way to do that is with the `filter()` function, which specifies the criteria that the data should meet in order to remain in the filtered dataset using logical expressions. For text analysis the most important logical operators are\n\n-   `==` to mean an exact match (e.g. `gloss == \"cake\"` would only keep observations where Amy said \"cake\")\n-   `%in%` to mean that the value matches one of multiple options (e.g. `gloss %in% c(\"cake\", \"presents\")` would keep observations where Amy said \"cake\" OR \"presents\")\n-   `!` which negates whatever statement it precedes (for exact matches the negated operator becomes `!=`).\n\nYou can also combine logical statements with `&` (and) and `|` (or) connectors. For instance, `gloss %in% c(\"cake\", \"presents\")` is equivalent to `gloss == \"cake\" | gloss == \"presents\"`. Used inside a `filter()` function, this would keep observations where Amy said \"cake\" OR \"presents\".\n\n```{r}\nfilter(tokens, gloss %in% c(\"cake\", \"presents\"))\n```\n\nAnother way to filter a dataset is to use the `anti_join()` function. With this function, you can remove all observations that also occur in a different dataset. This function can be especially useful if you want to exclude particular combinations of multiple variables, but it can also be used to remove stopwords from your dataset.\n\nWe can use the pre-defined dataset `stop_words` from the `stopwords` package to get a list of stopwords. This function creates a dataframe which has a variable `word` that serves as a list of stopwords. We can use the `anti_join()` function to keep only the words in `tokens` that are not in the stopword list. We need to specify the variables that we want to compare in the `by` argument. In our case, the `word` variable in the stop_words dataframe corresponds to the `stem` variable in the tokens dataframe.\n\n```{r}\ntokens_filtered <- anti_join(tokens, stop_words, by = c(\"stem\" = \"word\"))\n```\n\nNow we can use the `count()` function on our filtered data, and the results are very different than before.\n\n```{r}\nn_filtered <- count(tokens_filtered, stem, sort = TRUE)\nhead(n_filtered, 10)\n```\n\nThese words are less generic, and actually look like words that children likely say a lot.\n\n## Tidy workflows\n\nInstead of going through the previous previous two steps by assigning each intermediate result to a new object (see `tokens_filtered`), you can write a linear worflow where you start with your original data, apply each data manipulation step-by-step until you reach your final result.\n\nTo create such a workflow, you need the pipe operator (`%>%`): using it at the end of a line means that the next function uses the previous result as an input. In the following case, it takes the object `tokens` as the first argument of the `anti_join()` function, then introduces an additional filter to remove words where the stem is blank, and finally takes the resulting filtered data as the first argument of the `count()` function.\n\n```{r}\nn_filtered <- tokens %>% \n  anti_join(stop_words, by = c(\"stem\" = \"word\")) %>%\n  filter(stem != \"\") %>% \n  count(stem, sort = TRUE)\n```\n\n## Regular expressions (regex)\n\nWhen working with text data, looking for direct matches with logical operators is often not sufficient. For example if you want to find all utterances that include the phrase \"he is\", `filter(gloss == \"he is\")` will not keep \"he is tall\", because it only looks for exact matches. No more, no less.\n\nTo get results that match a certain pattern (e.g. starting in a specific way or containing a particular sequence), we can use regular expressions. These expressions practically form a language of their own, and their advanced use can get very complex, but the basics are straightforward. You specify the pattern (e.g. contains \"he is\"), and specify the string in which you want to find that pattern (e.g. the gloss variable of the utt dataframe). Then you can use the `str_detect()` function to find matches: `str_detect()` returns a logical vector that you can use in the `filter()` function.\n\n```{r}\nutterances %>% \n  filter(str_detect(gloss, \"he is\"))\n```\n\nAs you can see, Amy said the phrase \"he is\" four times, of which one time she said \"she is\". We can use regular expressions to further narrow down our results. Some of the most useful regex patterns are\n\n-   `^x` starts with x\n-   `x$` ends with x\n-   `(x|y)` x or y\n-   `.` any character\n-   `x*` x any number of times\n\nSo if we wanted to exclude \"she is\" from the previous results, the regular expression `\"(^| )he is\"` would do that: we look for the phrase \"he is\" either at the beginning of the phrase or preceded by whitespace.\n\nFor other regex patterns, see the second page of [this cheat sheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf).\n\n# Plotting with `ggplot`\n\nFigures made with `ggplot` are built from several layers. You always use the same basic code structure to create a wide range of figures:\n\n1.  The `ggplot()` function creates a blank canvas for you to work on.\n2.  Geoms add the visual elements, such as points, lines, bars, or other shapes.\n3.  Other specifications can include changing axis settings, setting the theme, adding labels, etc.\n4.  You connect all these different specifications to each other using `+` signs (similarly as you'd use the pipe operator `%>%`).\n\nThe variables that you want to display on the graph must always be wrapped in an `aes()` function, which stands for aesthetics. This specification tells R to determine the value of the aesthetic (x and y axes, colors, groups, line types, etc.) based on the value of the variable. `aes()` can be specified both in the main `ggplot()` function (in which case it will apply to all geoms) or within a `geom_...()` function (then it only applies to that geom).\n\nCommon plot types include\n\n-   scatterplots (`geom_point()`)\n-   line charts (`geom_line()`)\n-   bar charts (`geom_col()` and `geom_bar()`)\n-   histograms (`geom_histogram()`)\n-   fitted curves (`geom_smooth()`)\n\nFor instance you can create a bar chart of the frequencies of the 10 most common words Amy uses. We can use the `reorder()` function to arrange the columns by frequency.\n\n```{r}\nn_filtered %>% \n  head(10) %>% \n  ggplot() +\n  geom_col(aes(x = n, y = reorder(stem, n))) +\n  labs(x = \"Frequency\", y = \"\")\n```\n\nTo observe language development, it is interesting to look at how various measures change with age. Since we only observe Amy at one age, we can't plot that here, but as an alternative we can use the `utterance_order` variable to see whether Amy's speech patterns change over the course of one conversation.\n\nFor example, we can plot the mean utterance length (equivalent to mean length of utterance (MLU), a common measure of language development) as a scatterplot with a fitted curve. Note that if you specify aesthetics in the `ggplot()` function, they apply to all geoms.\n\n```{r}\nutterances %>% \n  ggplot(aes(x = utterance_order, y = num_tokens)) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"Time in conversation\", y = \"Mean length of utterance\") +\n  theme_light()\n```\n\nFrom this plot we can see that while there are some particularly long utterances Amy made during the conversations, there is no clear pattern of increasing or decreasing utterance length during the conversation.\n\nSince we have two transcripts from Amy, we can compare language development measures in the two transcripts (for your assignment yo can compare multiple children in a similar way). This figure is the same as the previous one, except that it specifies `transcript_id` as the grouping variable that should affect the color of the points, and therefore fits separate lines per transcript.\n\n```{r}\nutterances %>% \n  ggplot(aes(x = utterance_order, y = num_tokens, \n             color = as.character(transcript_id))) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"Time in conversation\", y = \"Mean length of utterance\") +\n  theme_light()\n```\n\nThis plot tells us that the second transcript seems to have a slightly larger MLU, mainly because most of the particularly long utterances occurred in the second conversation.\n\nWe can also use regular expressions to look for particular patterns. For instance, we can check which types of pronouns (denoted `pro:type`) Amy uses the most.\n\n```{r}\ntokens %>% \n  filter(str_detect(part_of_speech, \"pro:\")) %>% \n  count(part_of_speech) %>% \n  ggplot() +\n  geom_col(aes(x = part_of_speech, y = n)) +\n  labs(x = \"Pronoun type\", y = \"Frequency\") +\n  theme_light()\n```\n\nSubjective and personal pronouns (I, you, etc.) are clearly dominating Amy's speech, however, she uses a wide range of different pronouns.\n\nIf you would like a brief introduction to some more useful functions in calculating measures of linguistic development, have a look at the workshop [additional materials](../workshop1extra).\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, error = FALSE)\n```\n\n# Learning outcomes\n\nIn this tutorial you learn the steps needed to conduct basic text analysis in R using the CHILDES database of child language development. You will need these skills in the small homework assignment following this workshop, and in case you choose to complete a data assignment instead of an experiment for your final poster.\n\nYou learn how to import data into R with the `childesr` package, how to use basic text analysis tools, and how to visualize your results using the `ggplot2` package.\n\n# First steps\n\nThis tutorial assumes that you completed the preparatory steps listed [here](../../SCICOGN302#first_workshop).\n\nIf you get stuck at any point, check the help files of functions (access by running `?functionname`), look at more extensive [Data Center tutorials](../../../tutorials), try googling your question, attend Data Center office hours (TBA) or email [datacenter\\@ucr.nl](mailto:datacenter@ucr.nl).\n\nThe code used in this tutorial is also available on [Github](https://github.com/ucrdatacenter/projects/blob/main/SCICOGN302/2024h2/workshop1code.R).\n\n# Getting data from the CHILDES database\n\nOpen a new script and load the `tidyverse`, `tidytext` and `childesr` packages. We use `tidyverse` for data cleaning and plotting, `tidytext` for text analysis tools, and `childesr` to access the CHILDES database from within R.\n\n```{r}\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(childesr)\n```\n\nThe CHILDES database contains over 50 000 transcripts, featuring children at various ages and speaking various languages. To get an idea of what data is available, have a look at their [searchable online database](https://sla.talkbank.org/TBB/childes).\n\nIn this tutorial use data from a single, English-speaking child called Amy, who has two transcripts available in the \"VanKleeck\" corpus. For your own project(s), you should explore the database further to find a suitable sample.\n\nIn the online database transcripts are available as single raw text files, which would require some data cleaning before you can conveniently work with them in R. If you use the `childesr` package to load the data, then the transcripts will already load as clean dataframes.\n\nYou have two options for how your data should look like:\n\n-   If you only conduct analysis on the word level, you should use the `get_tokens()` function. One row in your dataframe will correspond to one token (a token is generally a single word).\n-   If you want to analyse longer expressions (up to full sentences), you should use the `get_utterances()` function. One row in your dataframe will correspond to one utterance (an utterance is a phrase/expression/sentence (fragment))\n\nIn this tutorial we show examples of how to work with both data types. For the case of studying the transcripts of Amy, you can load the token-based and utterance-based datasets as follows:\n\n```{r}\ntokens <- get_tokens(token = \"*\", collection = \"Eng-NA\", target_child = \"Amy\",\n                     corpus = \"VanKleeck\", role = \"target_child\")\n\nutterances <- get_utterances(collection = \"Eng-NA\", target_child = \"Amy\",\n                             corpus = \"VanKleeck\", role = \"target_child\")\n```\n\nSome notes for using different samples:\n\n-   `token = \"*\"` means to download all words in the transcript, and is a necessary argument of `get_tokens()`.\n-   `collection` and `corpus` specify the location of the transcripts and correspond to the categories in the [online database](https://sla.talkbank.org/TBB/childes).\n-   `role = \"target_child\"` means to download only the speech of the child, and not the parents or researchers having a conversation with the child. For other filters you can use, check the help files of the functions.\n-   Don't try to download large sets of transcripts, such as an entire collection without adding filters. Check the [online database](https://sla.talkbank.org/TBB/childes) to see whether the sample that you are considering has a reasonable size.\n\nYou can view the downloaded dataframes by calling the `View()` function or clicking on the name of the dataframes in the Environment tab.\n\nThe key variables of `tokens` (token-based dataframe) are\n\n-   gloss: token(/word) as used in the speech\n-   stem: stem of the word in gloss\n-   part_of_speech: syntactic function of the word\n-   language, corpus\\_..., collection\\_... speaker\\_..., target_child\\_...: transcript metadata\n\nThe key variables of `utterances` (utterance-based dataframe) are\n\n-   gloss: utterance (built from tokens) as used in the speech\n-   stem: stem of each word in the utterance\n-   part_of_speech: syntactic structure of the utterance\n-   num_tokens: number of tokens in the utterance\n-   language, corpus\\_..., collection\\_... speaker\\_..., target_child\\_...: transcript metadata\n\n# Data manipulation\n\n## Counting frequencies\n\nLet's say you want to know what words Amy uses most frequently. To count how many times each word occurs, you can use the `count()` function. You need to specify the data and the variable that you'd like to count as the function arguments, and assign the result to a new object to store it in R's memory. We use the \"stem\" variable instead of \"gloss\" to ignore suffixes, conjugation, etc. and set `sort = TRUE` to display the results from most to least frequent. Then we can print the top 10 most common words.\n\n```{r}\nn_tokens <- count(tokens, stem, sort = TRUE)\nhead(n_tokens, 10)\n```\n\nYou can see that these results do not tell us much. All of these words are very common and generic, and most don't hold much meaning -- they are so-called stopwords. In addition, the second most common token is blank (i.e. no intelligible word).\n\n## Filtering the data\n\nTo make our results more useful, we should filter the dataset first to keep only relevant observations. One way to do that is with the `filter()` function, which specifies the criteria that the data should meet in order to remain in the filtered dataset using logical expressions. For text analysis the most important logical operators are\n\n-   `==` to mean an exact match (e.g. `gloss == \"cake\"` would only keep observations where Amy said \"cake\")\n-   `%in%` to mean that the value matches one of multiple options (e.g. `gloss %in% c(\"cake\", \"presents\")` would keep observations where Amy said \"cake\" OR \"presents\")\n-   `!` which negates whatever statement it precedes (for exact matches the negated operator becomes `!=`).\n\nYou can also combine logical statements with `&` (and) and `|` (or) connectors. For instance, `gloss %in% c(\"cake\", \"presents\")` is equivalent to `gloss == \"cake\" | gloss == \"presents\"`. Used inside a `filter()` function, this would keep observations where Amy said \"cake\" OR \"presents\".\n\n```{r}\nfilter(tokens, gloss %in% c(\"cake\", \"presents\"))\n```\n\nAnother way to filter a dataset is to use the `anti_join()` function. With this function, you can remove all observations that also occur in a different dataset. This function can be especially useful if you want to exclude particular combinations of multiple variables, but it can also be used to remove stopwords from your dataset.\n\nWe can use the pre-defined dataset `stop_words` from the `stopwords` package to get a list of stopwords. This function creates a dataframe which has a variable `word` that serves as a list of stopwords. We can use the `anti_join()` function to keep only the words in `tokens` that are not in the stopword list. We need to specify the variables that we want to compare in the `by` argument. In our case, the `word` variable in the stop_words dataframe corresponds to the `stem` variable in the tokens dataframe.\n\n```{r}\ntokens_filtered <- anti_join(tokens, stop_words, by = c(\"stem\" = \"word\"))\n```\n\nNow we can use the `count()` function on our filtered data, and the results are very different than before.\n\n```{r}\nn_filtered <- count(tokens_filtered, stem, sort = TRUE)\nhead(n_filtered, 10)\n```\n\nThese words are less generic, and actually look like words that children likely say a lot.\n\n## Tidy workflows\n\nInstead of going through the previous previous two steps by assigning each intermediate result to a new object (see `tokens_filtered`), you can write a linear worflow where you start with your original data, apply each data manipulation step-by-step until you reach your final result.\n\nTo create such a workflow, you need the pipe operator (`%>%`): using it at the end of a line means that the next function uses the previous result as an input. In the following case, it takes the object `tokens` as the first argument of the `anti_join()` function, then introduces an additional filter to remove words where the stem is blank, and finally takes the resulting filtered data as the first argument of the `count()` function.\n\n```{r}\nn_filtered <- tokens %>% \n  anti_join(stop_words, by = c(\"stem\" = \"word\")) %>%\n  filter(stem != \"\") %>% \n  count(stem, sort = TRUE)\n```\n\n## Regular expressions (regex)\n\nWhen working with text data, looking for direct matches with logical operators is often not sufficient. For example if you want to find all utterances that include the phrase \"he is\", `filter(gloss == \"he is\")` will not keep \"he is tall\", because it only looks for exact matches. No more, no less.\n\nTo get results that match a certain pattern (e.g. starting in a specific way or containing a particular sequence), we can use regular expressions. These expressions practically form a language of their own, and their advanced use can get very complex, but the basics are straightforward. You specify the pattern (e.g. contains \"he is\"), and specify the string in which you want to find that pattern (e.g. the gloss variable of the utt dataframe). Then you can use the `str_detect()` function to find matches: `str_detect()` returns a logical vector that you can use in the `filter()` function.\n\n```{r}\nutterances %>% \n  filter(str_detect(gloss, \"he is\"))\n```\n\nAs you can see, Amy said the phrase \"he is\" four times, of which one time she said \"she is\". We can use regular expressions to further narrow down our results. Some of the most useful regex patterns are\n\n-   `^x` starts with x\n-   `x$` ends with x\n-   `(x|y)` x or y\n-   `.` any character\n-   `x*` x any number of times\n\nSo if we wanted to exclude \"she is\" from the previous results, the regular expression `\"(^| )he is\"` would do that: we look for the phrase \"he is\" either at the beginning of the phrase or preceded by whitespace.\n\nFor other regex patterns, see the second page of [this cheat sheet](https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf).\n\n# Plotting with `ggplot`\n\nFigures made with `ggplot` are built from several layers. You always use the same basic code structure to create a wide range of figures:\n\n1.  The `ggplot()` function creates a blank canvas for you to work on.\n2.  Geoms add the visual elements, such as points, lines, bars, or other shapes.\n3.  Other specifications can include changing axis settings, setting the theme, adding labels, etc.\n4.  You connect all these different specifications to each other using `+` signs (similarly as you'd use the pipe operator `%>%`).\n\nThe variables that you want to display on the graph must always be wrapped in an `aes()` function, which stands for aesthetics. This specification tells R to determine the value of the aesthetic (x and y axes, colors, groups, line types, etc.) based on the value of the variable. `aes()` can be specified both in the main `ggplot()` function (in which case it will apply to all geoms) or within a `geom_...()` function (then it only applies to that geom).\n\nCommon plot types include\n\n-   scatterplots (`geom_point()`)\n-   line charts (`geom_line()`)\n-   bar charts (`geom_col()` and `geom_bar()`)\n-   histograms (`geom_histogram()`)\n-   fitted curves (`geom_smooth()`)\n\nFor instance you can create a bar chart of the frequencies of the 10 most common words Amy uses. We can use the `reorder()` function to arrange the columns by frequency.\n\n```{r}\nn_filtered %>% \n  head(10) %>% \n  ggplot() +\n  geom_col(aes(x = n, y = reorder(stem, n))) +\n  labs(x = \"Frequency\", y = \"\")\n```\n\nTo observe language development, it is interesting to look at how various measures change with age. Since we only observe Amy at one age, we can't plot that here, but as an alternative we can use the `utterance_order` variable to see whether Amy's speech patterns change over the course of one conversation.\n\nFor example, we can plot the mean utterance length (equivalent to mean length of utterance (MLU), a common measure of language development) as a scatterplot with a fitted curve. Note that if you specify aesthetics in the `ggplot()` function, they apply to all geoms.\n\n```{r}\nutterances %>% \n  ggplot(aes(x = utterance_order, y = num_tokens)) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"Time in conversation\", y = \"Mean length of utterance\") +\n  theme_light()\n```\n\nFrom this plot we can see that while there are some particularly long utterances Amy made during the conversations, there is no clear pattern of increasing or decreasing utterance length during the conversation.\n\nSince we have two transcripts from Amy, we can compare language development measures in the two transcripts (for your assignment yo can compare multiple children in a similar way). This figure is the same as the previous one, except that it specifies `transcript_id` as the grouping variable that should affect the color of the points, and therefore fits separate lines per transcript.\n\n```{r}\nutterances %>% \n  ggplot(aes(x = utterance_order, y = num_tokens, \n             color = as.character(transcript_id))) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"Time in conversation\", y = \"Mean length of utterance\") +\n  theme_light()\n```\n\nThis plot tells us that the second transcript seems to have a slightly larger MLU, mainly because most of the particularly long utterances occurred in the second conversation.\n\nWe can also use regular expressions to look for particular patterns. For instance, we can check which types of pronouns (denoted `pro:type`) Amy uses the most.\n\n```{r}\ntokens %>% \n  filter(str_detect(part_of_speech, \"pro:\")) %>% \n  count(part_of_speech) %>% \n  ggplot() +\n  geom_col(aes(x = part_of_speech, y = n)) +\n  labs(x = \"Pronoun type\", y = \"Frequency\") +\n  theme_light()\n```\n\nSubjective and personal pronouns (I, you, etc.) are clearly dominating Amy's speech, however, she uses a wide range of different pronouns.\n\nIf you would like a brief introduction to some more useful functions in calculating measures of linguistic development, have a look at the workshop [additional materials](../workshop1extra).\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"output-file":"workshop1.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.555","editor":"visual","theme":"simplex","mainfont":"Cormorant SC","fontsize":"20px","layout":"page","title":"SCICOGN302 workshop I:<br> Introduction to the CHILDES dataset","subtitle":"Fall 2024","date":"Last updated: `r Sys.Date()`"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}