{"title":"SCICOGN302:<br> Exploring the CHILDES dataset (draft)","markdown":{"yaml":{"layout":"page","title":"SCICOGN302:<br> Exploring the CHILDES dataset (draft)","subtitle":"Fall 2024","date":"Last updated: `r Sys.Date()`"},"headingText":"Let's check the total number of words in this dataset.","containsRefs":false,"markdown":"\n\nIn this course, you will explore the childes data set in the data science assignments. It is important to get at least a little acquainted with the data set before you pick a research question, as there may not be a way to answer your research question using childes.\n\nIn this example, we will go over one possible analysis that can be done within the data set - comparison of three children and their language development. We will also cover useful codes and some tips and tricks on how to work with this data set. This data exploration does not answer a specific question, but rather provides some insight into this data set and ways it can be used.\n\n```{r}\n#First, download all the important libraries\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(childesr)\nlibrary(qdapDictionaries)\n\n#Let's take a look at all the available corpora in the data set.\ncorpora <- get_corpora()\nView(corpora)\n```\n\nWe are going to work with tokens and utterances a lot, so first, a little explanation on what they actually are. Utterances are data elements from a recorded piece of conversation (such as \"I had good dinner\"). Tokens are specific pieces of utterances (\"I\", \"had\", \"good\", \"dinner\"). Utterances are usually more useful when we look into the development of a child as a whole (syntax, semantics, context of the utterance), whereas with tokens you can look into the development of a child's vocabulary (child's vocabulary complexity and its development over time.)\n\nWhen picking a child/children for your research question, it is very important to make sure you have enough data (utterances and tokens) to work with. In a study done by MacWhinney, B., & Snow, C. (1990) you have a detailed overview of all the researchers who have added to the childes data set. There are some which follow a single child (Snow), some that look into the short-term speech of multiple children (Higginson), and some that follow the mothers and children as well (Howe).\n\nIn this example, we will work with Brown's addition to the data set (Brown's corpora, specifically tokens) - data acquired from three children Adam, Eve and Sarah, collected by Roger Brown and his students. Adam was studied from 2;3 to 4;10; Eve from 1;6 to 2;3; and Sarah from 2;3 to 5;1.\n\n```{r}\n#Get the and tokens from this corpus.\ntokens <- get_tokens(token = \"*\", collection = \"Eng-NA\", \n                     corpus = \"Brown\", role = \"target_child\")\n```\n\nNow we need to decide how to compare the development of these three children's language. We can introduce a variable for whether the each word is present in an English dictionary. This variable will be a logical vector, with `TRUE` and `FALSE` as possible values. The computer treats `TRUE` values as 1 and `FALSE` values as 0, so you can use mathematical operations such as averages on logical data. Then the average of the variable over a certain time period can be interpreted as the share of words that are present in an English dictionary: average values closer to zero would mean the child still has a lot of mumbling/incorrect pronunciation or misspoken words in their vocabulary, whereas a child with a score nearing 1 has a very developed vocabulary. This way, we can assign a value to every token, and then average it per certain time period. Afterwards, we can compare this value progression for all three children. This comparison does not look into syntax of the sentence, meaning that even a sentence \"banana mango apples man\" would be evaluated as entirely correst, when in reality this sentence is nonsensical. Therefore our way of looking into the language development is one-dimensional but there will always be some sort of limitation to conducting research.\n\nIn order to find out which words exist in an English dictionary, we need to choose a dictionary. The `qdapDictionaries` package contains the `GradyAugmented` object, which is a character vector containing over 120,000 English words.\n\n```{r}\nlength(GradyAugmented)\n```\n\nIt is suggested by multiple sources that we need approximately 3000 words to be able to communicate our point in English without any problems (VocabularyFirst, 2019; Yang, 2016). This data set has 40 times that amount, so we can deem it appropriate for our purposes. Even though it's not an exhaustive list of all English words, the chances that a child said a word that is correct and is not on the list above is close to zero.\n\nNow we need to compare the data sets of children's tokens with the valid words in our English dictionary. For this we need to do some data mutating.\n\n```{r}\n# Create a new column 'is_valid' in tokens\nvalid_tokens <- tokens %>%\n  mutate(is_valid = gloss %in% GradyAugmented)\n\n# View the first few rows to verify\nhead(tokens)\n```\n\nSince these children are not studied from the time when they first start speaking and are therefore more likely to stutter or create their own words to fill in gaps in vocabulary, the number will be quite high and approaching 1. However, there will still be some slips, such as in row 2 of adam_tokens, where we can see that Adam used the word choo-choo, which is not grammatically correct, probably to adress a train.\n\nLooking at these \"slips\" may show us how developed their language is. In reality, creating their own words and bridging gaps in their vocabulary is just a natural step in their language development, as it is experimentation with meaning, words structures and sounds (Michigan State University Extension, 2023, Social Sci LibreTexts, 2023). However, for us it shows how \"far\" a child is in their language development journey.\n\nBefore we look into comparison, let's create a graph where we can see the development over time.\n\n```{r}\nvalid_tokens <- tokens |> \n  mutate(is_valid = gloss %in% GradyAugmented,\n         age = floor(target_child_age)) |> \n  group_by(target_child_name, age) %>%\n  summarise(valid_share = mean(is_valid))\n\nvalid_tokens |> \n  ggplot(aes(x = age, y = valid_share, color = target_child_name)) +\n  geom_line() + \n  labs(title = \"Development of Average Valid Word Usage Over Time\",\n       x = \"Age (months)\",\n       y = \"Average Valid Word Usage\") +\n  theme_minimal() \n```\n\nAfter looking at the data we can see that initially, there is an increase of average valid word usage. However, there does not seem to be a stable trend, except for Eve, where there is a stable increase for period of 5 months. Her speech was recorded for shorter period of time than Adam and Sarah, so it's not an entirely fair comparison with the other two. When we compare only Adam and Sarah, we can see that Sarah's speech has stable results after the 40th month of her life of between 80-90% average valid words in speech, while Adam performs somewhat higher, having an average of between 85-95% after the 30th month of his life.\n\nThis way of analyzing tokens may be more suitable for children that are just learning how to speak, as all three data sets suggest that there is a certain development period of life when there is a rise of average valid use of words, and then the data becomes a bit messier, and the trends are harder to find.\n\nAnother way we can analyse and compare their vocabulary development is to look into the uniqueness of words the children use. For this we will track \"valid\" words from their utterances, but compare it against the 500 most common words in the English language to see how many words are NOT from this list (while still being valid words). We can do this as a count of less common words per month. Bear in mind that here we have to make sure we don't count the same word twice in a month!\n\nFirst, let's create a vector with the most common words according to english4today.com, as there is no data set directly in RStudio.\n\nUseful tip: you should not use ChatGPT for writing your code for you, but when it comes to formatting words/numbers into data sets and vectors, it can be quite helpful!\n\n```{r}\n# Create a vector with the provided list of common words\ncommon_words <- c(\"the\", \"of\", \"to\", \"and\", \"a\", \"in\", \"is\", \"it\", \"you\", \"that\", \"he\", \n                  \"was\", \"for\", \"on\", \"are\", \"with\", \"as\", \"I\", \"his\", \"they\", \"be\", \n                  \"at\", \"one\", \"have\", \"this\", \"from\", \"or\", \"had\", \"by\", \"hot\", \"but\", \n                  \"some\", \"what\", \"there\", \"we\", \"can\", \"out\", \"other\", \"were\", \"all\", \n                  \"your\", \"when\", \"up\", \"use\", \"word\", \"how\", \"said\", \"an\", \"each\", \n                  \"she\", \"which\", \"do\", \"their\", \"time\", \"if\", \"will\", \"way\", \"about\", \n                  \"many\", \"then\", \"them\", \"would\", \"write\", \"like\", \"so\", \"these\", \n                  \"her\", \"long\", \"make\", \"thing\", \"see\", \"him\", \"two\", \"has\", \"look\", \n                  \"more\", \"day\", \"could\", \"go\", \"come\", \"did\", \"my\", \"sound\", \"no\", \n                  \"most\", \"number\", \"who\", \"over\", \"know\", \"water\", \"than\", \"call\", \n                  \"first\", \"people\", \"may\", \"down\", \"side\", \"been\", \"now\", \"find\", \n                  \"any\", \"new\", \"work\", \"part\", \"take\", \"get\", \"place\", \"made\", \"live\", \n                  \"where\", \"after\", \"back\", \"little\", \"only\", \"round\", \"man\", \"year\", \n                  \"came\", \"show\", \"every\", \"good\", \"me\", \"give\", \"our\", \"under\", \"name\", \n                  \"very\", \"through\", \"just\", \"form\", \"much\", \"great\", \"think\", \"say\", \n                  \"help\", \"low\", \"line\", \"before\", \"turn\", \"cause\", \"same\", \"mean\", \n                  \"differ\", \"move\", \"right\", \"boy\", \"old\", \"too\", \"does\", \"tell\", \n                  \"sentence\", \"set\", \"three\", \"want\", \"air\", \"well\", \"also\", \"play\", \n                  \"small\", \"end\", \"put\", \"home\", \"read\", \"hand\", \"port\", \"large\", \n                  \"spell\", \"add\", \"even\", \"land\", \"here\", \"must\", \"big\", \"high\", \"such\", \n                  \"follow\", \"act\", \"why\", \"ask\", \"men\", \"change\", \"went\", \"light\", \n                  \"kind\", \"off\", \"need\", \"house\", \"picture\", \"try\", \"us\", \"again\", \n                  \"animal\", \"point\", \"mother\", \"world\", \"near\", \"build\", \"self\", \n                  \"earth\", \"father\", \"head\", \"stand\", \"own\", \"page\", \"should\", \"country\", \n                  \"found\", \"answer\", \"school\", \"grow\", \"study\", \"still\", \"learn\", \n                  \"plant\", \"cover\", \"food\", \"sun\", \"four\", \"thought\", \"let\", \"keep\", \n                  \"eye\", \"never\", \"last\", \"door\", \"between\", \"city\", \"tree\", \"cross\", \n                  \"since\", \"hard\", \"start\", \"might\", \"story\", \"saw\", \"far\", \"sea\", \n                  \"draw\", \"left\", \"late\", \"run\", \"don't\", \"while\", \"press\", \"close\", \n                  \"night\", \"real\", \"life\", \"few\", \"stop\", \"open\", \"seem\", \"together\", \n                  \"next\", \"white\", \"children\", \"begin\", \"got\", \"walk\", \"example\", \"ease\", \n                  \"paper\", \"often\", \"always\", \"music\", \"those\", \"both\", \"mark\", \"book\", \n                  \"letter\", \"until\", \"mile\", \"river\", \"car\", \"feet\", \"care\", \"second\", \n                  \"group\", \"carry\", \"took\", \"rain\", \"eat\", \"room\", \"friend\", \"began\", \n                  \"idea\", \"fish\", \"mountain\", \"north\", \"once\", \"base\", \"hear\", \"horse\", \n                  \"cut\", \"sure\", \"watch\", \"color\", \"face\", \"wood\", \"main\", \"enough\", \n                  \"plain\", \"girl\", \"usual\", \"young\", \"ready\", \"above\", \"ever\", \"red\", \n                  \"list\", \"though\", \"feel\", \"talk\", \"bird\", \"soon\", \"body\", \"dog\", \n                  \"family\", \"direct\", \"pose\", \"leave\", \"song\", \"measure\", \"state\", \n                  \"product\", \"black\", \"short\", \"numeral\", \"class\", \"wind\", \"question\", \n                  \"happen\", \"complete\", \"ship\", \"area\", \"half\", \"rock\", \"order\", \"fire\", \n                  \"south\", \"problem\", \"piece\", \"told\", \"knew\", \"pass\", \"farm\", \"top\", \n                  \"whole\", \"king\", \"size\", \"heard\", \"best\", \"hour\", \"better\", \"true\", \n                  \"during\", \"hundred\", \"am\", \"remember\", \"step\", \"early\", \"hold\", \"west\", \n                  \"ground\", \"interest\", \"reach\", \"fast\", \"five\", \"sing\", \"listen\", \"six\", \n                  \"table\", \"travel\", \"less\", \"morning\", \"ten\", \"simple\", \"several\", \n                  \"vowel\", \"toward\", \"war\", \"lay\", \"against\", \"pattern\", \"slow\", \"center\", \n                  \"love\", \"person\", \"money\", \"serve\", \"appear\", \"road\", \"map\", \"science\", \n                  \"rule\", \"govern\", \"pull\", \"cold\", \"notice\", \"voice\", \"fall\", \"power\", \n                  \"town\", \"fine\", \"certain\", \"fly\", \"unit\", \"lead\", \"cry\", \"dark\", \n                  \"machine\", \"note\", \"wait\", \"plan\", \"figure\", \"star\", \"box\", \"noun\", \n                  \"field\", \"rest\", \"correct\", \"able\", \"pound\", \"done\", \"beauty\", \"drive\", \n                  \"stood\", \"contain\", \"front\", \"teach\", \"week\", \"final\", \"gave\", \"green\", \n                  \"oh\", \"quick\", \"develop\", \"sleep\", \"warm\", \"free\", \"minute\", \"strong\", \n                  \"special\", \"mind\", \"behind\", \"clear\", \"tail\", \"produce\", \"fact\", \n                  \"street\", \"inch\", \"lot\", \"nothing\", \"course\", \"stay\", \"wheel\", \"full\", \n                  \"force\", \"blue\", \"object\", \"decide\", \"surface\", \"deep\", \"moon\", \"island\", \n                  \"foot\", \"yet\", \"busy\", \"test\", \"record\", \"boat\", \"common\", \"gold\", \n                  \"possible\", \"plane\", \"age\", \"dry\", \"wonder\", \"laugh\", \"thousand\", \"ago\", \n                  \"ran\", \"check\", \"game\", \"shape\", \"yes\", \"hot\", \"miss\", \"brought\", \"heat\", \n                  \"snow\", \"bed\", \"bring\", \"sit\", \"perhaps\", \"fill\", \"east\", \"weight\", \n                  \"language\", \"among\")\n\n\n```\n\nNow let's create a new variable in the three children's data sets, that will count the number of less common valid words.\n\n```{r}\n# Create the 'special_word' variable\nspecial_tokens <- tokens |> \n  mutate(is_valid = gloss %in% GradyAugmented,\n         is_common = gloss %in% common_words,\n         is_special = is_valid & !is_common)\n```\n\nNow let's count the number of special words per month for each child. We should also normalize this against all words said that month to have a resulting percentage.\n\n```{r}\nspecial_tokens <- tokens |> \n  mutate(is_valid = gloss %in% GradyAugmented,\n         is_common = gloss %in% common_words,\n         is_special = is_valid & !is_common,\n         age = floor(target_child_age)) |> \n  group_by(target_child_name, age) |> \n  summarize(special_share = mean(is_special))\n\n# Create the plot\nspecial_tokens |> \n  ggplot(aes(x = age, y = special_share, color = target_child_name)) +\n  geom_line() +\n  labs(title = \"Share of Special Words per Age Bracket\",\n       x = \"Age (months)\",\n       y = \"Share of Special Words\") +\n  theme_minimal()\n```\n\nAs an alternative measure, we can consider the share of unique special words out of the total number of unique words. So e.g. if a child says 1000 unique tokens in a month, out of which 200 are valid but not common words, the share of unique special words would be 200/1000=20%. We can calculate this alternative metric by adding one line of code that keeps only unique combinations of child name, age, token, and special word indicator.\n\n```{r}\nunique_special_tokens <- tokens |> \n  mutate(is_valid = gloss %in% GradyAugmented,\n         is_common = gloss %in% common_words,\n         is_special = is_valid & !is_common,\n         age = floor(target_child_age)) |> \n  distinct(target_child_name, age, gloss, is_special) |> \n  group_by(target_child_name, age) |> \n  summarize(special_share = mean(is_special))\n\n# Create the plot\nunique_special_tokens |> \n  ggplot(aes(x = age, y = special_share, color = target_child_name)) +\n  geom_line() +\n  labs(title = \"Share of Special Words per Age Bracket\",\n       x = \"Age (months)\",\n       y = \"Share of Special Words\") +\n  theme_minimal()\n```\n\n```{r, echo=FALSE,eval=FALSE}\n#Adam \n\n# Calculate tokens per month\ntokens_per_month_adam <- adam_tokens %>%\n  group_by(age_bracket) %>%\n  summarise(tokens_per_month = n())\n\n# Calculate special word counts per month\nspecial_word_counts_per_age_bracket_adam <- adam_tokens %>%\n  filter(special_word == 1) %>%\n  group_by(age_bracket, gloss) %>%\n  summarise(count_per_word = n_distinct(gloss), .groups = 'drop') %>%\n  group_by(age_bracket) %>%\n  summarise(count_special_words = n(), .groups = 'drop')\n\n# Add tokens_per_month to special_word_counts_per_age_bracket_adam\nspecial_word_counts_per_age_bracket_adam <- special_word_counts_per_age_bracket_adam %>%\n  left_join(tokens_per_month_adam, by = \"age_bracket\") %>%\n  mutate(normalized_special_word_count = count_special_words / tokens_per_month)\n\n# Create the plot\nggplot(special_word_counts_per_age_bracket_adam, aes(x = age_bracket, y = normalized_special_word_count)) +\n  geom_line(color = \"blue\") +                # Line graph\n  labs(title = \"Count of Special Words per Age Bracket for Adam \",\n       x = \"Age Bracket (Months)\",\n       y = \"Count of Special Words\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n```\n\nAs we can see in the graph, there are too many special words, so we will enlarge the common_words data set to around 1500, which is 1000 words less than the typical vocabulary size of a 5 year old (according to Linguisystems Milestones Guide it should be between 2200-2500). Since these children are looked at from early age until approximately 5 years, I have decided to compare their vocabulary to the one of a 5 year old child. However, as each child is raised in a different environment, there are different words that are considered common around the house, which definitely has the impact on the uniqness of words.\n\nThis data set includes common_words, as well as an additional circa 1000 common words. Since I could not find the data that would include he most commonly spoken 2500 words (average 5 year old vocabulary size), I had to make smaller sized data set. Sometimes the investigation of a research question can be halted because of such reasons, so it's very important to keep in mind the resources and data sets available on the internet. In this case, I included a data set made from two different websites, so there might be an overlap of words, which is why I had to remove duplicates. Furthermore, I put it in alphabetical order for my own convenience.\n\n```{r, echo=FALSE,eval=FALSE}\ncommon_words_2000 <- c(\"the\", \"of\", \"to\", \"and\", \"a\", \"in\", \"is\", \"it\", \"you\", \"that\", \"he\", \"was\", \"for\", \n           \"on\", \"are\", \"with\", \"as\", \"I\", \"his\", \"they\", \"be\", \"at\", \"one\", \"have\", \"this\", \n           \"from\", \"or\", \"had\", \"by\", \"hot\", \"but\", \"some\", \"what\", \"there\", \"we\", \"can\", \"out\", \n           \"other\", \"were\", \"all\", \"your\", \"when\", \"up\", \"use\", \"word\", \"how\", \"said\", \"an\", \n           \"each\", \"she\", \"which\", \"do\", \"their\", \"time\", \"if\", \"will\", \"way\", \"about\", \"many\", \n           \"then\", \"them\", \"would\", \"write\", \"like\", \"so\", \"these\", \"her\", \"long\", \"make\", \n           \"thing\", \"see\", \"him\", \"two\", \"has\", \"look\", \"more\", \"day\", \"could\", \"go\", \"come\", \n           \"did\", \"my\", \"sound\", \"no\", \"most\", \"number\", \"who\", \"over\", \"know\", \"water\", \"than\", \n           \"call\", \"first\", \"people\", \"may\", \"down\", \"side\", \"been\", \"now\", \"find\", \"any\", \"new\", \n           \"work\", \"part\", \"take\", \"get\", \"place\", \"made\", \"live\", \"where\", \"after\", \"back\", \n           \"little\", \"only\", \"round\", \"man\", \"year\", \"came\", \"show\", \"every\", \"good\", \"me\", \n           \"give\", \"our\", \"under\", \"name\", \"very\", \"through\", \"just\", \"form\", \"much\", \"great\", \n           \"think\", \"say\", \"help\", \"low\", \"line\", \"before\", \"turn\", \"cause\", \"same\", \"mean\", \n           \"differ\", \"move\", \"right\", \"boy\", \"old\", \"too\", \"does\", \"tell\", \"sentence\", \"set\", \n           \"three\", \"want\", \"air\", \"well\", \"also\", \"play\", \"small\", \"end\", \"put\", \"home\", \"read\", \n           \"hand\", \"port\", \"large\", \"spell\", \"add\", \"even\", \"land\", \"here\", \"must\", \"big\", \"high\", \n           \"such\", \"follow\", \"act\", \"why\", \"ask\", \"men\", \"change\", \"went\", \"light\", \"kind\", \"off\", \n           \"need\", \"house\", \"picture\", \"try\", \"us\", \"again\", \"animal\", \"point\", \"mother\", \"world\", \n           \"near\", \"build\", \"self\", \"earth\", \"father\", \"head\", \"stand\", \"own\", \"page\", \"should\", \n           \"country\", \"found\", \"answer\", \"school\", \"grow\", \"study\", \"still\", \"learn\", \"plant\", \n           \"cover\", \"food\", \"sun\", \"four\", \"thought\", \"let\", \"keep\", \"eye\", \"never\", \"last\", \n           \"door\", \"between\", \"city\", \"tree\", \"cross\", \"since\", \"hard\", \"start\", \"might\", \"story\", \n           \"saw\", \"far\", \"sea\", \"draw\", \"left\", \"late\", \"run\", \"don't\", \"while\", \"press\", \"close\", \n           \"night\", \"real\", \"life\", \"few\", \"stop\", \"open\", \"seem\", \"together\", \"next\", \"white\", \n           \"children\", \"begin\", \"got\", \"walk\", \"example\", \"ease\", \"paper\", \"often\", \"always\", \n           \"music\", \"those\", \"both\", \"mark\", \"book\", \"letter\", \"until\", \"mile\", \"river\", \"car\", \n           \"feet\", \"care\", \"second\", \"group\", \"carry\", \"took\", \"rain\", \"eat\", \"room\", \"friend\", \n           \"began\", \"idea\", \"fish\", \"mountain\", \"north\", \"once\", \"base\", \"hear\", \"horse\", \"cut\", \n           \"sure\", \"watch\", \"color\", \"face\", \"wood\", \"main\", \"enough\", \"plain\", \"girl\", \"usual\", \n           \"young\", \"ready\", \"above\", \"ever\", \"red\", \"list\", \"though\", \"feel\", \"talk\", \"bird\", \n           \"soon\", \"body\", \"dog\", \"family\", \"direct\", \"pose\", \"leave\", \"song\", \"measure\", \"state\", \n           \"product\", \"black\", \"short\", \"numeral\", \"class\", \"wind\", \"question\", \"happen\", \n           \"complete\", \"ship\", \"area\", \"half\", \"rock\", \"order\", \"fire\", \"south\", \"problem\", \n           \"piece\", \"told\", \"knew\", \"pass\", \"farm\", \"top\", \"whole\", \"king\", \"size\", \"heard\", \n           \"best\", \"hour\", \"better\", \"true\", \"during\", \"hundred\", \"am\", \"remember\", \"step\", \n           \"early\", \"hold\", \"west\", \"ground\", \"interest\", \"reach\", \"fast\", \"five\", \"sing\", \n           \"listen\", \"six\", \"table\", \"travel\", \"less\", \"morning\", \"ten\", \"simple\", \"several\", \n           \"vowel\", \"toward\", \"war\", \"lay\", \"against\", \"pattern\", \"slow\", \"center\", \"love\", \n           \"person\", \"money\", \"serve\", \"appear\", \"road\", \"map\", \"science\", \"rule\", \"govern\", \n           \"pull\", \"cold\", \"notice\", \"voice\", \"fall\", \"power\", \"town\", \"fine\", \"certain\", \n           \"fly\", \"unit\", \"lead\", \"cry\", \"dark\", \"machine\", \"note\", \"wait\", \"plan\", \"figure\", \n           \"star\", \"box\", \"noun\", \"field\", \"rest\", \"correct\", \"able\", \"pound\", \"done\", \n           \"beauty\", \"drive\", \"stood\", \"contain\", \"front\", \"teach\", \"week\", \"final\", \"gave\", \n           \"green\", \"oh\", \"quick\", \"develop\", \"sleep\", \"warm\", \"free\", \"minute\", \"strong\", \n           \"special\", \"mind\", \"behind\", \"clear\", \"tail\", \"produce\", \"fact\", \"street\", \"inch\", \n           \"lot\", \"nothing\", \"course\", \"stay\", \"wheel\", \"full\", \"force\", \"blue\", \"object\", \n           \"decide\", \"surface\", \"deep\", \"moon\", \"island\", \"foot\", \"yet\", \"busy\", \"test\", \n           \"record\", \"boat\", \"common\", \"gold\", \"possible\", \"plane\", \"age\", \"dry\", \"wonder\", \n           \"laugh\", \"thousand\", \"ago\", \"ran\", \"check\", \"game\", \"shape\", \"yes\", \"hot\", \n           \"miss\", \"brought\", \"heat\", \"snow\", \"bed\", \"bring\", \"sit\", \"perhaps\", \"fill\", \n           \"east\", \"weight\", \"language\", \"among\", \"a\", \"abide\", \"ability\", \"able\", \"about\", \"above\",\n    \"abroad\", \"access\", \"accommodation\",\n    \"accomplish\", \"account\", \"accuracy\", \"accurate\", \"achieve\", \"achievement\", \"acknowledge\",\n  \"acquaintance\", \"acquire\", \"across\", \"act\", \"actual\", \"actually\", \"add\", \"additional\",\n  \"address\", \"advance\", \"advantage\", \"advertise\", \"advertisement\", \"advice\", \"advise\",\n  \"advocate\", \"affair\", \"affect\", \"afford\", \"affordable\", \"afraid\", \"after\", \"afternoon\",\n  \"afterwards\", \"again\", \"against\", \"age\", \"ago\", \"agree\", \"agreement\", \"ahead\", \"aid\",\n  \"aim\", \"albeit\", \"alike\", \"alive\", \"all\", \"allow\", \"allowance\", \"almost\", \"alone\",\n  \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"amazing\", \"amend\", \"among\",\n  \"amount\", \"and\", \"anger\", \"angry\", \"ankle\", \"annoy\", \"annoyed\", \"annoying\", \"another\",\n  \"answer\", \"anxious\", \"any\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"apart\", \"apologize\",\n  \"appeal\", \"appear\", \"appearance\", \"apple\", \"application\", \"apply\", \"appointment\", \"appraisal\",\n  \"appreciate\", \"approach\", \"appropriate\", \"are\", \"area\", \"argue\", \"argument\", \"arise\",\n  \"arm\", \"around\", \"arrange\", \"arrangement\", \"array\", \"arrive\", \"art\", \"as\", \"ashamed\",\n  \"aside\", \"ask\", \"asleep\", \"assert\", \"assertive\", \"assess\", \"assessment\", \"asset\",\n  \"assignment\", \"assume\", \"assumption\", \"at\", \"Ate\", \"attach\", \"attached\", \"attempt\",\n  \"attend\", \"attention\", \"attitude\", \"audience\", \"aunt\", \"available\", \"average\", \"avoid\",\n  \"awake\", \"award\", \"aware\", \"awareness\", \"away\", \"awe\", \"awesome\", \"awful\", \"awkward\",\n  \"bachelor\", \"back\", \"background\", \"bad\", \"bag\", \"bake\", \"balance\", \"bald\", \"ball\", \"ban\",\n  \"band\", \"bank\", \"bar\", \"bare\", \"barely\", \"bargain\", \"bark\", \"base\", \"bass\", \"bat\",\n  \"batch\", \"be\", \"beach\", \"beam\", \"bear\", \"beard\", \"bearing\", \"beat\", \"beautiful\", \"because\",\n  \"become\", \"bed\", \"beef\", \"before\", \"beg\", \"begin\", \"beginning\", \"behave\", \"behavior\",\n  \"behind\", \"being\", \"belief\", \"believe\", \"belong\", \"below\", \"belt\", \"bench\", \"bend\",\n  \"beneath\", \"benefit\", \"beside\", \"besides\", \"best\", \"bet\", \"better\", \"between\", \"beyond\",\n  \"bias\", \"biased\", \"bid\", \"big\", \"bill\", \"bind\", \"binding\", \"bird\", \"birthday\", \"bit\",\n  \"bitch\", \"bite\", \"bitter\", \"blame\", \"blanket\", \"blast\", \"blend\", \"blind\", \"block\",\n  \"blood\", \"blow\", \"blue\", \"blunt\", \"board\", \"boast\", \"boat\", \"body\", \"bold\", \"bolt\",\n  \"bond\", \"book\", \"boost\", \"boot\", \"border\", \"bore\", \"bored\", \"boring\", \"born\", \"borrow\",\n  \"bossy\", \"both\", \"bother\", \"bottom\", \"bounce\", \"bound\", \"boundary\", \"bow\", \"bowl\",\n  \"box\", \"branch\", \"brand\", \"brave\", \"breach\", \"bread\", \"break\", \"breakdown\", \"breakfast\",\n  \"breakthrough\", \"breath\", \"breathe\", \"breed\", \"bridge\", \"brief\", \"bright\", \"bring\",\n  \"broad\", \"broadcast\", \"broke\", \"brother\", \"brown\", \"brush\", \"bucket\", \"budget\", \"bug\",\n  \"build\", \"building\", \"bulk\", \"bully\", \"bump\", \"bunch\", \"bundle\", \"burden\", \"burn\",\n  \"burst\", \"bush\", \"business\", \"bust\", \"busy\", \"but\", \"butt\", \"buy\", \"buzz\", \"by\",\n  \"cabbage\", \"cake\", \"calf\", \"call\", \"called\", \"calm\", \"can\", \"cap\", \"car\", \"care\",\n  \"career\", \"careful\", \"carefully\", \"caring\", \"carry\", \"case\", \"cast\", \"cat\", \"catch\",\n  \"cattle\", \"caught\", \"cause\", \"ceiling\", \"certain\", \"certainly\", \"chain\", \"chair\",\n  \"challenge\", \"challenging\", \"chance\", \"chandelier\", \"change\", \"character\", \"charge\",\n  \"charity\", \"charming\", \"chart\", \"chase\", \"cheap\", \"cheat\", \"check\", \"cheek\", \"cheeky\",\n  \"cheer\", \"cheerful\", \"chest\", \"chicken\", \"chief\", \"child\", \"childhood\", \"chill\",\n  \"chin\", \"choice\", \"choose\", \"chop\", \"church\", \"city\", \"claim\", \"class\", \"clay\",\n  \"clean\", \"clear\", \"clerk\", \"clever\", \"cliff\", \"climb\", \"close\", \"clue\", \"clumsy\",\n  \"cluster\", \"coach\", \"coal\", \"coat\", \"cold\", \"colleague\", \"collect\", \"college\", \"come\",\n  \"comfortable\", \"commit\", \"commitment\", \"committed\", \"common\", \"commute\", \"company\",\n  \"compelling\", \"complain\", \"complaint\", \"complete\", \"compliance\", \"comply\", \"compound\",\n  \"comprehensive\", \"compulsory\", \"computer\", \"concern\", \"concerned\", \"conduct\",\n  \"confidence\", \"confident\", \"consider\", \"consist\", \"consistent\", \"constraint\", \"contact\",\n  \"contain\", \"content\", \"control\", \"convenient\", \"convey\", \"cook\", \"cool\", \"cope\", \"core\",\n  \"correct\", \"cost\", \"costume\", \"couch\", \"cough\", \"could\", \"count\", \"counter\", \"country\",\n  \"couple\", \"course\", \"court\", \"cousin\", \"cover\", \"crack\", \"craft\", \"crap\", \"crash\",\n  \"crawl\", \"crazy\", \"create\", \"creep\", \"creepy\", \"crew\", \"crop\", \"cross\", \"crowd\",\n  \"crowded\", \"crush\", \"cry\", \"cuddle\", \"cue\", \"culture\", \"cup\", \"cupboard\", \"curb\",\n  \"currency\", \"current\", \"currently\", \"curse\", \"custom\", \"customer\", \"cut\", \"cute\",\n  \"daily\", \"damage\", \"damn\", \"damp\", \"dance\", \"dangerous\", \"dare\", \"dark\", \"dash\",\n  \"data\", \"date\", \"daughter\", \"dawn\", \"day\", \"dead\", \"deadline\", \"deal\", \"dear\", \"death\",\n  \"deceive\", \"decide\", \"decision\", \"deck\", \"decline\", \"decrease\", \"deed\", \"deem\",\n  \"deep\", \"deer\", \"default\", \"defeat\", \"definitely\", \"degree\", \"delay\", \"delight\",\n  \"delighted\", \"deliver\", \"delivery\", \"demand\", \"demanding\", \"deny\", \"depict\", \"deploy\",\n  \"depth\", \"deserve\", \"design\", \"desire\", \"desk\", \"despite\", \"dessert\", \"determined\",\n  \"develop\", \"development\", \"device\", \"dictionary\", \"die\", \"different\", \"difficult\",\n  \"dig\", \"dim\", \"dinner\", \"dip\", \"dire\", \"dirty\", \"disappointed\", \"disclosure\",\n  \"discover\", \"discuss\", \"disease\", \"disguise\", \"dish\", \"dismiss\", \"display\", \"distress\",\n  \"ditch\", \"dive\", \"dizzy\", \"do\", \"doctor\", \"does\", \"dog\", \"done\", \"door\", \"doubt\",\n  \"down\", \"draft\", \"drag\", \"drain\", \"draw\", \"drawback\", \"drawer\", \"drawing\", \"drawn\",\n  \"dread\", \"dreadful\", \"dream\", \"dress\", \"drift\", \"drill\", \"drink\", \"drive\", \"drop\",\n  \"drought\", \"drown\", \"drug\", \"drunk\", \"dry\", \"duck\", \"dust\", \"duty\", \"each\", \"ear\",\n  \"early\", \"earn\", \"earnings\", \"earth\", \"ease\", \"easier\", \"easily\", \"east\", \"easy\",\n  \"eat\", \"eclectic\", \"edge\", \"educate\", \"education\", \"effect\", \"effective\", \"effort\",\n  \"either\", \"element\", \"elevate\", \"elite\", \"else\", \"embarrassed\", \"embrace\", \"emotion\",\n  \"emotional\", \"employ\", \"employee\", \"employer\", \"enact\", \"end\", \"enemy\", \"energy\",\n  \"engage\", \"engagement\", \"engine\", \"enough\", \"enrich\", \"enroll\", \"ensure\", \"enter\",\n  \"entire\", \"environment\", \"equal\", \"equally\", \"error\", \"escape\", \"especially\", \"essence\",\n  \"establish\", \"estimate\", \"even\", \"event\", \"eventually\", \"ever\", \"every\", \"everybody\",\n  \"everyone\", \"everything\", \"evidence\", \"evolve\", \"exact\", \"exactly\", \"example\", \"excellent\",\n  \"excited\", \"exciting\", \"exercise\", \"exhibit\", \"exist\", \"expect\", \"expensive\", \"experience\",\n  \"expert\", \"explain\", \"expose\", \"extend\", \"extra\", \"eye\", \"fable\", \"face\", \"fact\",\n  \"factory\", \"fail\", \"fair\", \"fall\", \"fame\", \"family\", \"famous\", \"fan\", \"far\", \"fare\",\n  \"fast\", \"fate\", \"fault\", \"fear\", \"feature\", \"feel\", \"feeling\", \"few\", \"fight\", \"file\",\n  \"fill\", \"final\", \"find\", \"fine\", \"finish\", \"fire\", \"firm\", \"first\", \"fish\", \"fit\",\n  \"fix\", \"flame\", \"flash\", \"flat\", \"flee\", \"flight\", \"flip\", \"floor\", \"flow\", \"flower\",\n  \"fly\", \"focus\", \"fold\", \"follow\", \"food\", \"force\", \"forget\", \"form\", \"formal\", \"format\",\n  \"fort\", \"fortune\", \"found\", \"four\", \"fragile\", \"frame\", \"free\", \"freedom\", \"fresh\",\n  \"friend\", \"friendly\", \"frighten\", \"from\", \"front\", \"fruit\", \"full\", \"function\", \"fund\",\n  \"future\", \"gain\", \"game\", \"gap\", \"garbage\", \"garden\", \"gas\", \"gather\", \"gave\", \"general\",\n  \"generally\", \"generate\", \"gentle\", \"get\", \"gift\", \"give\", \"glass\", \"goal\", \"god\", \"gold\",\n  \"good\", \"government\", \"grand\", \"grant\", \"grass\", \"great\", \"green\", \"greet\", \"ground\",\n  \"group\", \"grow\", \"growth\", \"guarantee\", \"guard\", \"guess\", \"guest\", \"guide\", \"guilt\",\n  \"habit\", \"happy\", \"hard\", \"hardly\", \"harmony\", \"harsh\", \"hate\", \"have\", \"he\", \"head\",\n  \"health\", \"heart\", \"heat\", \"help\", \"her\", \"here\", \"hesitate\", \"hi\", \"high\", \"hill\",\n  \"his\", \"history\", \"hit\", \"hold\", \"hole\", \"home\", \"honest\", \"honey\", \"hope\", \"horrible\",\n  \"host\", \"hot\", \"house\", \"how\", \"huge\", \"human\", \"humor\", \"hungry\", \"hurry\", \"hurt\",\n  \"ice\", \"idea\", \"identify\", \"if\", \"ignore\", \"ill\", \"image\", \"imagine\", \"impact\", \"implement\",\n  \"important\", \"improve\", \"in\", \"include\", \"income\", \"increase\", \"influence\", \"inform\",\n  \"information\", \"inspire\", \"instead\", \"interest\", \"into\", \"invest\", \"involve\", \"is\", \"issue\",\n  \"it\", \"item\", \"jack\", \"job\", \"join\", \"joke\", \"judge\", \"juice\", \"just\", \"keep\", \"kind\",\n  \"king\", \"kiss\", \"knee\", \"know\", \"knowledge\", \"lack\", \"lady\", \"land\", \"language\", \"large\",\n  \"last\", \"late\", \"lately\", \"laugh\", \"lawn\", \"lead\", \"learn\", \"lesson\", \"let\", \"letter\",\n  \"lie\", \"life\", \"likely\", \"limit\", \"line\", \"list\", \"listen\", \"live\", \"load\", \"local\",\n  \"lock\", \"long\", \"look\", \"loose\", \"loud\", \"love\", \"luck\", \"lucky\", \"mad\", \"mail\",\n  \"main\", \"make\", \"man\", \"many\", \"market\", \"married\", \"mass\", \"master\", \"match\", \"matter\",\n  \"mean\", \"meaning\", \"measure\", \"medicine\", \"meet\", \"member\", \"memory\", \"mention\",\n  \"merry\", \"message\", \"middle\", \"might\", \"mind\", \"mine\", \"minute\", \"miss\", \"mistake\",\n  \"mix\", \"model\", \"money\", \"more\", \"most\", \"mother\", \"move\", \"much\", \"must\", \"myself\",\n  \"name\", \"nature\", \"near\", \"need\", \"negative\", \"network\", \"never\", \"new\", \"news\", \"nice\",\n  \"night\", \"none\", \"normal\", \"not\", \"note\", \"nothing\", \"now\", \"number\", \"object\",\n  \"obtain\", \"obvious\", \"occasion\", \"off\", \"offer\", \"office\", \"often\", \"on\", \"one\",\n  \"only\", \"open\", \"opportunity\", \"order\", \"organization\", \"others\", \"out\", \"over\", \"own\",\n  \"pack\", \"pain\", \"part\", \"party\", \"pass\", \"path\", \"patient\", \"pay\", \"peace\", \"pen\",\n  \"people\", \"perfect\", \"perhaps\", \"period\", \"place\", \"plan\", \"play\", \"point\", \"policy\",\n  \"poor\", \"position\", \"possible\", \"potential\", \"power\", \"prepare\", \"present\", \"press\",\n  \"price\", \"private\", \"problem\", \"process\", \"produce\", \"product\", \"professional\", \"profit\",\n  \"progress\", \"project\", \"promise\", \"proper\", \"protect\", \"public\", \"purpose\", \"quality\",\n  \"question\", \"quick\", \"quiet\", \"raise\", \"rate\", \"read\", \"ready\", \"real\", \"reality\",\n  \"reason\", \"receive\", \"recent\", \"record\", \"reduce\", \"reflect\", \"refuse\", \"regard\",\n  \"region\", \"regret\", \"relate\", \"relationship\", \"release\", \"rely\", \"remain\", \"remember\",\n  \"remove\", \"rent\", \"reply\", \"report\", \"represent\", \"request\", \"require\", \"research\",\n  \"resource\", \"respond\", \"result\", \"return\", \"reveal\", \"rich\", \"right\", \"risk\", \"road\",\n  \"role\", \"room\", \"rule\", \"run\", \"safe\", \"same\", \"save\", \"school\", \"score\", \"search\",\n  \"season\", \"see\", \"self\", \"sell\", \"sense\", \"serve\", \"service\", \"set\", \"share\", \"short\",\n  \"show\", \"side\", \"sign\", \"simple\", \"since\", \"site\", \"size\", \"social\", \"some\", \"soon\",\n  \"sort\", \"space\", \"special\", \"spend\", \"stand\", \"start\", \"state\", \"status\", \"stay\",\n  \"step\", \"still\", \"store\", \"strategy\", \"study\", \"stuff\", \"success\", \"such\", \"suggest\",\n  \"support\", \"sure\", \"table\", \"take\", \"task\", \"team\", \"tell\", \"term\", \"test\", \"than\",\n  \"that\", \"their\", \"them\", \"then\", \"there\", \"these\", \"they\", \"thing\", \"this\", \"time\",\n  \"to\", \"today\", \"together\", \"tomorrow\", \"too\", \"total\", \"touch\", \"toward\", \"trade\",\n  \"train\", \"transport\", \"travel\", \"try\", \"turn\", \"type\", \"understand\", \"union\", \"unit\",\n  \"until\", \"up\", \"use\", \"value\", \"very\", \"view\", \"visit\", \"wait\", \"walk\", \"want\",\n  \"warm\", \"way\", \"we\", \"well\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\",\n  \"why\", \"wide\", \"will\", \"win\", \"with\", \"work\", \"world\", \"worry\", \"write\", \"year\",\n  \"young\", \"your\", \"yourself\")\n\n# Remove duplicates\ncommon_words_unique <- unique(common_words_2000)\n\n# Sort alphabetically\ncommon_words_sorted <- sort(common_words_unique)\n\n```\n\nAfter looking at the graphs, we can see that all three children use language that is developed and also contains more unique words. However, that does not tell us directly whether their other language skills such as syntax are also on par with where they should be with their language development.\n\nWhen it comes to their vocabulary specifically, we can see the use of a more developed language, compared to an average 5 year old, or even a little more advanced than that. That is true for all of them even from a younger age, where they use less common words. This may be explainable by their environment - if the child's father is a doctor, they may come in contact with more advanced words from the medical field, simply due to existing in the same household. What can also be the case is that they do know some more advanced words, but lack in basics.\n\nOnce again, we can't really fully compare Eve with Adam and Sarah, as she does not have data coverage over extended periods of time. However, when we compare the special words Adam has, we can notice that Eve has the higher unique words usage, which suggests her vocabulary is more advanced. This finding is interesting, as Adam has a higher average of valid words used over time compared with Eve. Sarah's data are rather inconclusive, as she shows very high usage of unique words at first, and then it drops below 60%. Overall, these data don't show as much of a trend as we'd hope for an investigation. It's key to play around with data and find good tools to reach more definite conclusion.\n\nTo conclude, we looked into three children's tokens, and did two analyses - one that looked into their average usage of valid words over time using an English vocabulary, and a second one that looked into special words usage per month. We can conclude that Eve does not have enough data to be compared with Sarah and Adam, but all of them have enough data to be analysed independently. Furthermore, Adam shows more advancements compared to Sarah when it comes to average valid word usage, but Eve has higher unique word usage.\n\nList of sources: Hsu, M.-L. (2013). Language play: The development of linguistic consciousness and creative speech in early childhood education. In Advances in early education and day care (Vol. 17, pp. 127–139). Emerald Group Publishing Limited. \\[<https://doi.org/10.1108/S0270-4021(2013)0000012007>\\]\\[<https://doi.org/10.1108/S0270-4021(2013)0000012007>\\]\n\nThe Education Hub. (n.d.). Effective vocabulary instruction. The Education Hub. <https://theeducationhub.org.nz/effective-vocabulary-instruction/>\n\nChildes-db. (2019). Childes-db: A flexible and reproducible interface to the child language data exchange system. Journal of Child Language, 51, 1928–1941. <https://doi.org/10.1017/s0305000900013866>\n\nWordReference. (n.d.). Top 2000 English words. WordReference. <https://lists.wordreference.com/show/Top-2000-English-words.1/>\n\nVocabularyFirst. (2019). How many words do I need to speak English language? VocabularyFirst. <https://www.vocabularyfirst.com/how-many-words-do-i-need-to-know/>\n\nYang, D. (2016). How many words do you need to know to be fluent in English? Day Translations. <https://www.daytranslations.com/blog/how-many-words-to-be-fluent-in-english/>\n\nSocial Sci LibreTexts. (2023). Language Development in Early Childhood.\n\nMichigan State University Extension. (2023). Language development – Part 2: Principles that are the stem and branch of speech.\n","srcMarkdownNoYaml":"\n\nIn this course, you will explore the childes data set in the data science assignments. It is important to get at least a little acquainted with the data set before you pick a research question, as there may not be a way to answer your research question using childes.\n\nIn this example, we will go over one possible analysis that can be done within the data set - comparison of three children and their language development. We will also cover useful codes and some tips and tricks on how to work with this data set. This data exploration does not answer a specific question, but rather provides some insight into this data set and ways it can be used.\n\n```{r}\n#First, download all the important libraries\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(childesr)\nlibrary(qdapDictionaries)\n\n#Let's take a look at all the available corpora in the data set.\ncorpora <- get_corpora()\nView(corpora)\n```\n\nWe are going to work with tokens and utterances a lot, so first, a little explanation on what they actually are. Utterances are data elements from a recorded piece of conversation (such as \"I had good dinner\"). Tokens are specific pieces of utterances (\"I\", \"had\", \"good\", \"dinner\"). Utterances are usually more useful when we look into the development of a child as a whole (syntax, semantics, context of the utterance), whereas with tokens you can look into the development of a child's vocabulary (child's vocabulary complexity and its development over time.)\n\nWhen picking a child/children for your research question, it is very important to make sure you have enough data (utterances and tokens) to work with. In a study done by MacWhinney, B., & Snow, C. (1990) you have a detailed overview of all the researchers who have added to the childes data set. There are some which follow a single child (Snow), some that look into the short-term speech of multiple children (Higginson), and some that follow the mothers and children as well (Howe).\n\nIn this example, we will work with Brown's addition to the data set (Brown's corpora, specifically tokens) - data acquired from three children Adam, Eve and Sarah, collected by Roger Brown and his students. Adam was studied from 2;3 to 4;10; Eve from 1;6 to 2;3; and Sarah from 2;3 to 5;1.\n\n```{r}\n#Get the and tokens from this corpus.\ntokens <- get_tokens(token = \"*\", collection = \"Eng-NA\", \n                     corpus = \"Brown\", role = \"target_child\")\n```\n\nNow we need to decide how to compare the development of these three children's language. We can introduce a variable for whether the each word is present in an English dictionary. This variable will be a logical vector, with `TRUE` and `FALSE` as possible values. The computer treats `TRUE` values as 1 and `FALSE` values as 0, so you can use mathematical operations such as averages on logical data. Then the average of the variable over a certain time period can be interpreted as the share of words that are present in an English dictionary: average values closer to zero would mean the child still has a lot of mumbling/incorrect pronunciation or misspoken words in their vocabulary, whereas a child with a score nearing 1 has a very developed vocabulary. This way, we can assign a value to every token, and then average it per certain time period. Afterwards, we can compare this value progression for all three children. This comparison does not look into syntax of the sentence, meaning that even a sentence \"banana mango apples man\" would be evaluated as entirely correst, when in reality this sentence is nonsensical. Therefore our way of looking into the language development is one-dimensional but there will always be some sort of limitation to conducting research.\n\nIn order to find out which words exist in an English dictionary, we need to choose a dictionary. The `qdapDictionaries` package contains the `GradyAugmented` object, which is a character vector containing over 120,000 English words.\n\n```{r}\n# Let's check the total number of words in this dataset.\nlength(GradyAugmented)\n```\n\nIt is suggested by multiple sources that we need approximately 3000 words to be able to communicate our point in English without any problems (VocabularyFirst, 2019; Yang, 2016). This data set has 40 times that amount, so we can deem it appropriate for our purposes. Even though it's not an exhaustive list of all English words, the chances that a child said a word that is correct and is not on the list above is close to zero.\n\nNow we need to compare the data sets of children's tokens with the valid words in our English dictionary. For this we need to do some data mutating.\n\n```{r}\n# Create a new column 'is_valid' in tokens\nvalid_tokens <- tokens %>%\n  mutate(is_valid = gloss %in% GradyAugmented)\n\n# View the first few rows to verify\nhead(tokens)\n```\n\nSince these children are not studied from the time when they first start speaking and are therefore more likely to stutter or create their own words to fill in gaps in vocabulary, the number will be quite high and approaching 1. However, there will still be some slips, such as in row 2 of adam_tokens, where we can see that Adam used the word choo-choo, which is not grammatically correct, probably to adress a train.\n\nLooking at these \"slips\" may show us how developed their language is. In reality, creating their own words and bridging gaps in their vocabulary is just a natural step in their language development, as it is experimentation with meaning, words structures and sounds (Michigan State University Extension, 2023, Social Sci LibreTexts, 2023). However, for us it shows how \"far\" a child is in their language development journey.\n\nBefore we look into comparison, let's create a graph where we can see the development over time.\n\n```{r}\nvalid_tokens <- tokens |> \n  mutate(is_valid = gloss %in% GradyAugmented,\n         age = floor(target_child_age)) |> \n  group_by(target_child_name, age) %>%\n  summarise(valid_share = mean(is_valid))\n\nvalid_tokens |> \n  ggplot(aes(x = age, y = valid_share, color = target_child_name)) +\n  geom_line() + \n  labs(title = \"Development of Average Valid Word Usage Over Time\",\n       x = \"Age (months)\",\n       y = \"Average Valid Word Usage\") +\n  theme_minimal() \n```\n\nAfter looking at the data we can see that initially, there is an increase of average valid word usage. However, there does not seem to be a stable trend, except for Eve, where there is a stable increase for period of 5 months. Her speech was recorded for shorter period of time than Adam and Sarah, so it's not an entirely fair comparison with the other two. When we compare only Adam and Sarah, we can see that Sarah's speech has stable results after the 40th month of her life of between 80-90% average valid words in speech, while Adam performs somewhat higher, having an average of between 85-95% after the 30th month of his life.\n\nThis way of analyzing tokens may be more suitable for children that are just learning how to speak, as all three data sets suggest that there is a certain development period of life when there is a rise of average valid use of words, and then the data becomes a bit messier, and the trends are harder to find.\n\nAnother way we can analyse and compare their vocabulary development is to look into the uniqueness of words the children use. For this we will track \"valid\" words from their utterances, but compare it against the 500 most common words in the English language to see how many words are NOT from this list (while still being valid words). We can do this as a count of less common words per month. Bear in mind that here we have to make sure we don't count the same word twice in a month!\n\nFirst, let's create a vector with the most common words according to english4today.com, as there is no data set directly in RStudio.\n\nUseful tip: you should not use ChatGPT for writing your code for you, but when it comes to formatting words/numbers into data sets and vectors, it can be quite helpful!\n\n```{r}\n# Create a vector with the provided list of common words\ncommon_words <- c(\"the\", \"of\", \"to\", \"and\", \"a\", \"in\", \"is\", \"it\", \"you\", \"that\", \"he\", \n                  \"was\", \"for\", \"on\", \"are\", \"with\", \"as\", \"I\", \"his\", \"they\", \"be\", \n                  \"at\", \"one\", \"have\", \"this\", \"from\", \"or\", \"had\", \"by\", \"hot\", \"but\", \n                  \"some\", \"what\", \"there\", \"we\", \"can\", \"out\", \"other\", \"were\", \"all\", \n                  \"your\", \"when\", \"up\", \"use\", \"word\", \"how\", \"said\", \"an\", \"each\", \n                  \"she\", \"which\", \"do\", \"their\", \"time\", \"if\", \"will\", \"way\", \"about\", \n                  \"many\", \"then\", \"them\", \"would\", \"write\", \"like\", \"so\", \"these\", \n                  \"her\", \"long\", \"make\", \"thing\", \"see\", \"him\", \"two\", \"has\", \"look\", \n                  \"more\", \"day\", \"could\", \"go\", \"come\", \"did\", \"my\", \"sound\", \"no\", \n                  \"most\", \"number\", \"who\", \"over\", \"know\", \"water\", \"than\", \"call\", \n                  \"first\", \"people\", \"may\", \"down\", \"side\", \"been\", \"now\", \"find\", \n                  \"any\", \"new\", \"work\", \"part\", \"take\", \"get\", \"place\", \"made\", \"live\", \n                  \"where\", \"after\", \"back\", \"little\", \"only\", \"round\", \"man\", \"year\", \n                  \"came\", \"show\", \"every\", \"good\", \"me\", \"give\", \"our\", \"under\", \"name\", \n                  \"very\", \"through\", \"just\", \"form\", \"much\", \"great\", \"think\", \"say\", \n                  \"help\", \"low\", \"line\", \"before\", \"turn\", \"cause\", \"same\", \"mean\", \n                  \"differ\", \"move\", \"right\", \"boy\", \"old\", \"too\", \"does\", \"tell\", \n                  \"sentence\", \"set\", \"three\", \"want\", \"air\", \"well\", \"also\", \"play\", \n                  \"small\", \"end\", \"put\", \"home\", \"read\", \"hand\", \"port\", \"large\", \n                  \"spell\", \"add\", \"even\", \"land\", \"here\", \"must\", \"big\", \"high\", \"such\", \n                  \"follow\", \"act\", \"why\", \"ask\", \"men\", \"change\", \"went\", \"light\", \n                  \"kind\", \"off\", \"need\", \"house\", \"picture\", \"try\", \"us\", \"again\", \n                  \"animal\", \"point\", \"mother\", \"world\", \"near\", \"build\", \"self\", \n                  \"earth\", \"father\", \"head\", \"stand\", \"own\", \"page\", \"should\", \"country\", \n                  \"found\", \"answer\", \"school\", \"grow\", \"study\", \"still\", \"learn\", \n                  \"plant\", \"cover\", \"food\", \"sun\", \"four\", \"thought\", \"let\", \"keep\", \n                  \"eye\", \"never\", \"last\", \"door\", \"between\", \"city\", \"tree\", \"cross\", \n                  \"since\", \"hard\", \"start\", \"might\", \"story\", \"saw\", \"far\", \"sea\", \n                  \"draw\", \"left\", \"late\", \"run\", \"don't\", \"while\", \"press\", \"close\", \n                  \"night\", \"real\", \"life\", \"few\", \"stop\", \"open\", \"seem\", \"together\", \n                  \"next\", \"white\", \"children\", \"begin\", \"got\", \"walk\", \"example\", \"ease\", \n                  \"paper\", \"often\", \"always\", \"music\", \"those\", \"both\", \"mark\", \"book\", \n                  \"letter\", \"until\", \"mile\", \"river\", \"car\", \"feet\", \"care\", \"second\", \n                  \"group\", \"carry\", \"took\", \"rain\", \"eat\", \"room\", \"friend\", \"began\", \n                  \"idea\", \"fish\", \"mountain\", \"north\", \"once\", \"base\", \"hear\", \"horse\", \n                  \"cut\", \"sure\", \"watch\", \"color\", \"face\", \"wood\", \"main\", \"enough\", \n                  \"plain\", \"girl\", \"usual\", \"young\", \"ready\", \"above\", \"ever\", \"red\", \n                  \"list\", \"though\", \"feel\", \"talk\", \"bird\", \"soon\", \"body\", \"dog\", \n                  \"family\", \"direct\", \"pose\", \"leave\", \"song\", \"measure\", \"state\", \n                  \"product\", \"black\", \"short\", \"numeral\", \"class\", \"wind\", \"question\", \n                  \"happen\", \"complete\", \"ship\", \"area\", \"half\", \"rock\", \"order\", \"fire\", \n                  \"south\", \"problem\", \"piece\", \"told\", \"knew\", \"pass\", \"farm\", \"top\", \n                  \"whole\", \"king\", \"size\", \"heard\", \"best\", \"hour\", \"better\", \"true\", \n                  \"during\", \"hundred\", \"am\", \"remember\", \"step\", \"early\", \"hold\", \"west\", \n                  \"ground\", \"interest\", \"reach\", \"fast\", \"five\", \"sing\", \"listen\", \"six\", \n                  \"table\", \"travel\", \"less\", \"morning\", \"ten\", \"simple\", \"several\", \n                  \"vowel\", \"toward\", \"war\", \"lay\", \"against\", \"pattern\", \"slow\", \"center\", \n                  \"love\", \"person\", \"money\", \"serve\", \"appear\", \"road\", \"map\", \"science\", \n                  \"rule\", \"govern\", \"pull\", \"cold\", \"notice\", \"voice\", \"fall\", \"power\", \n                  \"town\", \"fine\", \"certain\", \"fly\", \"unit\", \"lead\", \"cry\", \"dark\", \n                  \"machine\", \"note\", \"wait\", \"plan\", \"figure\", \"star\", \"box\", \"noun\", \n                  \"field\", \"rest\", \"correct\", \"able\", \"pound\", \"done\", \"beauty\", \"drive\", \n                  \"stood\", \"contain\", \"front\", \"teach\", \"week\", \"final\", \"gave\", \"green\", \n                  \"oh\", \"quick\", \"develop\", \"sleep\", \"warm\", \"free\", \"minute\", \"strong\", \n                  \"special\", \"mind\", \"behind\", \"clear\", \"tail\", \"produce\", \"fact\", \n                  \"street\", \"inch\", \"lot\", \"nothing\", \"course\", \"stay\", \"wheel\", \"full\", \n                  \"force\", \"blue\", \"object\", \"decide\", \"surface\", \"deep\", \"moon\", \"island\", \n                  \"foot\", \"yet\", \"busy\", \"test\", \"record\", \"boat\", \"common\", \"gold\", \n                  \"possible\", \"plane\", \"age\", \"dry\", \"wonder\", \"laugh\", \"thousand\", \"ago\", \n                  \"ran\", \"check\", \"game\", \"shape\", \"yes\", \"hot\", \"miss\", \"brought\", \"heat\", \n                  \"snow\", \"bed\", \"bring\", \"sit\", \"perhaps\", \"fill\", \"east\", \"weight\", \n                  \"language\", \"among\")\n\n\n```\n\nNow let's create a new variable in the three children's data sets, that will count the number of less common valid words.\n\n```{r}\n# Create the 'special_word' variable\nspecial_tokens <- tokens |> \n  mutate(is_valid = gloss %in% GradyAugmented,\n         is_common = gloss %in% common_words,\n         is_special = is_valid & !is_common)\n```\n\nNow let's count the number of special words per month for each child. We should also normalize this against all words said that month to have a resulting percentage.\n\n```{r}\nspecial_tokens <- tokens |> \n  mutate(is_valid = gloss %in% GradyAugmented,\n         is_common = gloss %in% common_words,\n         is_special = is_valid & !is_common,\n         age = floor(target_child_age)) |> \n  group_by(target_child_name, age) |> \n  summarize(special_share = mean(is_special))\n\n# Create the plot\nspecial_tokens |> \n  ggplot(aes(x = age, y = special_share, color = target_child_name)) +\n  geom_line() +\n  labs(title = \"Share of Special Words per Age Bracket\",\n       x = \"Age (months)\",\n       y = \"Share of Special Words\") +\n  theme_minimal()\n```\n\nAs an alternative measure, we can consider the share of unique special words out of the total number of unique words. So e.g. if a child says 1000 unique tokens in a month, out of which 200 are valid but not common words, the share of unique special words would be 200/1000=20%. We can calculate this alternative metric by adding one line of code that keeps only unique combinations of child name, age, token, and special word indicator.\n\n```{r}\nunique_special_tokens <- tokens |> \n  mutate(is_valid = gloss %in% GradyAugmented,\n         is_common = gloss %in% common_words,\n         is_special = is_valid & !is_common,\n         age = floor(target_child_age)) |> \n  distinct(target_child_name, age, gloss, is_special) |> \n  group_by(target_child_name, age) |> \n  summarize(special_share = mean(is_special))\n\n# Create the plot\nunique_special_tokens |> \n  ggplot(aes(x = age, y = special_share, color = target_child_name)) +\n  geom_line() +\n  labs(title = \"Share of Special Words per Age Bracket\",\n       x = \"Age (months)\",\n       y = \"Share of Special Words\") +\n  theme_minimal()\n```\n\n```{r, echo=FALSE,eval=FALSE}\n#Adam \n\n# Calculate tokens per month\ntokens_per_month_adam <- adam_tokens %>%\n  group_by(age_bracket) %>%\n  summarise(tokens_per_month = n())\n\n# Calculate special word counts per month\nspecial_word_counts_per_age_bracket_adam <- adam_tokens %>%\n  filter(special_word == 1) %>%\n  group_by(age_bracket, gloss) %>%\n  summarise(count_per_word = n_distinct(gloss), .groups = 'drop') %>%\n  group_by(age_bracket) %>%\n  summarise(count_special_words = n(), .groups = 'drop')\n\n# Add tokens_per_month to special_word_counts_per_age_bracket_adam\nspecial_word_counts_per_age_bracket_adam <- special_word_counts_per_age_bracket_adam %>%\n  left_join(tokens_per_month_adam, by = \"age_bracket\") %>%\n  mutate(normalized_special_word_count = count_special_words / tokens_per_month)\n\n# Create the plot\nggplot(special_word_counts_per_age_bracket_adam, aes(x = age_bracket, y = normalized_special_word_count)) +\n  geom_line(color = \"blue\") +                # Line graph\n  labs(title = \"Count of Special Words per Age Bracket for Adam \",\n       x = \"Age Bracket (Months)\",\n       y = \"Count of Special Words\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.title.x = element_text(margin = margin(t = 10)),\n    axis.title.y = element_text(margin = margin(r = 10))\n  )\n```\n\nAs we can see in the graph, there are too many special words, so we will enlarge the common_words data set to around 1500, which is 1000 words less than the typical vocabulary size of a 5 year old (according to Linguisystems Milestones Guide it should be between 2200-2500). Since these children are looked at from early age until approximately 5 years, I have decided to compare their vocabulary to the one of a 5 year old child. However, as each child is raised in a different environment, there are different words that are considered common around the house, which definitely has the impact on the uniqness of words.\n\nThis data set includes common_words, as well as an additional circa 1000 common words. Since I could not find the data that would include he most commonly spoken 2500 words (average 5 year old vocabulary size), I had to make smaller sized data set. Sometimes the investigation of a research question can be halted because of such reasons, so it's very important to keep in mind the resources and data sets available on the internet. In this case, I included a data set made from two different websites, so there might be an overlap of words, which is why I had to remove duplicates. Furthermore, I put it in alphabetical order for my own convenience.\n\n```{r, echo=FALSE,eval=FALSE}\ncommon_words_2000 <- c(\"the\", \"of\", \"to\", \"and\", \"a\", \"in\", \"is\", \"it\", \"you\", \"that\", \"he\", \"was\", \"for\", \n           \"on\", \"are\", \"with\", \"as\", \"I\", \"his\", \"they\", \"be\", \"at\", \"one\", \"have\", \"this\", \n           \"from\", \"or\", \"had\", \"by\", \"hot\", \"but\", \"some\", \"what\", \"there\", \"we\", \"can\", \"out\", \n           \"other\", \"were\", \"all\", \"your\", \"when\", \"up\", \"use\", \"word\", \"how\", \"said\", \"an\", \n           \"each\", \"she\", \"which\", \"do\", \"their\", \"time\", \"if\", \"will\", \"way\", \"about\", \"many\", \n           \"then\", \"them\", \"would\", \"write\", \"like\", \"so\", \"these\", \"her\", \"long\", \"make\", \n           \"thing\", \"see\", \"him\", \"two\", \"has\", \"look\", \"more\", \"day\", \"could\", \"go\", \"come\", \n           \"did\", \"my\", \"sound\", \"no\", \"most\", \"number\", \"who\", \"over\", \"know\", \"water\", \"than\", \n           \"call\", \"first\", \"people\", \"may\", \"down\", \"side\", \"been\", \"now\", \"find\", \"any\", \"new\", \n           \"work\", \"part\", \"take\", \"get\", \"place\", \"made\", \"live\", \"where\", \"after\", \"back\", \n           \"little\", \"only\", \"round\", \"man\", \"year\", \"came\", \"show\", \"every\", \"good\", \"me\", \n           \"give\", \"our\", \"under\", \"name\", \"very\", \"through\", \"just\", \"form\", \"much\", \"great\", \n           \"think\", \"say\", \"help\", \"low\", \"line\", \"before\", \"turn\", \"cause\", \"same\", \"mean\", \n           \"differ\", \"move\", \"right\", \"boy\", \"old\", \"too\", \"does\", \"tell\", \"sentence\", \"set\", \n           \"three\", \"want\", \"air\", \"well\", \"also\", \"play\", \"small\", \"end\", \"put\", \"home\", \"read\", \n           \"hand\", \"port\", \"large\", \"spell\", \"add\", \"even\", \"land\", \"here\", \"must\", \"big\", \"high\", \n           \"such\", \"follow\", \"act\", \"why\", \"ask\", \"men\", \"change\", \"went\", \"light\", \"kind\", \"off\", \n           \"need\", \"house\", \"picture\", \"try\", \"us\", \"again\", \"animal\", \"point\", \"mother\", \"world\", \n           \"near\", \"build\", \"self\", \"earth\", \"father\", \"head\", \"stand\", \"own\", \"page\", \"should\", \n           \"country\", \"found\", \"answer\", \"school\", \"grow\", \"study\", \"still\", \"learn\", \"plant\", \n           \"cover\", \"food\", \"sun\", \"four\", \"thought\", \"let\", \"keep\", \"eye\", \"never\", \"last\", \n           \"door\", \"between\", \"city\", \"tree\", \"cross\", \"since\", \"hard\", \"start\", \"might\", \"story\", \n           \"saw\", \"far\", \"sea\", \"draw\", \"left\", \"late\", \"run\", \"don't\", \"while\", \"press\", \"close\", \n           \"night\", \"real\", \"life\", \"few\", \"stop\", \"open\", \"seem\", \"together\", \"next\", \"white\", \n           \"children\", \"begin\", \"got\", \"walk\", \"example\", \"ease\", \"paper\", \"often\", \"always\", \n           \"music\", \"those\", \"both\", \"mark\", \"book\", \"letter\", \"until\", \"mile\", \"river\", \"car\", \n           \"feet\", \"care\", \"second\", \"group\", \"carry\", \"took\", \"rain\", \"eat\", \"room\", \"friend\", \n           \"began\", \"idea\", \"fish\", \"mountain\", \"north\", \"once\", \"base\", \"hear\", \"horse\", \"cut\", \n           \"sure\", \"watch\", \"color\", \"face\", \"wood\", \"main\", \"enough\", \"plain\", \"girl\", \"usual\", \n           \"young\", \"ready\", \"above\", \"ever\", \"red\", \"list\", \"though\", \"feel\", \"talk\", \"bird\", \n           \"soon\", \"body\", \"dog\", \"family\", \"direct\", \"pose\", \"leave\", \"song\", \"measure\", \"state\", \n           \"product\", \"black\", \"short\", \"numeral\", \"class\", \"wind\", \"question\", \"happen\", \n           \"complete\", \"ship\", \"area\", \"half\", \"rock\", \"order\", \"fire\", \"south\", \"problem\", \n           \"piece\", \"told\", \"knew\", \"pass\", \"farm\", \"top\", \"whole\", \"king\", \"size\", \"heard\", \n           \"best\", \"hour\", \"better\", \"true\", \"during\", \"hundred\", \"am\", \"remember\", \"step\", \n           \"early\", \"hold\", \"west\", \"ground\", \"interest\", \"reach\", \"fast\", \"five\", \"sing\", \n           \"listen\", \"six\", \"table\", \"travel\", \"less\", \"morning\", \"ten\", \"simple\", \"several\", \n           \"vowel\", \"toward\", \"war\", \"lay\", \"against\", \"pattern\", \"slow\", \"center\", \"love\", \n           \"person\", \"money\", \"serve\", \"appear\", \"road\", \"map\", \"science\", \"rule\", \"govern\", \n           \"pull\", \"cold\", \"notice\", \"voice\", \"fall\", \"power\", \"town\", \"fine\", \"certain\", \n           \"fly\", \"unit\", \"lead\", \"cry\", \"dark\", \"machine\", \"note\", \"wait\", \"plan\", \"figure\", \n           \"star\", \"box\", \"noun\", \"field\", \"rest\", \"correct\", \"able\", \"pound\", \"done\", \n           \"beauty\", \"drive\", \"stood\", \"contain\", \"front\", \"teach\", \"week\", \"final\", \"gave\", \n           \"green\", \"oh\", \"quick\", \"develop\", \"sleep\", \"warm\", \"free\", \"minute\", \"strong\", \n           \"special\", \"mind\", \"behind\", \"clear\", \"tail\", \"produce\", \"fact\", \"street\", \"inch\", \n           \"lot\", \"nothing\", \"course\", \"stay\", \"wheel\", \"full\", \"force\", \"blue\", \"object\", \n           \"decide\", \"surface\", \"deep\", \"moon\", \"island\", \"foot\", \"yet\", \"busy\", \"test\", \n           \"record\", \"boat\", \"common\", \"gold\", \"possible\", \"plane\", \"age\", \"dry\", \"wonder\", \n           \"laugh\", \"thousand\", \"ago\", \"ran\", \"check\", \"game\", \"shape\", \"yes\", \"hot\", \n           \"miss\", \"brought\", \"heat\", \"snow\", \"bed\", \"bring\", \"sit\", \"perhaps\", \"fill\", \n           \"east\", \"weight\", \"language\", \"among\", \"a\", \"abide\", \"ability\", \"able\", \"about\", \"above\",\n    \"abroad\", \"access\", \"accommodation\",\n    \"accomplish\", \"account\", \"accuracy\", \"accurate\", \"achieve\", \"achievement\", \"acknowledge\",\n  \"acquaintance\", \"acquire\", \"across\", \"act\", \"actual\", \"actually\", \"add\", \"additional\",\n  \"address\", \"advance\", \"advantage\", \"advertise\", \"advertisement\", \"advice\", \"advise\",\n  \"advocate\", \"affair\", \"affect\", \"afford\", \"affordable\", \"afraid\", \"after\", \"afternoon\",\n  \"afterwards\", \"again\", \"against\", \"age\", \"ago\", \"agree\", \"agreement\", \"ahead\", \"aid\",\n  \"aim\", \"albeit\", \"alike\", \"alive\", \"all\", \"allow\", \"allowance\", \"almost\", \"alone\",\n  \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"amazing\", \"amend\", \"among\",\n  \"amount\", \"and\", \"anger\", \"angry\", \"ankle\", \"annoy\", \"annoyed\", \"annoying\", \"another\",\n  \"answer\", \"anxious\", \"any\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"apart\", \"apologize\",\n  \"appeal\", \"appear\", \"appearance\", \"apple\", \"application\", \"apply\", \"appointment\", \"appraisal\",\n  \"appreciate\", \"approach\", \"appropriate\", \"are\", \"area\", \"argue\", \"argument\", \"arise\",\n  \"arm\", \"around\", \"arrange\", \"arrangement\", \"array\", \"arrive\", \"art\", \"as\", \"ashamed\",\n  \"aside\", \"ask\", \"asleep\", \"assert\", \"assertive\", \"assess\", \"assessment\", \"asset\",\n  \"assignment\", \"assume\", \"assumption\", \"at\", \"Ate\", \"attach\", \"attached\", \"attempt\",\n  \"attend\", \"attention\", \"attitude\", \"audience\", \"aunt\", \"available\", \"average\", \"avoid\",\n  \"awake\", \"award\", \"aware\", \"awareness\", \"away\", \"awe\", \"awesome\", \"awful\", \"awkward\",\n  \"bachelor\", \"back\", \"background\", \"bad\", \"bag\", \"bake\", \"balance\", \"bald\", \"ball\", \"ban\",\n  \"band\", \"bank\", \"bar\", \"bare\", \"barely\", \"bargain\", \"bark\", \"base\", \"bass\", \"bat\",\n  \"batch\", \"be\", \"beach\", \"beam\", \"bear\", \"beard\", \"bearing\", \"beat\", \"beautiful\", \"because\",\n  \"become\", \"bed\", \"beef\", \"before\", \"beg\", \"begin\", \"beginning\", \"behave\", \"behavior\",\n  \"behind\", \"being\", \"belief\", \"believe\", \"belong\", \"below\", \"belt\", \"bench\", \"bend\",\n  \"beneath\", \"benefit\", \"beside\", \"besides\", \"best\", \"bet\", \"better\", \"between\", \"beyond\",\n  \"bias\", \"biased\", \"bid\", \"big\", \"bill\", \"bind\", \"binding\", \"bird\", \"birthday\", \"bit\",\n  \"bitch\", \"bite\", \"bitter\", \"blame\", \"blanket\", \"blast\", \"blend\", \"blind\", \"block\",\n  \"blood\", \"blow\", \"blue\", \"blunt\", \"board\", \"boast\", \"boat\", \"body\", \"bold\", \"bolt\",\n  \"bond\", \"book\", \"boost\", \"boot\", \"border\", \"bore\", \"bored\", \"boring\", \"born\", \"borrow\",\n  \"bossy\", \"both\", \"bother\", \"bottom\", \"bounce\", \"bound\", \"boundary\", \"bow\", \"bowl\",\n  \"box\", \"branch\", \"brand\", \"brave\", \"breach\", \"bread\", \"break\", \"breakdown\", \"breakfast\",\n  \"breakthrough\", \"breath\", \"breathe\", \"breed\", \"bridge\", \"brief\", \"bright\", \"bring\",\n  \"broad\", \"broadcast\", \"broke\", \"brother\", \"brown\", \"brush\", \"bucket\", \"budget\", \"bug\",\n  \"build\", \"building\", \"bulk\", \"bully\", \"bump\", \"bunch\", \"bundle\", \"burden\", \"burn\",\n  \"burst\", \"bush\", \"business\", \"bust\", \"busy\", \"but\", \"butt\", \"buy\", \"buzz\", \"by\",\n  \"cabbage\", \"cake\", \"calf\", \"call\", \"called\", \"calm\", \"can\", \"cap\", \"car\", \"care\",\n  \"career\", \"careful\", \"carefully\", \"caring\", \"carry\", \"case\", \"cast\", \"cat\", \"catch\",\n  \"cattle\", \"caught\", \"cause\", \"ceiling\", \"certain\", \"certainly\", \"chain\", \"chair\",\n  \"challenge\", \"challenging\", \"chance\", \"chandelier\", \"change\", \"character\", \"charge\",\n  \"charity\", \"charming\", \"chart\", \"chase\", \"cheap\", \"cheat\", \"check\", \"cheek\", \"cheeky\",\n  \"cheer\", \"cheerful\", \"chest\", \"chicken\", \"chief\", \"child\", \"childhood\", \"chill\",\n  \"chin\", \"choice\", \"choose\", \"chop\", \"church\", \"city\", \"claim\", \"class\", \"clay\",\n  \"clean\", \"clear\", \"clerk\", \"clever\", \"cliff\", \"climb\", \"close\", \"clue\", \"clumsy\",\n  \"cluster\", \"coach\", \"coal\", \"coat\", \"cold\", \"colleague\", \"collect\", \"college\", \"come\",\n  \"comfortable\", \"commit\", \"commitment\", \"committed\", \"common\", \"commute\", \"company\",\n  \"compelling\", \"complain\", \"complaint\", \"complete\", \"compliance\", \"comply\", \"compound\",\n  \"comprehensive\", \"compulsory\", \"computer\", \"concern\", \"concerned\", \"conduct\",\n  \"confidence\", \"confident\", \"consider\", \"consist\", \"consistent\", \"constraint\", \"contact\",\n  \"contain\", \"content\", \"control\", \"convenient\", \"convey\", \"cook\", \"cool\", \"cope\", \"core\",\n  \"correct\", \"cost\", \"costume\", \"couch\", \"cough\", \"could\", \"count\", \"counter\", \"country\",\n  \"couple\", \"course\", \"court\", \"cousin\", \"cover\", \"crack\", \"craft\", \"crap\", \"crash\",\n  \"crawl\", \"crazy\", \"create\", \"creep\", \"creepy\", \"crew\", \"crop\", \"cross\", \"crowd\",\n  \"crowded\", \"crush\", \"cry\", \"cuddle\", \"cue\", \"culture\", \"cup\", \"cupboard\", \"curb\",\n  \"currency\", \"current\", \"currently\", \"curse\", \"custom\", \"customer\", \"cut\", \"cute\",\n  \"daily\", \"damage\", \"damn\", \"damp\", \"dance\", \"dangerous\", \"dare\", \"dark\", \"dash\",\n  \"data\", \"date\", \"daughter\", \"dawn\", \"day\", \"dead\", \"deadline\", \"deal\", \"dear\", \"death\",\n  \"deceive\", \"decide\", \"decision\", \"deck\", \"decline\", \"decrease\", \"deed\", \"deem\",\n  \"deep\", \"deer\", \"default\", \"defeat\", \"definitely\", \"degree\", \"delay\", \"delight\",\n  \"delighted\", \"deliver\", \"delivery\", \"demand\", \"demanding\", \"deny\", \"depict\", \"deploy\",\n  \"depth\", \"deserve\", \"design\", \"desire\", \"desk\", \"despite\", \"dessert\", \"determined\",\n  \"develop\", \"development\", \"device\", \"dictionary\", \"die\", \"different\", \"difficult\",\n  \"dig\", \"dim\", \"dinner\", \"dip\", \"dire\", \"dirty\", \"disappointed\", \"disclosure\",\n  \"discover\", \"discuss\", \"disease\", \"disguise\", \"dish\", \"dismiss\", \"display\", \"distress\",\n  \"ditch\", \"dive\", \"dizzy\", \"do\", \"doctor\", \"does\", \"dog\", \"done\", \"door\", \"doubt\",\n  \"down\", \"draft\", \"drag\", \"drain\", \"draw\", \"drawback\", \"drawer\", \"drawing\", \"drawn\",\n  \"dread\", \"dreadful\", \"dream\", \"dress\", \"drift\", \"drill\", \"drink\", \"drive\", \"drop\",\n  \"drought\", \"drown\", \"drug\", \"drunk\", \"dry\", \"duck\", \"dust\", \"duty\", \"each\", \"ear\",\n  \"early\", \"earn\", \"earnings\", \"earth\", \"ease\", \"easier\", \"easily\", \"east\", \"easy\",\n  \"eat\", \"eclectic\", \"edge\", \"educate\", \"education\", \"effect\", \"effective\", \"effort\",\n  \"either\", \"element\", \"elevate\", \"elite\", \"else\", \"embarrassed\", \"embrace\", \"emotion\",\n  \"emotional\", \"employ\", \"employee\", \"employer\", \"enact\", \"end\", \"enemy\", \"energy\",\n  \"engage\", \"engagement\", \"engine\", \"enough\", \"enrich\", \"enroll\", \"ensure\", \"enter\",\n  \"entire\", \"environment\", \"equal\", \"equally\", \"error\", \"escape\", \"especially\", \"essence\",\n  \"establish\", \"estimate\", \"even\", \"event\", \"eventually\", \"ever\", \"every\", \"everybody\",\n  \"everyone\", \"everything\", \"evidence\", \"evolve\", \"exact\", \"exactly\", \"example\", \"excellent\",\n  \"excited\", \"exciting\", \"exercise\", \"exhibit\", \"exist\", \"expect\", \"expensive\", \"experience\",\n  \"expert\", \"explain\", \"expose\", \"extend\", \"extra\", \"eye\", \"fable\", \"face\", \"fact\",\n  \"factory\", \"fail\", \"fair\", \"fall\", \"fame\", \"family\", \"famous\", \"fan\", \"far\", \"fare\",\n  \"fast\", \"fate\", \"fault\", \"fear\", \"feature\", \"feel\", \"feeling\", \"few\", \"fight\", \"file\",\n  \"fill\", \"final\", \"find\", \"fine\", \"finish\", \"fire\", \"firm\", \"first\", \"fish\", \"fit\",\n  \"fix\", \"flame\", \"flash\", \"flat\", \"flee\", \"flight\", \"flip\", \"floor\", \"flow\", \"flower\",\n  \"fly\", \"focus\", \"fold\", \"follow\", \"food\", \"force\", \"forget\", \"form\", \"formal\", \"format\",\n  \"fort\", \"fortune\", \"found\", \"four\", \"fragile\", \"frame\", \"free\", \"freedom\", \"fresh\",\n  \"friend\", \"friendly\", \"frighten\", \"from\", \"front\", \"fruit\", \"full\", \"function\", \"fund\",\n  \"future\", \"gain\", \"game\", \"gap\", \"garbage\", \"garden\", \"gas\", \"gather\", \"gave\", \"general\",\n  \"generally\", \"generate\", \"gentle\", \"get\", \"gift\", \"give\", \"glass\", \"goal\", \"god\", \"gold\",\n  \"good\", \"government\", \"grand\", \"grant\", \"grass\", \"great\", \"green\", \"greet\", \"ground\",\n  \"group\", \"grow\", \"growth\", \"guarantee\", \"guard\", \"guess\", \"guest\", \"guide\", \"guilt\",\n  \"habit\", \"happy\", \"hard\", \"hardly\", \"harmony\", \"harsh\", \"hate\", \"have\", \"he\", \"head\",\n  \"health\", \"heart\", \"heat\", \"help\", \"her\", \"here\", \"hesitate\", \"hi\", \"high\", \"hill\",\n  \"his\", \"history\", \"hit\", \"hold\", \"hole\", \"home\", \"honest\", \"honey\", \"hope\", \"horrible\",\n  \"host\", \"hot\", \"house\", \"how\", \"huge\", \"human\", \"humor\", \"hungry\", \"hurry\", \"hurt\",\n  \"ice\", \"idea\", \"identify\", \"if\", \"ignore\", \"ill\", \"image\", \"imagine\", \"impact\", \"implement\",\n  \"important\", \"improve\", \"in\", \"include\", \"income\", \"increase\", \"influence\", \"inform\",\n  \"information\", \"inspire\", \"instead\", \"interest\", \"into\", \"invest\", \"involve\", \"is\", \"issue\",\n  \"it\", \"item\", \"jack\", \"job\", \"join\", \"joke\", \"judge\", \"juice\", \"just\", \"keep\", \"kind\",\n  \"king\", \"kiss\", \"knee\", \"know\", \"knowledge\", \"lack\", \"lady\", \"land\", \"language\", \"large\",\n  \"last\", \"late\", \"lately\", \"laugh\", \"lawn\", \"lead\", \"learn\", \"lesson\", \"let\", \"letter\",\n  \"lie\", \"life\", \"likely\", \"limit\", \"line\", \"list\", \"listen\", \"live\", \"load\", \"local\",\n  \"lock\", \"long\", \"look\", \"loose\", \"loud\", \"love\", \"luck\", \"lucky\", \"mad\", \"mail\",\n  \"main\", \"make\", \"man\", \"many\", \"market\", \"married\", \"mass\", \"master\", \"match\", \"matter\",\n  \"mean\", \"meaning\", \"measure\", \"medicine\", \"meet\", \"member\", \"memory\", \"mention\",\n  \"merry\", \"message\", \"middle\", \"might\", \"mind\", \"mine\", \"minute\", \"miss\", \"mistake\",\n  \"mix\", \"model\", \"money\", \"more\", \"most\", \"mother\", \"move\", \"much\", \"must\", \"myself\",\n  \"name\", \"nature\", \"near\", \"need\", \"negative\", \"network\", \"never\", \"new\", \"news\", \"nice\",\n  \"night\", \"none\", \"normal\", \"not\", \"note\", \"nothing\", \"now\", \"number\", \"object\",\n  \"obtain\", \"obvious\", \"occasion\", \"off\", \"offer\", \"office\", \"often\", \"on\", \"one\",\n  \"only\", \"open\", \"opportunity\", \"order\", \"organization\", \"others\", \"out\", \"over\", \"own\",\n  \"pack\", \"pain\", \"part\", \"party\", \"pass\", \"path\", \"patient\", \"pay\", \"peace\", \"pen\",\n  \"people\", \"perfect\", \"perhaps\", \"period\", \"place\", \"plan\", \"play\", \"point\", \"policy\",\n  \"poor\", \"position\", \"possible\", \"potential\", \"power\", \"prepare\", \"present\", \"press\",\n  \"price\", \"private\", \"problem\", \"process\", \"produce\", \"product\", \"professional\", \"profit\",\n  \"progress\", \"project\", \"promise\", \"proper\", \"protect\", \"public\", \"purpose\", \"quality\",\n  \"question\", \"quick\", \"quiet\", \"raise\", \"rate\", \"read\", \"ready\", \"real\", \"reality\",\n  \"reason\", \"receive\", \"recent\", \"record\", \"reduce\", \"reflect\", \"refuse\", \"regard\",\n  \"region\", \"regret\", \"relate\", \"relationship\", \"release\", \"rely\", \"remain\", \"remember\",\n  \"remove\", \"rent\", \"reply\", \"report\", \"represent\", \"request\", \"require\", \"research\",\n  \"resource\", \"respond\", \"result\", \"return\", \"reveal\", \"rich\", \"right\", \"risk\", \"road\",\n  \"role\", \"room\", \"rule\", \"run\", \"safe\", \"same\", \"save\", \"school\", \"score\", \"search\",\n  \"season\", \"see\", \"self\", \"sell\", \"sense\", \"serve\", \"service\", \"set\", \"share\", \"short\",\n  \"show\", \"side\", \"sign\", \"simple\", \"since\", \"site\", \"size\", \"social\", \"some\", \"soon\",\n  \"sort\", \"space\", \"special\", \"spend\", \"stand\", \"start\", \"state\", \"status\", \"stay\",\n  \"step\", \"still\", \"store\", \"strategy\", \"study\", \"stuff\", \"success\", \"such\", \"suggest\",\n  \"support\", \"sure\", \"table\", \"take\", \"task\", \"team\", \"tell\", \"term\", \"test\", \"than\",\n  \"that\", \"their\", \"them\", \"then\", \"there\", \"these\", \"they\", \"thing\", \"this\", \"time\",\n  \"to\", \"today\", \"together\", \"tomorrow\", \"too\", \"total\", \"touch\", \"toward\", \"trade\",\n  \"train\", \"transport\", \"travel\", \"try\", \"turn\", \"type\", \"understand\", \"union\", \"unit\",\n  \"until\", \"up\", \"use\", \"value\", \"very\", \"view\", \"visit\", \"wait\", \"walk\", \"want\",\n  \"warm\", \"way\", \"we\", \"well\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\",\n  \"why\", \"wide\", \"will\", \"win\", \"with\", \"work\", \"world\", \"worry\", \"write\", \"year\",\n  \"young\", \"your\", \"yourself\")\n\n# Remove duplicates\ncommon_words_unique <- unique(common_words_2000)\n\n# Sort alphabetically\ncommon_words_sorted <- sort(common_words_unique)\n\n```\n\nAfter looking at the graphs, we can see that all three children use language that is developed and also contains more unique words. However, that does not tell us directly whether their other language skills such as syntax are also on par with where they should be with their language development.\n\nWhen it comes to their vocabulary specifically, we can see the use of a more developed language, compared to an average 5 year old, or even a little more advanced than that. That is true for all of them even from a younger age, where they use less common words. This may be explainable by their environment - if the child's father is a doctor, they may come in contact with more advanced words from the medical field, simply due to existing in the same household. What can also be the case is that they do know some more advanced words, but lack in basics.\n\nOnce again, we can't really fully compare Eve with Adam and Sarah, as she does not have data coverage over extended periods of time. However, when we compare the special words Adam has, we can notice that Eve has the higher unique words usage, which suggests her vocabulary is more advanced. This finding is interesting, as Adam has a higher average of valid words used over time compared with Eve. Sarah's data are rather inconclusive, as she shows very high usage of unique words at first, and then it drops below 60%. Overall, these data don't show as much of a trend as we'd hope for an investigation. It's key to play around with data and find good tools to reach more definite conclusion.\n\nTo conclude, we looked into three children's tokens, and did two analyses - one that looked into their average usage of valid words over time using an English vocabulary, and a second one that looked into special words usage per month. We can conclude that Eve does not have enough data to be compared with Sarah and Adam, but all of them have enough data to be analysed independently. Furthermore, Adam shows more advancements compared to Sarah when it comes to average valid word usage, but Eve has higher unique word usage.\n\nList of sources: Hsu, M.-L. (2013). Language play: The development of linguistic consciousness and creative speech in early childhood education. In Advances in early education and day care (Vol. 17, pp. 127–139). Emerald Group Publishing Limited. \\[<https://doi.org/10.1108/S0270-4021(2013)0000012007>\\]\\[<https://doi.org/10.1108/S0270-4021(2013)0000012007>\\]\n\nThe Education Hub. (n.d.). Effective vocabulary instruction. The Education Hub. <https://theeducationhub.org.nz/effective-vocabulary-instruction/>\n\nChildes-db. (2019). Childes-db: A flexible and reproducible interface to the child language data exchange system. Journal of Child Language, 51, 1928–1941. <https://doi.org/10.1017/s0305000900013866>\n\nWordReference. (n.d.). Top 2000 English words. WordReference. <https://lists.wordreference.com/show/Top-2000-English-words.1/>\n\nVocabularyFirst. (2019). How many words do I need to speak English language? VocabularyFirst. <https://www.vocabularyfirst.com/how-many-words-do-i-need-to-know/>\n\nYang, D. (2016). How many words do you need to know to be fluent in English? Day Translations. <https://www.daytranslations.com/blog/how-many-words-to-be-fluent-in-english/>\n\nSocial Sci LibreTexts. (2023). Language Development in Early Childhood.\n\nMichigan State University Extension. (2023). Language development – Part 2: Principles that are the stem and branch of speech.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"output-file":"workshop1_old.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.555","editor":"visual","theme":"simplex","mainfont":"Cormorant SC","fontsize":"20px","layout":"page","title":"SCICOGN302:<br> Exploring the CHILDES dataset (draft)","subtitle":"Fall 2024","date":"Last updated: `r Sys.Date()`"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}