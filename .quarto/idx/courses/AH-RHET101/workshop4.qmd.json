{"title":"AH-RHET101 Workshop 4: Sentiment analysis in R","markdown":{"yaml":{"layout":"page","title":"AH-RHET101 Workshop 4: Sentiment analysis in R","subtitle":"Spring 2025","date":"Last updated: `r Sys.Date()`"},"headingText":"Sentiment Analysis of a TED talk","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, error=FALSE)\n```\n\n\n**Objective:** In this workshop, you will learn how to perform a sentiment analysis on a TED Talk using R and the bing sentiment lexicon.\n\n**Sentiment analysis** is a method used to determine the emotional tone behind a body of text using Natural Language Processing (NLP). It involves analysing the words in a text to classify whether the sentiment expressed is positive, negative, neutral or a specific emotion.\n\nIn this workshop, you will be analysing a TED Talk by Ketakandriana Rafitoson on democracy to identify the emotional tones present in the speaker's words and see how these emotional expressions can be interpreted to gain a deeper understanding of the speakerâ€™s message and intent.\n\nBy the end of this session, you will have gained experience with tokenising text, applying sentiment lexicons, and visualising the sentiment results. In the homework assignment, you will apply this knowledge to conduct a more complex sentiment analysis using the nrc sentiment lexicon and compare the result across different TED talks on democracy.\n\n## Step 1: Starting up the R Script and R project\n\nOpen your AH-RHET101 R project from the previous session and start a new R script for the sentiment analysis case study.\n\nLoad, and if needed install, the following packages by placing and running the following code in your R script:\n\n```{r}\n# install.packages(\"tidyverse\")\n# install.packages(\"tidytext\")\n\nlibrary(tidyverse)\nlibrary(tidytext)\n```\n\n## Step 2: Tokenising the text into sentences\n\nBefore you can do any analysis, RStudio needs to know the text file you want to work. For this you need to import the text into the environment. To do this, you have to create a new object (*ted_lines*). This will store the text that is located in the .txt file by using the function **readLines()***.*This is a function that reads the text file line by line and stores it in a vector.\n\nTo use **readLines()** you have to specify the path of the file. In this case, the file is located on [GitHub](https://github.com/ucrdatacenter/projects/blob/main/AH-RHET101/2025h1/Texts/Ted_Talk_Ketakandriana_Rafitoson.txt), and you can import it directly without having to download it using the following import code.\n\nYou can use **view()** to open the object to understand what has happened after running the code.\n\n```{r}\nted_lines <- readLines(\"https://github.com/ucrdatacenter/projects/raw/refs/heads/main/AH-RHET101/2025h1/Texts/Ted_Talk_Ketakandriana_Rafitoson.txt\")\nview(ted_lines)\n```\n\nHowever, when you view the object *ted_lines*, you will see that the text is not easily readable in its current format. This is because the text is stored in a single vector. It would be easier if the text was stored in a table format, where each sentence is stored in a separate row.\n\nTo convert a vector into a tibble in r, you can use the function **as_tibble()**. However, this will store all sentences into a single row. To split the text into sentences and have each sentence stored in a new row you also have to use the function **unnest_tokens()** (You can learn more about the function by opening the helpfile by typing *?unnest_tokens* in your console.)\n\n**unnest_tokens()** can tokenise, which means break up texts into, words, characters, ngrams, sentences, lines, paragraphs and more. As you are interested in sentences, the **output** and **token** should be set to \"sentences\" within the function. Additionally, it is useful to keep capitalisation, which can be done by setting the argument **to_lower** to equal FALSE. The result should be stored in a new object, which you can title *sentences_ted.* **View()** the result to see what has happend.\n\n```{r}\nsentences_ted <- ted_lines |>\n  as_tibble() |> \n  unnest_tokens(output = sentences, input = value, token = \"sentences\", to_lower = FALSE)\n\nview(sentences_ted)\n```\n\n## Step 3: Tokenising the text into words\n\nNow that the text has been tokenised into sentences, you and other people can process the text more easily, which is useful when you want to extract sentences that contain specific words from the text as you will be doing later.\n\nOne way to know which specific words to extract, is by conducting a sentiment analysis. The goal of this is to get a general overview of the emotional loadings or sentiment that a text conveys. This is typically done by looking at the words in a text and see whether they have a positive, negative, neutral, or other emotional connotation.\n\nTo do this, you should tokenise the text again but this time into words. You can reuse the code used above but change \"*sentences\"* into \"*words*\" as that is the new token you want.\n\n```{r}\nwords_ted <- ted_lines |>\n  as_tibble() |> \n  unnest_tokens(output = word, input = value, token = \"words\")\n\nview(words_ted)\n```\n\nWhen you view the list of words, you will notice that it contains 1,864 elements. This is because it contains a lot of duplicate words. For some tasks, however, it can be useful to work with word types, and not word instances. Word types are the unique words in a text, and word instances are all words in a text including duplicates. To obtain the word types of a text you can use the function **unique()** which will return the word list but without any duplicates. Indeed, the word list now contains only 658 elements.\n\n```{r}\nwords_ted_unique <- words_ted |> \n  unique()\n\nview(words_ted_unique)\n```\n\n## Step 4: Performing Bing sentiment analysis\n\nNow, that the words and sentences of the ted talk have been properly tokenised, a sentiment analysis can be performed. To do this, you have to import an existing sentiment lexicon, which is a list of words that have been pre-labelled as positive, negative, neutral or a specific emotion.\n\nTo load a sentiment lexicon in your script you can use the **get_sentiment()** function and store it as a new object. R offers four different sentiment lexicons for this function: *afinn, bing, nrc,* and *loughran*.\n\nFor this workshop you will be using the *bing* sentiment lexicon, which contains 6786 English words which have been labelled to be either positive or negative. Note that this list is not all-encompassing.\n\n```{r}\nbing <- get_sentiments(\"bing\")\n\nview(bing)\n```\n\nNow that you have the list of words of the Ted talk, as well as a sentiment lexicon, it is time to see whether any of the words in the talk match with a word that is part of the sentiment lexicon.\n\nTo do this you have to combine the two tibbles, using the function **inner_join()**. More specifically, **inner_join()** will join the word values of the tibbles of *words_ted* and *bing* together, and keep only those that overlap. The result will be stored in a new object, which you can title *words_ted_bing*.\n\nIn addition, it is nice to know how many times a specific word and sentiment occur together, for this you can use the function **count()** You can view the results using the **view()** function.\n\n```{r}\nwords_ted_bing <- words_ted |>\n  inner_join(bing) |>  \n  count(word, sentiment)  \n\nview(words_ted_bing)\n```\n\n## Step 5: Making a visualisation\n\nAlthough, the resulting word list already provides insight into the different words that are included in the talk, it would be nice if you can interpret the result in one glance with the help of a visualisation.\n\nIndeed, a visualisation will make it much clearer whether more positive or negative words have been included, and which words have been used the most.\n\nCurrently, the list of words is ordered alphabetically and contains 88 different words. The visualisation would be more clear if it only includes the top 15 words of each sentiment, and orders them from most used to least used.\n\nYou can do this in r by using the function **slice_max()**. This function will order the words by the number of times they occur and only keep the words with the highest values. To elaborate, **n = 15** specifies that you would like to keep the top 15 words with the highest frequency. The argument **order_by** specifies that the words should be ordered based on their count (*n*) instead of alphabetically. The argument **by** specifies that the words should be grouped based on their sentiment, so you get both the top 15 positive words and the top 15 negative words. Lastly, **with_ties = FALSE** specifies that you want only 15 elements even if there are other elements that have the same amount.\n\n```{r}\nwords_ted_bing_arranged <- words_ted_bing |>\n  slice_max(n = 15, order_by = n, by = sentiment, with_ties = FALSE) \n```\n\nNow that the appropriate words have been selected for the visualisation, you can create it. For this **geom_col()** can be used, which is a type of bar chart. Within **geom_col()** you can specify that you would like the count (*n*) on the x-axis, the word on the y-axis ordered from highest to lowest frequency (*y = reorder(word, n))*, and the sentiment as the fill, so positive and negative words have different colours. The title and labels can be specified using the **labs()** function. NULL means that the y-axis will not be labelled.\n\n```{r}\nwords_ted_bing_arranged |> \n  ggplot() + \n  #Specify the aestethics in the geom_col, reorder() makes sure that the words with the highest frequency are printed at the top.\n  geom_col(aes(x = n, y = reorder(word, n), fill = sentiment)) + \n  #Label the graph properly\n  labs(\n    title = \"Most Common Positive and Negative Words in Ted Talk\",\n    x = \"Frequency\", \n    y = NULL\n  )\n```\n\n## Step 6: (If time): Improve the visualisation using facet_wrap\n\nIf time: Currently, the visualisation can easily be interpreted to see which positive and negative words have been used the most. However, to make this even clearer the negative and positive words can be split from one another. You can do this in r by using the **facet_wrap()** function, which will split the visualisation into two separate graphs, in this case based on their *sentiment*. The argument **scales = \"free\"** specifies that the y-axis should be free, so the words do not overlap with one another.\n\n```{r}\nwords_ted_bing_arranged |> \n  ggplot() + \n  geom_col(aes(x = n, y = reorder(word, n), fill = sentiment)) + \n  labs(\n    title = \"Most common positive and negative words in ted talk\",\n    x = \"Frequency\", \n    y = NULL\n  ) +\n  facet_wrap(~sentiment, scales = \"free\")\n```\n\n## Step 7.1: Interpreting results and investigating usage of like\n\nNow, that we have a clear visualisation, we can see whether we can draw any interesting conclusions from the results.The most common positive word is \"like\",\n\nHowever, in English, the word \"like\" can be used as a noun or verb where it has a positive connotation but it can also be used as a filler or adverb, where it denotes a similarity. As a result, sentences containing *\"like\"* should be investigated.\n\nThis can be done using the **filter()**, **detect_str()** and **pull()** function. For **str_detect()**, you specify the pattern you are looking for in a string of text, using a regular expression in this case \"*like*\". Unfortunately, discussing the exact details of regular expressions falls outside the scope of this workshop.\n\nHowever, it is good to know that it is a way of specifying patterns in text. **str_detect()** also requires an input, which in our case are the words. If a word matches the pattern it returns TRUE otherwise it will return FALSE. Using **filter()** it will only keep sentences that match the mattern.\n\n```{r}\nsentences_ted |> \n  #Only keep sentences that contain the word \"like\"\n  filter(str_detect(sentences, \"like\")) |>\n  pull(sentences)\n```\n\nLook at the sentences, is *\"like\"* being used in a positive manner here or does it take on a different meaning? Indeed, when you look closely you can interpret this from the following sentences:\n\n1.  Here like is negated by the use of \"don't\", this is a downside of the current sentiment analysis as it cannot take this into account.\n2.  Here like is an adverb. 3, 4, 5: Here like is used as a verb which arguable could be positive\n3.  Here like is used as a filler.\n\nThis shows that sentiment analysis are not 100% accurate and are there to give us a general idea of the sentiment of the text. However, human inspection is still needed to interpret and verify the results.\n\n## Step 7.2: Interpreting results and investigating usage of corruption\n\nLuckily, many English words do only have one function, such as the word *\"corruption\"*. It would be interesting to see how the speaker user the word \"corruption\" to appeal for their case and whether any arguments or ethos, pathos, logos can be detected. This again can be done using the functions used above.\n\n```{r}\nsentences_ted |> \n  #Only keep sentences that contain the word \"corruption\"\n  filter(str_detect(sentences, \"corruption\")) |>\n  pull(sentences)\n```\n\nLook at the results, how does the speaker use the words corruption to frame the current political government?\n\nSome of the relevant findings may be: This talk frames the current politicians and political system in a very negative light, which becomes clear from words like *\"corruption\"*, *'dictatorship\"*. Moreover, those who are against corruption seem to then be *\"attacked\"*. The words bring out emotion and there is a great contrast between the the politicians and *\"we\"* and *\"I\"*. Indeed, *\"we are also outspoken\"* or *\"we are also connecting\"* show that the speaker views itself in a more positive light fighting against this corruption.\n\n## Step 7.3: Interpreting results and investigating usage of democracy\n\nLastly, as this talk is about democracy, it can be interesting to see how this talk frames democracy. In our first workshop we discussed a digital humanities paper that investigated how parliamentary debates conceptualise democracry.\n\nThe three most common cases were: 1.) Viewing democracy as a value 2.) Believing democracy is under threat and in need of defence 3.) Using democracy as leverage, trying to make a point persuasive by linking it to democracy.\n\nDoes this talk frame it similarly to one of these points or something else? For this you can reuse the code above but changes *\"corruption\"* into *\"democracy\"*.\n\n```{r}\nsentences_ted |> \n  #Only keep sentences that contain the word \"democracy\"\n  filter(str_detect(sentences, \"democracy\")) |>\n  pull(sentences)\n```\n\nSome of the relevant findings may be: This Ted Talk seems to think that democracy is under threat and needs to be *\"restored\"* or there is a knowledge gap to be *\"filled\"* or it is *\"high time\"* that it is *\"restored back\"*, or there is need for a *\"remedy\"*. The usage of democracy in point 7 could be said to either value democracy, or is used as leverage.\n\n## Step 8: If time: Assignment\n\nChoose another positive and negative word from the list and investigate its usage in the text. What does this word tell you about the speaker's message and intent?\n\n## Step 9. Homework 4\n\nCurrently, the sentiment analysis only uses \"positive\" and \"negative\" words. However, R has another sentiment lexicon titled \"nrc\" which includes 10 different sentiments that are based on emotion. This could perhaps show more specific information. Additionally, you can learn compare results between texts to see whether there is a pattern. This is something you will be exploring in the homework assignment.\n\nYou can find the homework assignment [here](TBA). Download it and save it in your current project folder.\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, error=FALSE)\n```\n\n# Sentiment Analysis of a TED talk\n\n**Objective:** In this workshop, you will learn how to perform a sentiment analysis on a TED Talk using R and the bing sentiment lexicon.\n\n**Sentiment analysis** is a method used to determine the emotional tone behind a body of text using Natural Language Processing (NLP). It involves analysing the words in a text to classify whether the sentiment expressed is positive, negative, neutral or a specific emotion.\n\nIn this workshop, you will be analysing a TED Talk by Ketakandriana Rafitoson on democracy to identify the emotional tones present in the speaker's words and see how these emotional expressions can be interpreted to gain a deeper understanding of the speakerâ€™s message and intent.\n\nBy the end of this session, you will have gained experience with tokenising text, applying sentiment lexicons, and visualising the sentiment results. In the homework assignment, you will apply this knowledge to conduct a more complex sentiment analysis using the nrc sentiment lexicon and compare the result across different TED talks on democracy.\n\n## Step 1: Starting up the R Script and R project\n\nOpen your AH-RHET101 R project from the previous session and start a new R script for the sentiment analysis case study.\n\nLoad, and if needed install, the following packages by placing and running the following code in your R script:\n\n```{r}\n# install.packages(\"tidyverse\")\n# install.packages(\"tidytext\")\n\nlibrary(tidyverse)\nlibrary(tidytext)\n```\n\n## Step 2: Tokenising the text into sentences\n\nBefore you can do any analysis, RStudio needs to know the text file you want to work. For this you need to import the text into the environment. To do this, you have to create a new object (*ted_lines*). This will store the text that is located in the .txt file by using the function **readLines()***.*This is a function that reads the text file line by line and stores it in a vector.\n\nTo use **readLines()** you have to specify the path of the file. In this case, the file is located on [GitHub](https://github.com/ucrdatacenter/projects/blob/main/AH-RHET101/2025h1/Texts/Ted_Talk_Ketakandriana_Rafitoson.txt), and you can import it directly without having to download it using the following import code.\n\nYou can use **view()** to open the object to understand what has happened after running the code.\n\n```{r}\nted_lines <- readLines(\"https://github.com/ucrdatacenter/projects/raw/refs/heads/main/AH-RHET101/2025h1/Texts/Ted_Talk_Ketakandriana_Rafitoson.txt\")\nview(ted_lines)\n```\n\nHowever, when you view the object *ted_lines*, you will see that the text is not easily readable in its current format. This is because the text is stored in a single vector. It would be easier if the text was stored in a table format, where each sentence is stored in a separate row.\n\nTo convert a vector into a tibble in r, you can use the function **as_tibble()**. However, this will store all sentences into a single row. To split the text into sentences and have each sentence stored in a new row you also have to use the function **unnest_tokens()** (You can learn more about the function by opening the helpfile by typing *?unnest_tokens* in your console.)\n\n**unnest_tokens()** can tokenise, which means break up texts into, words, characters, ngrams, sentences, lines, paragraphs and more. As you are interested in sentences, the **output** and **token** should be set to \"sentences\" within the function. Additionally, it is useful to keep capitalisation, which can be done by setting the argument **to_lower** to equal FALSE. The result should be stored in a new object, which you can title *sentences_ted.* **View()** the result to see what has happend.\n\n```{r}\nsentences_ted <- ted_lines |>\n  as_tibble() |> \n  unnest_tokens(output = sentences, input = value, token = \"sentences\", to_lower = FALSE)\n\nview(sentences_ted)\n```\n\n## Step 3: Tokenising the text into words\n\nNow that the text has been tokenised into sentences, you and other people can process the text more easily, which is useful when you want to extract sentences that contain specific words from the text as you will be doing later.\n\nOne way to know which specific words to extract, is by conducting a sentiment analysis. The goal of this is to get a general overview of the emotional loadings or sentiment that a text conveys. This is typically done by looking at the words in a text and see whether they have a positive, negative, neutral, or other emotional connotation.\n\nTo do this, you should tokenise the text again but this time into words. You can reuse the code used above but change \"*sentences\"* into \"*words*\" as that is the new token you want.\n\n```{r}\nwords_ted <- ted_lines |>\n  as_tibble() |> \n  unnest_tokens(output = word, input = value, token = \"words\")\n\nview(words_ted)\n```\n\nWhen you view the list of words, you will notice that it contains 1,864 elements. This is because it contains a lot of duplicate words. For some tasks, however, it can be useful to work with word types, and not word instances. Word types are the unique words in a text, and word instances are all words in a text including duplicates. To obtain the word types of a text you can use the function **unique()** which will return the word list but without any duplicates. Indeed, the word list now contains only 658 elements.\n\n```{r}\nwords_ted_unique <- words_ted |> \n  unique()\n\nview(words_ted_unique)\n```\n\n## Step 4: Performing Bing sentiment analysis\n\nNow, that the words and sentences of the ted talk have been properly tokenised, a sentiment analysis can be performed. To do this, you have to import an existing sentiment lexicon, which is a list of words that have been pre-labelled as positive, negative, neutral or a specific emotion.\n\nTo load a sentiment lexicon in your script you can use the **get_sentiment()** function and store it as a new object. R offers four different sentiment lexicons for this function: *afinn, bing, nrc,* and *loughran*.\n\nFor this workshop you will be using the *bing* sentiment lexicon, which contains 6786 English words which have been labelled to be either positive or negative. Note that this list is not all-encompassing.\n\n```{r}\nbing <- get_sentiments(\"bing\")\n\nview(bing)\n```\n\nNow that you have the list of words of the Ted talk, as well as a sentiment lexicon, it is time to see whether any of the words in the talk match with a word that is part of the sentiment lexicon.\n\nTo do this you have to combine the two tibbles, using the function **inner_join()**. More specifically, **inner_join()** will join the word values of the tibbles of *words_ted* and *bing* together, and keep only those that overlap. The result will be stored in a new object, which you can title *words_ted_bing*.\n\nIn addition, it is nice to know how many times a specific word and sentiment occur together, for this you can use the function **count()** You can view the results using the **view()** function.\n\n```{r}\nwords_ted_bing <- words_ted |>\n  inner_join(bing) |>  \n  count(word, sentiment)  \n\nview(words_ted_bing)\n```\n\n## Step 5: Making a visualisation\n\nAlthough, the resulting word list already provides insight into the different words that are included in the talk, it would be nice if you can interpret the result in one glance with the help of a visualisation.\n\nIndeed, a visualisation will make it much clearer whether more positive or negative words have been included, and which words have been used the most.\n\nCurrently, the list of words is ordered alphabetically and contains 88 different words. The visualisation would be more clear if it only includes the top 15 words of each sentiment, and orders them from most used to least used.\n\nYou can do this in r by using the function **slice_max()**. This function will order the words by the number of times they occur and only keep the words with the highest values. To elaborate, **n = 15** specifies that you would like to keep the top 15 words with the highest frequency. The argument **order_by** specifies that the words should be ordered based on their count (*n*) instead of alphabetically. The argument **by** specifies that the words should be grouped based on their sentiment, so you get both the top 15 positive words and the top 15 negative words. Lastly, **with_ties = FALSE** specifies that you want only 15 elements even if there are other elements that have the same amount.\n\n```{r}\nwords_ted_bing_arranged <- words_ted_bing |>\n  slice_max(n = 15, order_by = n, by = sentiment, with_ties = FALSE) \n```\n\nNow that the appropriate words have been selected for the visualisation, you can create it. For this **geom_col()** can be used, which is a type of bar chart. Within **geom_col()** you can specify that you would like the count (*n*) on the x-axis, the word on the y-axis ordered from highest to lowest frequency (*y = reorder(word, n))*, and the sentiment as the fill, so positive and negative words have different colours. The title and labels can be specified using the **labs()** function. NULL means that the y-axis will not be labelled.\n\n```{r}\nwords_ted_bing_arranged |> \n  ggplot() + \n  #Specify the aestethics in the geom_col, reorder() makes sure that the words with the highest frequency are printed at the top.\n  geom_col(aes(x = n, y = reorder(word, n), fill = sentiment)) + \n  #Label the graph properly\n  labs(\n    title = \"Most Common Positive and Negative Words in Ted Talk\",\n    x = \"Frequency\", \n    y = NULL\n  )\n```\n\n## Step 6: (If time): Improve the visualisation using facet_wrap\n\nIf time: Currently, the visualisation can easily be interpreted to see which positive and negative words have been used the most. However, to make this even clearer the negative and positive words can be split from one another. You can do this in r by using the **facet_wrap()** function, which will split the visualisation into two separate graphs, in this case based on their *sentiment*. The argument **scales = \"free\"** specifies that the y-axis should be free, so the words do not overlap with one another.\n\n```{r}\nwords_ted_bing_arranged |> \n  ggplot() + \n  geom_col(aes(x = n, y = reorder(word, n), fill = sentiment)) + \n  labs(\n    title = \"Most common positive and negative words in ted talk\",\n    x = \"Frequency\", \n    y = NULL\n  ) +\n  facet_wrap(~sentiment, scales = \"free\")\n```\n\n## Step 7.1: Interpreting results and investigating usage of like\n\nNow, that we have a clear visualisation, we can see whether we can draw any interesting conclusions from the results.The most common positive word is \"like\",\n\nHowever, in English, the word \"like\" can be used as a noun or verb where it has a positive connotation but it can also be used as a filler or adverb, where it denotes a similarity. As a result, sentences containing *\"like\"* should be investigated.\n\nThis can be done using the **filter()**, **detect_str()** and **pull()** function. For **str_detect()**, you specify the pattern you are looking for in a string of text, using a regular expression in this case \"*like*\". Unfortunately, discussing the exact details of regular expressions falls outside the scope of this workshop.\n\nHowever, it is good to know that it is a way of specifying patterns in text. **str_detect()** also requires an input, which in our case are the words. If a word matches the pattern it returns TRUE otherwise it will return FALSE. Using **filter()** it will only keep sentences that match the mattern.\n\n```{r}\nsentences_ted |> \n  #Only keep sentences that contain the word \"like\"\n  filter(str_detect(sentences, \"like\")) |>\n  pull(sentences)\n```\n\nLook at the sentences, is *\"like\"* being used in a positive manner here or does it take on a different meaning? Indeed, when you look closely you can interpret this from the following sentences:\n\n1.  Here like is negated by the use of \"don't\", this is a downside of the current sentiment analysis as it cannot take this into account.\n2.  Here like is an adverb. 3, 4, 5: Here like is used as a verb which arguable could be positive\n3.  Here like is used as a filler.\n\nThis shows that sentiment analysis are not 100% accurate and are there to give us a general idea of the sentiment of the text. However, human inspection is still needed to interpret and verify the results.\n\n## Step 7.2: Interpreting results and investigating usage of corruption\n\nLuckily, many English words do only have one function, such as the word *\"corruption\"*. It would be interesting to see how the speaker user the word \"corruption\" to appeal for their case and whether any arguments or ethos, pathos, logos can be detected. This again can be done using the functions used above.\n\n```{r}\nsentences_ted |> \n  #Only keep sentences that contain the word \"corruption\"\n  filter(str_detect(sentences, \"corruption\")) |>\n  pull(sentences)\n```\n\nLook at the results, how does the speaker use the words corruption to frame the current political government?\n\nSome of the relevant findings may be: This talk frames the current politicians and political system in a very negative light, which becomes clear from words like *\"corruption\"*, *'dictatorship\"*. Moreover, those who are against corruption seem to then be *\"attacked\"*. The words bring out emotion and there is a great contrast between the the politicians and *\"we\"* and *\"I\"*. Indeed, *\"we are also outspoken\"* or *\"we are also connecting\"* show that the speaker views itself in a more positive light fighting against this corruption.\n\n## Step 7.3: Interpreting results and investigating usage of democracy\n\nLastly, as this talk is about democracy, it can be interesting to see how this talk frames democracy. In our first workshop we discussed a digital humanities paper that investigated how parliamentary debates conceptualise democracry.\n\nThe three most common cases were: 1.) Viewing democracy as a value 2.) Believing democracy is under threat and in need of defence 3.) Using democracy as leverage, trying to make a point persuasive by linking it to democracy.\n\nDoes this talk frame it similarly to one of these points or something else? For this you can reuse the code above but changes *\"corruption\"* into *\"democracy\"*.\n\n```{r}\nsentences_ted |> \n  #Only keep sentences that contain the word \"democracy\"\n  filter(str_detect(sentences, \"democracy\")) |>\n  pull(sentences)\n```\n\nSome of the relevant findings may be: This Ted Talk seems to think that democracy is under threat and needs to be *\"restored\"* or there is a knowledge gap to be *\"filled\"* or it is *\"high time\"* that it is *\"restored back\"*, or there is need for a *\"remedy\"*. The usage of democracy in point 7 could be said to either value democracy, or is used as leverage.\n\n## Step 8: If time: Assignment\n\nChoose another positive and negative word from the list and investigate its usage in the text. What does this word tell you about the speaker's message and intent?\n\n## Step 9. Homework 4\n\nCurrently, the sentiment analysis only uses \"positive\" and \"negative\" words. However, R has another sentiment lexicon titled \"nrc\" which includes 10 different sentiments that are based on emotion. This could perhaps show more specific information. Additionally, you can learn compare results between texts to see whether there is a pattern. This is something you will be exploring in the homework assignment.\n\nYou can find the homework assignment [here](TBA). Download it and save it in your current project folder.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"workshop4.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.555","editor":"visual","theme":"simplex","mainfont":"Cormorant SC","fontsize":"20px","layout":"page","title":"AH-RHET101 Workshop 4: Sentiment analysis in R","subtitle":"Spring 2025","date":"Last updated: `r Sys.Date()`"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}